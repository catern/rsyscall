* papers
** rsyscall: A cross-process syscall API for Linux
** cross-host operations with rsyscall
** asynchronous IO with rsyscall thread pool (flexsc style)
* what to do after writing
1. send to labs team at TS for review
2. send to fork paper guy for review
3. send to professors as Kai says and ask about research opportunities
* Direct-style process creation on Linux
** title
"direct-style" just being a nicer way to say "imperative"

"imperative process creation" doesn't sound good because it says "imperative"
** notes
Hmm maybe I should move the features of processes thing into a subsequent background section?
Basically talk about fork, spawn, and direct style in the intro,
and then say, "hey we made direct style for Linux".

fork/spawn/direct-style is a strong opening I think.
*** performance I guess?
can mmap a bunch of stuff and show that without having to copy page tables, it's faster

can show a chart even, two lines, speed with and without, nice
*** features to emphasize
- it covers everything you might want unlike posix_spawn
- it's immediately deployable
- it's efficient
- it has a coherent theory behind it and is relatively easy to use
** abstract
Traditional process creation interfaces are complex to use, limited in power, and difficult to abstract over.
We develop an alternative process creation interface for Linux
which allows a program to create a new process in an inert state
and then initialize it from the outside by directly operating on it
in the same style as any other operating system object.


This method of process creation results in more comprehensible programs, 
has better error handling,
is more efficient,
and is more amenable to abstraction.
Our implementation is immediately deployable without kernel modifications on any recent Linux kernel versions.
** introduction
*** start of introduction
Processes are a powerful and widespread abstraction.
Most operating systems provide processes,
and most applications are distributed as executables which run in isolated processes.
*** features of processes
Beyond the basic usage of a processes to implement an application,
the proces interface has many further uses.
**** file descriptor inheritance allows abstracting over resources
In Unix, the mechanism of file descriptor inheritance
allows a process to be provided a resource by its creator,
while abstracting over the precise nature of that resource.
For example, a process can be provided a file descriptor which it is expected to read and write,
which can be a file in a filesystem, a pipe, a network connection, or some other resource,
without the process being aware of the type of resource it has been passed.
As another example,
a process can be provided a socket file descriptor from which it is expected to accept connections,
without being aware of whether those connections come from the internet or from a local Unix socket.
This mechanism is the basic principle of pipelines and redirection the Unix shell,
but it is rarely used outside of the shell.
***** notes
      mention ucspi

      and inetd
**** namespace modification allows customization without explicit support
In many systems,
it's possible to modify a process's view of nominally "global" resources.
In Unix-derived systems, this ability is most influentially provided in Plan 9,
which allows each process to customize its view of the filesystem with private mounts and union directories.
In Linux, these concepts were implemented as per-process namespaces.
In its most basic application,
this allows customizing the environment of a process without having to write explicit support code for customization.
For example, Plan 9, unlike most other Unix-derived systems,
did not have a =PATH= environment variable which was searched by code in the process to find executables;
instead, each process was executed with a =/bin= directory at the root of the filesystem,
which was a union of many other directories,
and simply executed =/bin/foo= to run the program named =foo=.
In this way a process could get a customized set of executables,
without needing code to parse and handle =PATH=.
**** privilege separation allows sandboxing
# attempt
The basic isolation powers of processes are used to simplify application development:
it is beneficial to have a private virtual memory space when developing a stand-alone program.
But most systems have additional mechanisms of isolation between processes,
such as different privilege levels and access to global resources,
which can be used to provide a form of sandboxing.
For example, components which may exposed to hostile network requests
can be run in a separate process, at a lower privilege level than the main program;
in this way, even if an attacker gains control over that component,
the attacker will only have access to the lower level of privileges of that component,
rather than the full privileges of the main program.
**** robust privilege separation and resource privion allows capability-based-security
# we separate "regular" sandboxing and capability-based security
# because lots people won't understand they're the same thing
As a further, more robust development of process-based sandboxing,
the privileges of a process can be explicitly enumerated
in a capability-based security model.
By using previously-mentioned resource passing mechanisms,
such as file descriptor inheritance or namespace manipulation,
and by disabling the process's access to global resources such as the shared filesystem,
we can enforce that all resources used by the process are passed at creation time.
***** notes
      mention capsicum
**** non-shared-memory concurrency allows exploiting parallelism in a simple way
# TODO not sure about this one
Processes run concurrently,
which allows exploiting parallelism in the hardware.
Since processes don't share memory,
they can provide a less complex parallel programming environment
than shared-memory thread-based approaches.
The most popular parallel programming environment in existence today is the Unix shell,
which obtains its parallelism by running multiple processes connected via pipes.
The Unix shell has a relatively constrained form of parallel processing,
but it's also possible to create more complex webs of parallel processes,
where, for example, one process might take multiple inputs over multiple pipes,
or produce multiple outputs.
**** services?
  - Failure monitoring? Concurrency control? Concurrency in general? Service-oriented distributed systems?
    Shared-nothing message-passing concurrency? (aka "distributed systems")
**** notes
     remember: talk about the features we use in the demos in this process-features section!
     and likewise, demonstrate the features we talk about in this section, in the demos sectioN!
*** therefore it is ipmortant to have a good process creation interface
These features of processes are all used at process creation and initialization time;
hence, it is important to have a good process creation interface,
so that these features can be used.
*** TODO existing process creation mechanisms are bad
Currently available process creation interfaces
on Unix-like systems such as Linux
are not generally considered high quality.

The two interfaces currently available can be divided into "fork-style" and "spawn-style".
**** TODO fork-style
Fork-style interfaces are those that follow the model of fork.
A process calls fork, and a copy of that process is made,
with the return value of fork being the only differentiation between the process and its copy.
The same program continues executing in both processes.
Typically, the program will case on the return value to determine if it's running in the parent or the child,
and run process setup and initialization code if the program is running in the child,
calling various syscalls to set up the state of the child.

Fork has a number of flaws,
and much ink has been spilled detailing them.
We'll cover the most prominent briefly here,
but see Baumann 2019 for more detail.

Fork has an unusual execution model that makes it fundamentally difficult to integrate into normal programs.
***** notes
expand this
- it's weird to return twice
- errors and fallbacks can't be communicated across fork
- it's fundamentally slow
- it's weird and sucks
- fork can't be used in many scenarios, such as in:
    - large memory programs
    - multithreaded programs
    - programs using unusual fork-unaware libraries such as CUDA or kernel networking bypass
**** TODO spawn-style
Spawn-style interfaces are those that follow the model of =posix_spawn=.
All the details about the new process are provided up-front.

This does not extend to all the possible attributes that might want to set for a new process.
Most systems have a large number of syscalls which can mutate the state of a process during its lifetime;
for a spawn-style interface to work in all scenarios,
all those possible mutations must be reproduced in the interface.
This is difficult;
furthermore, even if that can be achieved,
a spawn-style API does not allow for conditional logic during the setup,
where later syscalls depend on the results of earlier syscalls.
***** notes
need to list more flaws I guess
- limited number of modifiable things
- limited expressiveness (conditionals?)
- error handling (haven't included this one yet!)
**** TODO these mechanisms are bad
# this is a weird place to put this?
Most applications would benefit from all the features of processes we listed above.
But most applications don't use any of those features.
We don't claim it is the sole reason,
but the poor quality of the process creation interface
is one thing preventing more advanced usage of processes.

***** notes
should establish more that applications don't use these features

- We delegate many of these features to specific separate programs/servers to abstract over them,
  which means we can't use these features in combination.
  - shells, container engines, process supervisors
- If we make it simpler to create processes, we can increase our usage of these features, including in combination
  - This will also make it possible to replace separate programs running as system servers, with libraries
**** TODO direct-style isn't bad
In some systems, such as KeyKOS, seL4 and other capability-based operating systems,
there is another style of process creation.
In this style, a process is created by its parent in an inert state,
and then supplied with various resources,
and then started once it is fully set up.
The same mechanisms that can mutate a process while it is running,
are used to mutate the process while it is in an inert state;
in such systems, these mechanisms can be used on the process from *outside* the context of the process,
just as easily as they can be used from inside the process.
We refer to this as "direct-style" process creation,
because the parent creating the process operates on it directly and imperatively
rather than dispatching a distinct unit of code to perform setup from inside the context of the new process,
as in fork-style,
or building up a declarative specification of what the new process should look like,
as in spawn-style.

Direct-style process creation lacks many of the disadvantages of fork-style or spawn-style process creation.
***** notes
      need to say more about how it lacks the disadvantages

      SEL4 has this style.
      https://docs.sel4.systems/Tutorials/threads.html
*** we've made a new process creation interface that doesn't suck
We have adapted direct-style process creation for Linux.
In the rest of the paper, we will examine the design and implementation of this style of process creation on Linux,
and demonstrate its usage.
** overview/example
   In a direct-style program,
   we cause things to happen by directly invoking commands which have effects.
   In an object-oriented language,
   this most naturally takes the form of calling methods on objects.

   With process creation,
   we want to setup a process in a specific state.
   We can make syscalls to change the state of a process.
   In an object-oriented language,
   direct-style process creation
   then takes the form of calling syscalls in a process,
   by calling methods *on* an object.

   The change from needing to be *in* the process,
   to being able to operate *on* the process is the primary source of improvement.

   We call a syscall to create a new process;
   instead of the syscall forking off the new process,
   it returns an object;
   then our program can act both on the original process and the new process.

   We can call syscalls in the new process to change how it is set up,
   and, typically, ultimately call =execve= to exec a new program in the process.

   Then we can monitor the child process,
   no longer under our control,
   using normal Unix child monitoring techniques.
** demos
*** basic example
Simply creating a new process and immediately execing a binary:

#+BEGIN_SRC python
# Use the clone syscall in the local thread to create a new thread;
# we use a wrapper that supplies defaults for all arguments.
child = local.thread.clone()
# Call execve to run a different executable in the child thread;
# We pass the executable path as the first argument in the argument list, as is traditional.
# We use a wrapper that defaults envp to an unchanged environment, so we don't pass envp.
child.execve(hello_path, [hello_path])
# Wait on the child process to exit.
event = child.process.waitpid(W.EXITED)
if not (event.code == CLD.EXITED and event.exit_status == 0):
    raise Exception("hello exited uncleanly :(")
#+END_SRC

We use the word "thread" to refer to a process we control,
along with some associated resources for that process.

As exceptions are used for error-handling in the Python API,
there is no need for error-checking code here.
**** introducing
thr

we refer to processes as threads.

hmm maybe we shouldn't
no it's fine


clone

not using:
execve returining the child process

*** passing down fds
We can open file descriptors, including sockets, in the child, and pass them down straightforwardly:

#+BEGIN_SRC python
child = local.thread.clone()
sock = child.socket(AF.INET, SOCK.DGRAM)
# bind the socket to a sockaddr_in;
# the sockaddr is allocated in memory with child.ptr and is garbage collected
sock.bind(child.ptr(SockaddrIn(0, 0)))
sock.listen(10)
sock.disable_cloexec()
child.execve(executable_path, [executable_path, "--listening-socket", str(int(sock))])
#+END_SRC

File descriptors, here, are object oriented and have relevant syscalls as methods.
They make syscalls in the process they are created in by default;
we can create more objects referring to the same file descriptor from different processes
if we want to make the syscalls from another process.
**** introducing
disable_cloexec
socket creation
using int on sock
.ptr

not using:
.args
as_argument
*** piping
We can connect two children with a pipe,
similar to the shell pipeline "yes | head -n 15":

#+BEGIN_SRC python
# create the pipe
pipe = local.thread.pipe()
child1 = local.thread.clone()
# inherit the write-end of the pipe to child1, and replace child1.stdout with it
child1.inherit_fd(pipe.write).dup2(child1.stdout)
child1_proc = child1.execve(yes_path, [yes_path])
child2 = local.thread.clone()
# inherit the read-end of the pipe to child2, and replace child2.stdin with it
child2.inherit_fd(pipe.read).dup2(child2.stdin)
child2_proc = child2.execve(head_path, [head_path, "-n", "15"])
#+END_SRC

After a process is created with clone,
it maybe have inherited file descriptors.
We make this inheritance explicit with =inherit_fd=,
which returns a new handle to the file descriptor.
Then we simply =dup2= as normal to replace child1's stdout with the write end of the pipe.
We do the same operation with child2.
**** introduce
dup2
pipe

not using:
syscalls returning the buffer passed into them
malloc
*** mount namespace
We can make a new mount namespace and rearranging the filesystem tree for the child process.
We bind-mount /proc at /proc inside the chroot directory,
chroot into the directory,
and exec an executable which will run inside the chroot.

#+BEGIN_SRC python
child = local.thread.clone(CLONE.NEWUSER|CLONE.NEWNS)
child.mkdir(rootdir/"proc")
child.mount(Path("/proc"), rootdir/"proc", "", MS.BIND, "")
child.chroot(rootdir)
child.execve(executable_path, [executable_path])
#+END_SRC
**** introduce
namespaces

slashes in paths
*** network namespace
Here we show real code for launching a non-trivial application and namespace:
the Miredo IPv6 tunneling software.
Miredo is separated into two components, a privileged process which sets up network interfaces,
and an unprivileged process which talks to the network.
With minimal modifications,
we launch Miredo entirely unprivileged inside a user namespace and network namespace.

Complex coordination and setup of multiple processes would previously require hundreds of lines of concurrent code;
we reduce it to a few dozen lines of sequential code, with no loss in power.
We use a few helper functions to keep the attention focused on the interesting parts.
#+BEGIN_SRC python
### create socket outside network namespace that Miredo will use for internet access
inet_sock = local.thread.socket(AF.INET, SOCK.DGRAM)
inet_sock.bind(local.thread.ptr(SockaddrIn(0, 0)))
# set some miscellaneous additional sockopts that Miredo wants
set_miredo_sockopts(local.thread, inet_sock)
### create main network namespace thread
ns_thread = local.thread.clone(CLONE.NEWNET|CLONE.NEWUSER)
### create in-network-namespace raw INET6 socket which Miredo will use to relay pings
icmp6_fd = ns_thread.socket(AF.INET6, SOCK.RAW, IPPROTO.ICMPV6)
### create in-network-namespace socket which Miredo will use for unassociated Ifreq ioctls
reqsock = ns_thread.socket(AF.INET, SOCK.STREAM)
### create and set up the TUN interface
tun_fd, tun_index = make_tun(ns_thread, "miredo", reqsock)
### create socketpair which Miredo will use to communicate between privileged process and Teredo client
privproc_pair = ns_thread.socketpair(AF.UNIX, SOCK.STREAM)
### start up privileged process which manipulates the network setup in the namespace
# privproc_thread is cloned from ns_thread, which was also created through clone; this nesting is fully supported
privproc_thread = ns_thread.clone()
# preserve NET_ADMIN capability over exec so that privproc can manipulate the TUN interface
# helper function used because manipulating Linux ambient capabilities is fairly verbose
add_to_ambient_caps(privproc_thread, {CAP.NET_ADMIN})
# privproc expects to communicate with the main client over stdin and stdout
privproc_side = privproc_thread.inherit_fd(privproc_pair.first)
privproc_side.dup2(privproc_thread.stdin)
privproc_side.dup2(privproc_thread.stdout)
privproc_child = privproc_thread.execve(miredo_privproc_executable_path, [
    miredo_privproc_executable_path, str(tun_index)
])
### start up Miredo client process which communicates over the internet to implement the tunnel
# the client process doesn't need to be in the same network namespace, since it is passed all
# the resources it needs as fds at startup.
client_thread = ns_thread.clone(CLONE.NEWUSER|CLONE.NEWNET|CLONE.NEWNS|CLONE.NEWPID)
# lightly sandbox by unmounting everything except for the executable and its deps (known via package manager)
unmount_everything_except(client_thread, miredo_exec.run_client.executable_path)
# a helper function for preparing the fds that are passed as command line arguments
async def pass_fd(fd: FileDescriptor) -> str:
    client_thread.inherit_fd(fd).disable_cloexec()
    return str(int(fd))
client_child = client_thread.execve(miredo_client_executable_path, [
    miredo_client_executable_path,
    pass_fd(inet_sock), pass_fd(tun_fd), pass_fd(reqsock),
    pass_fd(icmp6_fd), pass_fd(privproc_pair.second),
    "teredo.remlab.net", "teredo.remlab.net"
])
#+END_SRC

Note that, as mentioned in a comment, we call =clone= on a thread we previously created with =clone= on =local.thread=.
Such nested cloning is fully supported and allows us to set up complex graphs of processes and namespaces.

**** introduces
nested clone
socketpair
several helper functions
**** paring down
ok so maybe I should remove some of the unnecessary networking stuff?
actually we can come back to this later
# TODO maybe I should remove some more of the networking stuff into helper functions or something
** implementation
*** basics about rsyscall
Our primary need for implementing direct-style process creation
was a robust system for cross-process operations.
We implemented this in the rsyscall project.
rsyscall is a toolkit for cross-process operations on Linux,
with several language-specific library implementations.

In this paper, we'll give a brief overview of rsyscall,
and focus on implementation issues specific to process creation.
# no need to mention more detailed papers here

rsyscall's basic primitive is a syscall function
which explictly specifies the process in which to perform the syscall.
On Linux x86_64, calling a syscall is completely specified by the syscall number plus six register-sized arguments;
a syscall returns exactly one register-sized value.
rsyscall's most deployable form implements these explicit-process syscalls by sending these seven integers over a pipe
to a stub process running a stub function which reads syscall requests off the pipe,
performs them,
and writes the return value back over a return pipe.
This allows us to calls syscalls in other processes under our control,
in a way that is easily deployable on stock Linux kernels.

Many syscalls either take or return pointers to memory,
and require the caller to read or write that memory to provide arguments or receive results.
Hence, rsyscall needs a means of memory access for the target process.
We implement this through another set of pipes,
and by explicitly copying memory into and out of those pipes using the =read= and =write= system calls.
When we wish to read =N= bytes of memory at address =A= in the target process,
we =write(memory_pipe, A, N)= and then read the memory off the other end of the pipe in the parent process.
To write some data to memory, we instead first write that data to the pipe,
then =read(memory_pipe, A, N)= to copy that data from the pipe into memory.

ptrace provides an alternative means to perform arbitrary actions on other processes.
However, among other issues, it has the unavoidable substantial disadvantage of not permitting multiple ptracers.
A ptrace-based implementation would prevent using strace or gdb on rsyscall-controlled processes,
which is an unacceptable limitation for a general-purpose utility.

The =process_vm_readv= and =process_vm_writev= system calls
allow the caller to read and write memory from the virtual address space of other processes.
However, they require that the caller have specific credentials relative to the process being accessed,
which may not always be the case.
Additionally, these system calls are disabled if ptrace is disabled system-wide,
which is a niche but possible system configuration.
To ensure that rsyscall can be used for arbitrary purposes and on arbitrary systems, we avoided these calls.

rsyscall provides a suite of useful functionality,
including garbage collected file descriptors,
which make it straightforward to write programs which perform arbitrary cross-process syscalls.
# TODO make some more notes here? about features in rsyscall that we assume down below
# I'll explain them in another paper
*** clone
Now that we've established the basic operations which rsyscall provides,
let's consider the specific issues related to process creation and initialization.

There are three Linux system calls which create processes:
=fork=, =vfork= and =clone=.

=vfork= has some intriguing features,
and we performed some early investigation into process-creation primitives based on it[fn:vfork],
but ultimately discarded it in favor of our rsyscall-based approach.
With an rsyscall-based approach, =vfork= is not suitable,
because when a child process is created with =vfork=,
the parent process is suspended until the child process either calls =exec= or =exit=;
since the parent process is where our actual program is running, this is unsuitable.

=fork= is the typical approach,
but we can emulate it by essentially calling =clone= with =CLONE_VM= unset,
so that the new process has a copy of the parent process's address space;
hence we'll move on directly to consider the details of =clone=.

=clone= (along with =fork=) creates a new process
which starts executing at the next instruction after the syscall instruction,
with its registers in generally the same state as the parent process.[fn:glibc]
In the style of Plan 9's =fork= syscall, which inspired =clone=,
=clone= takes a mask of flags which determines whether several attributes of the new process
are either shared with, or copied from, the parent process.

While this is the traditional design,
it's not suitable for us:
We want, at a minimum, to be able to set arbitrary registers for the new process,
so that we can control where it begins executing and the stack it executes on.

Fortunately, =clone= lets us set the stack register to an arbitrary pointer,
and this is sufficient.
To avoid special-casing =clone=,
we ensure that the next instruction executed after any syscall
is (in x86 terms) a =RET=.
Since we control the stack,
this allows us to cause the new process to jump to an arbitrary address;
then, we can provide additional arguments and data from this code under our control
by putting them on the stack.

As a useful general-purpose utility,
we provide a trampoline which sets all registers to values found on the stack.
We take a moment here to note that this is, admittedly,
also a generally useful utility for hackers performing return-oriented-programming attacks,
but we console ourselves with the knowledge that similar functionality exists in any standards-compliant C library.
With this trampoline code linked into the target process,
we can provide a helper Python function that,
when given a function pointer following C calling conventions, and some arguments,
will prepare a stack for a call to clone such that the new process will call that function with those arguments.

With our new ability to call arbitrary functions,
we can now call =clone= so that it launches a process running our stub syscall function,
described in the previous section,
which will use two pipes passed as arguments to receive syscall requests and respond with syscall results.

The addresses of these functions and trampolines are discovered through a linking procedure.
When the process being created is in the same address space as the main process which is running user code,
the location of the rsyscall library and the code within it is known through normal language linking mechanisms,
and no special effort needs to be taken.
However, when a process is created with a different address space,
such as when we establish a connection to another process after it's been started,
we need to perform linking to learn the addresses of functions.
This linking procedure is performed while bootstrapping the connection,
and generally involves the target process sending a table of important addresses to the parent process.

After creating a new process with clone,
most system calls can be called as normal.
The new process can be modified freely through unshare, dup2, and other system calls.
*** exec
Eventually, most processes will want to call =exec=.
=exec= is unusual and requires careful design,
because it does not return if successful.
Therefore we need a way to determine if =exec= is successful;
naively waiting for a response to the syscall will leave us waiting forever.

One traditional means is to create a pipe before forking,
ensure both ends are marked =O_CLOEXEC=,
perform the fork and exec,
close the write end of the pipe,
and wait for EOF on the read end.
If the child process has neither called exec nor exited,
then the write end of the pipe will still be open in the child process's fd table.
But once the child process calls exec,
=O_CLOEXEC= will cause the write end of the pipe to be closed after the exec.

This is a somewhat indirect way of achieving the goal, but works acceptably with fork.
Unfortunately, many of our child processes will share their fd table with the parent at the time they call exec,
so if the write end of the pipe is closed in the parent, then it will be closed in the child as well.

Fortunately, there is an alternative.
The =ctid= argument to =clone= specifies a memory address which,
when the =CLONE_CHILD_CLEARTID= flag is set,
the kernel sets to zero when the child exits or execs,
and then performs a futex wakeup on.
More specifically,
the kernel clears and does a wakeup on =ctid= when the child process leaves its current address space;
this precisely coincides with exiting or execing,
since those are the only way to change address space in current Linux.

Unfortunately, futexes in current Linux integrate poorly:
There is no way for to block on more than one futex at a time,
and no way to integrate a futex into a file-descriptor based event loop.
The best we can do is create a dedicated process for each futex we want to wait on.
We have this process exit zero when the futex wakes up;
we can integrate waiting for the process exit into our event loop through normal means.

Nevertheless, this provides a solution.
We provide an argument for =ctid= whenever we call =clone=,
and set up a process to wait on that futex so that when we call =exec=,
we wait for either the =exec= to return or the futex process to exit,
whichever comes first.
If the futex process exits,
we close the write end of the pipe and wait for EOF on the read end;
if we don't read a response for the exec,
we know that the child has successfully called exec.

This works well;
however, it would be nicer if Linux natively provided functionality to wait for a child's exec.
Several systems with kqueue, including FreeBSD,
allowing waiting for exec events of arbitrary processes through kqueue's =EVFILT_PROC=.
One approach for Linux would be to add a new =clone= flag to opt-in to receiving =WEXECED= events through =waitid=;
note that a =waitid= flag alone is not sufficient,
since it's necessary to receive =SIGCHLD= signals for the =WEXECED= event if waiting for it from an event loop.
Alternatively, some form of the now long-removed =FUTEX_FD= functionality could be re-added to Linux
so that waiting for the =ctid= futex could be integrated into an event loop.
*** unshare
#+BEGIN_QUOTE
int unshare(int flags);

unshare()  allows  a process (or thread) to disassociate parts of its execution context
that are currently being shared  with  other  processes  (or  threads).
Part of the execution context, such as the mount namespace,
is shared implicitly when a new process is created  using  fork(2) or vfork(2),
while other parts, such as virtual memory, 
may be shared by explicit request when creating a process or thread using clone(2).

The flags argument is a bit mask that specifies which parts of the execution context should be unshared.
#+END_QUOTE

More concretely, when some part of the execution context is currently shared,
and it's specified by the flags to =unshare=,
=unshare= will make a copy of that part of the execution context and switch the process to that copy.

For most =unshare= flags, the complexities of usage are minor and well-known,
and are also encountered with =fork=;
for example, =unshare(CLONE_FS)= and =fork= unshare the current working directory,
so that one process might change its current working directory without affecting the other process.
In this case, user code must be careful to not use a relative path from one process
in a process that has a different current working directory.

The most important issues lie with =unshare(CLONE_FILES)=.
When calling =unshare(CLONE_FILES)=, or =clone(CLONE_FILES)=, or =fork=,
a new file descriptor table is created
with a copy of all file descriptors existing at the time of the system call.
This is known as "file descriptor inheritance".

The new file descriptor table will contain 
private file descriptors from libraries and other processes sharing the same file descriptor table.
Leaving these file descriptors open in the new table is a form of resource leakage,
but it also will cause erroneous behavior.
For example, it's a common practice to close the write end of a pipe
and expect an EOF on the read end;
if the write end is copied into the new file descriptor table before being closed,
and the write end is never closed in the new table,
the read end will never get an EOF.

The traditional way to deal with this is the =CLOEXEC= file descriptor flag.
This flag can be set on a file descriptor and affects what happens when the process calls =execve=.
Typically, the effect is described as "the file descriptor is closed when the process calls =execve=".
Since we are discussing processes which share a file descriptor table,
it's useful to clarify that =execve= first creates a new file descriptor table
and copies all file descriptors existing at the time of the system call,
before performing the =CLOEXEC= behavior;
thus, if a process calls =execve= while sharing its file descriptor table,
that won't resulting in =CLOEXEC= fds being closed in that file descriptor table for other processes sharing the table;
rather, the observed effect is instead that
the =CLOEXEC= fd is not copied into the new file descriptor table that the process uses after the =execve= completes.

In a correct conventional program or library, =CLOEXEC= is set on all file descriptors at creation time,
to avoid unexpected file descriptor inheritance;
and it is only unset after creation for file descriptors for which inheritance is explicitly desired.
We follow the same discipline.

However, just setting =CLOEXEC= is not sufficient for us,
because we can inherit file descriptors through =unshare(CLONE_FILES)=
into processes that may never call =execve= at all,
and thus never clean up the file descriptor table through =CLOEXEC=.

We can solve this by explicitly closing all =CLOEXEC= file descriptors,
achieving the same behavior as =execve= and cleaning up the file descriptor table.
However, this would require us to unset =CLOEXEC= on any file descriptor we want to preserve into the new table.
As an optimization, since file descriptors in rsyscall are garbage collected,
we enumerate all live file descriptor references in the process and except those from being closed,
even if they have =CLOEXEC= set.

This issue and solution have a surface similarity to the =closefrom= function provided on some Unix systems;
=closefrom= allows closing all file descriptors with an numeric value greater than a specified integer.
Unlike =closefrom=, explicitly closing only =CLOEXEC= file descriptors
preserves the ability to opt-in to inheritance when desired.
=closefrom= lets the program special-case stdin/stdout/stderr as inherited,
while not permitting anything else, such as higher fds, to be inherited.
This is not recommended for a program that is part of a general-purpose system;
inheritance of useful fds to unaware programs is a useful, if niche, feature,
which can be used for many of the same purposes as inheritance of environment variables.
Thus, we close fds based on whether they have =CLOEXEC= set,
not based on their numeric value.

During the time between calling =unshare(CLONE_FILES)= and closing all =CLOEXEC= file descriptors,
all the file descriptors that were in the original fd table at the time of calling =unshare=
are in the new file descriptor table.
Rather than ignore this as an implementation detail,
we explicitly expose this to the user;
after an =unshare(CLONE_FILES)=,
the user can call an =inherit= method on a file descriptor
to get a new live reference to the file descriptor 
so that it is preserved by the garbage collector across the closing of =CLOEXEC= file descriptors.
If =inherit= is called on a file descriptor that wasn't copied into the new file descriptor table
(for example, a file descriptor that was opened in the parent after the =unshare(CLONE_FILES)= call),
or is called after the file descriptor table has been cleaned by closing all =CLOEXEC= file descriptors,
then an error is returned to the user.

# TODO note that exec should also have this exception stuff,
# that way we could have scripts use fexecve
**** outline
     we'll cover inheritance through a clone_files making a new fd table,
     then doing a do_cloexec or exec to clear it out.

     and maybe details of unshare?

     yeah, we'll focus on unshare.
     basically just talk about unshare.
**** text
** evaluation (or, measurement)
something?
** future work
rsyscall has many uses unrelated to process creation;
we are actively exploring such directions.
** conclusion
* Footnotes

[fn:glibc]
Note that =glibc= defines a wrapper for the raw kernel syscall;
we are here talking about the kernel syscall.
