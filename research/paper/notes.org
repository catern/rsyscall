* papers
** rsyscall: A cross-process syscall API for Linux
** cross-host operations with rsyscall
** asynchronous IO with rsyscall thread pool (flexsc style)
* what to do after writing
1. send to labs team at TS for review
2. send to fork paper guy for review
3. send to professors as Kai says and ask about research opportunities
* misc other references
porting unix to windows NT, mentions implementing fork on top of CreateProcess
https://www.usenix.org/legacy/publications/library/proceedings/nt-sysadmin97/full_papers/korn/korn.pdf

* other ideas
can do stuff to prepare the process up-front and replicate it to make many

* comments
>1. We need a name for your technique. Let's call it direct_spawn()

Sure, but the name I use in the paper is "direct-style". There's no
real individual function that would be called "direct_spawn". The
closest possible is the syscall to clone to create a new process, but
that has the same interface as clone and is ultimately a small wrapper
around clone. Does "direct-style" have some problem? Maybe
"direct-style clone"?

>2. For OSDI, you need an evaluation section of 3/4 pages. I think we
>   can take a bunch of open-source apps and show that direct_spawn()
>   is better than the alternatives on all sorts of dimensions (errors
>   handling, performance, LoC). IIRC, the JVM forks early to provide a
>   parent that can do low-cost children spawning. Can you modify the
>   JVM to use direct_spawn() and simplify their code?

Yes, it's something that I've considered, but that would require
porting the API to Java, which would take at least a year of work.

Right now my plan is to do three comparisons:

- Show that some code with equivalent behavior in both direct-style
  and fork is much longer in fork-style
- Benchmark fork against clone in large address spaces
- Benchmark Python's subprocess library against Python rsyscall

Do those sound plausible? Should I add some more?

>3. As direct_spawn() is user-space only, Maybe port it to BSD, and
>   show that it also make things better there?

A good suggestion, but it's "user-space only" in a similary way that
CRIU is "user-space only" :) There is a great deal of Linux-specific
functionality being used. Porting is non-trivial. rsyscall is
essentially a libc, which is not something that's easily portable.

>4. You are competing against fork() and posix_spawn() for creating a
>   new process. These are C APIs. So I think you should show your API
>   with C, not Python. Further, in the design section, Python is a
>   distraction. It makes the API unclear and more complex than it
>   needs to be (e.g., child.ptr() is very odd in Python, or
>   inherid_fd() is chainable is strange. It's hard to imagine the C
>   API from that).

I understand, and I wish I could, but it's not possible. The C API
is far more primitive than the the Python API. The usable
implementation is mostly in Python. (bizarrely low-level Python,
but Python nonetheless.) I wish I could have written primarily in
C, but there are numerous disadvantages to writing in C. Good
alternatives would have been Go or Rust, but I chose Python due to
its wide adoption.

I'll avoid using the chaining of inherit_fd, since it might be
confusing. The use of .ptr is hard to avoid without significantly
reworking the API. Other thoughts on the use of Python?

>5. You say that main complaint of fork() is that it's
>   slow. direct_spawn() should absolutely be faster than fork then.

Will discuss in the evaluation

>6. The fork() memory copy-on-write in multi-threaded environment also
>   have the terrible downside that the application memory usage could
>   double while the child has not yet execve(). This can send an
>   application in OOM land, which is terrible.

This doesn't happen if the application doesn't touch its memory,
right?

>7. When you argue that you use a pipe for communicating between the
>   parent and the child of direct_spawn(), it seems that you don't
>   explain why it's better than using shared memory (e.g.,
>   mmap(MAP_SHARED), and communicate there). Also, memory structures
>   that need to be sent around (here, via write(memory_pipe) )could
>   just sit on this piece of shared memory and you have a zero-copy
>   solution. You should describe all the possible ways to do IPC, and
>   argue why the one you picked is better than all the others.

Good point, I'll discuss that as well. (The reason is that that
wouldn't work across different kernels, which is the primary case
for the processes being in separate address spaces)

>8. I don't understand in the design of direct_spawn(), why the child
>   is in a different memory space than the parent. Skip
>   mmap(MAP_SHARED) all together, and just use clone(...,
>   CLONE_VM). Performance will be much better in multi-threaded
>   programs.

Good point, I'll discuss that. (I forgot to note that usually they
are in the same address space; the memory stuff I discussed is to
support when they can't be, such as remote hosts.)

>9. Nit picking regarding CLONE_CHILD_CLEARTID: more precisely, the
>   ctid address is cleared when the child releases its memory space
>   (which is the case when the child exits, or execve, but also when
>   it unshare(CLONE_VM)).

AFAIK, unshare(CLONE_VM) either returns an error or has no
effect. I thought it was better to not go into depth on the
mechanism of CLEARTID; do you disagree?

>10. You argue that you want to support cases of CLONE_FILES, and so
>    it's hard to use pipes. I don't think you need to support
>    CLONE_FILES for process creation. Why would anyone spawn a new
>    process with CLONE_FILES? I think it's fine to not offer such
>    feature. You are competing against fork+execve, and posix_spawn,
>    not clone.

I did briefly explain one application: "for example, the child
process might be in a different network namespace from the parent,
and the shared file descriptor table would allow the child to bind
a socket and the parent to use it." Should I go into more depth?
Maybe it's not clear that execve unshare the file descriptor table,
so you can use this as part of a clone+execve?

CLONE_FILES is something I actually use in an example that I
commented out. (You can still see it in the .tex, it's under
Miredo) I commented it out because it was too long and complex.
(The example doesn't actually explicitly pass CLONE_FILES to calls to clone(),
but that's just an oversight,  - CLONE_FILES was previously the default and 
I didn't update the example since commenting it out)

And, of course, CLONE_FILES is mandatory for creating, for
example, worker pools for asynchronous execution of system calls,
or things like that.  Maybe I should mention that as well?

>11. I think the paper should be about rsyscall (which is what you seem
>    to do), and direct_spawn() should be one of its application. It
>    should also have other compelling applications (at least 3 in the
>    paper), which you seem to have. I'm not sure there's enough
>    content for direct_spawn() alone for OSDI. Maybe for another
>    conference though.

Hmm. I did cut a fair bit of content on Larry's suggestion;
namely, a section that explained the implications of a good
process creation interface, as a justification for extensive
investment into it.

I attached that section to this email. It was between the current
"Background" and "Overview and Examples" sections. Do you think it
makes sense to include?

Is the implementation section too long/uninteresting?

I'm fairly sure that I won't be able to write a paper focusing on all
the applications of rsyscall before the OSDI deadline, so let's just
take that off the table. I understand the appeal though; I considered
it, but rejected it early on because the different applications are
just too distinct, and the resulting paper would be fairly chaotic.

* arguments
  right now people are forced to write C to access this functionality
  I enable it to be used from Python.


  rsyscall is a reimplementation of libc.
  It's inherently not portable.

** ok ok ok
let's think about this calmly and rationally.

i made the decision to not use C over a long time.

right, we were talking about,
how do we multiplex over multiple processes instead of blocking?

and also how do i monitor readiness in a remote process?
epoll_wait, of course.

but presumably while doing that I also want to be able to service local requests.

so I inherently need this async interface at least for blocking syscalls.

and so I might as well have it for everything.

so the interface either becomes,
I take over your event loop and give you callbacks (which is unacceptable for integration into arbitrary applications),
or each system call is split into "call" and "response" parts.

at that point it's too complex to work with.

I could have a blocking interface which is only suitable for setting up child processes, sure.

so some kind of direct_spawn which creates the process,
then some more blocking calls for them.

but that would be too abstracted and too much work.
well, okay right.

so if I did that, which was my original consideration,
then I would need to define a custom protocol for serializing all system calls.

right now, instead, my protocol is simple,
and doesn't serialize system calls.

which means it's very easy to add new system calls - just define the interface for it,
and we're good; no need to add it to a protocol or anything.


so, let's say we avoid that,
by using my current technique,
being explicit about transferring memory and resources,
and using "near" and "far" pointers.

ok, so if we do that then...

a blocking interface could work maybe? let's see....

ok so sure, maybe we could do that.

but it wouldn't be a generically useful interface! i mean, i guess it would be fine, but...

i mean, you might as well use sfork then. if you were going to do that.

yeah, I would just do it with sfork instead.

so the ultimate conclusion of this constrained interface, just for process creation, is sfork.

but sfork is very limited.
but, sigh, I guess it can work.

so the entire point of this paper, then,
is that we can do direct-style process creation from a generic interface for cross-process operation.
if we only wanted to do direct-style process creation (for one process at a time, in a fairly constrained way),
then, we could use sfork.

well, sfork isn't quite perfect;
it doesn't let you do operations in the parent process.
it's kind of just an incremental improvement over fork/vfork.

and, like, doing operations in the parent process might mean doing async operations!
well, but they wouldn't be async in the child.
well, they might be!

** portability
   Yeah I guess a simple port to BSD could be viable. I mean...
   It's pretty dang hard to do all this.

   ok, so they have an rfork.
   and... we could wrap some basic syscalls and make a protocol for them...


** cleaned up
   So the basic operation we want to do is remote syscalls;
   naively, this just takes the form of transforming
   =int read(int fd, void \*buf, size_t count)= to
   =int read(process\* proc, int fd, void *buf, size_t count)=.

   The immediate issue with this is that it's a blocking interface.
   I can't service local requests while making one of these calls;
   these calls roundtrip to another process which might not respond promptly for any number of reasons,
   so calling this naively may block for a long time, which isn't acceptable for a complex application.
   We need an interface that we can integrate into an event loop.

   There are two options:
   We can transform every syscall into a callback,
   for example,
   =int read(process* proc, int fd, void *buf, size_t count, func_type callback)=,
   or we can split syscalls into "call" and "response" parts,
   for example, maybe,
   =void read_call(process* proc, int fd, void \*buf, size_t count)= and
   =int read_response(process* proc)=,
   or we can do something else; in any of these cases, it's no longer easy to use.

   Making things asynchronous in this way is necessary even in a toy example
   if we want to set up multiple processes at once,
   or if we want to perform an operation in the parent and child at the same time
   (such as connecting from the child to a listening socket in the parent).
   Those are capabilities that fork has, that we won't have.

   This is fine for C, and it can be integrated nicely into C daemons,
   but it's not easy to use.
   I could make asynchronous stuff easy to use by adopting, say, a coroutine system for C,
   and then forcing the user to use it,
   but then it's not a library, it's a framework that takes over your whole program.
   I could loosen the requirement that the calls not be blocking,
   but then it's not useful in a complex application.

   The problem is with C.
   Luckily, 1. people don't really like C these days,
   and 2. it's not actually useful to do this in C.
   No-one is writing their container orchestration system in C,
   or any other kind of application that would want to do complex process manipulation.
   Those have no need for C - except inasmuch as they're forced to use C today.
   There's precedent for interacting with Linux in a C-free, language-specific way (Go),
   so that's what I did.

   BTW, if we did loosen the requirement that calls not be blocking,
   then you might as well use my sfork library,
   which is yet another semantics for process creation,
   one which AFAIK no system has ever done before.
   https://github.com/catern/sfork
   But it's something I've considered and rejected,
   because it won't work for real systems,
   which are concurrent.

   Second off, supposing we're fine with a blocking interface,
   the naive interface still has the problem of serialization.
   To call, say, =int write(process\* proc, int fd, void *buf, size_t count)=,
   either buf is a local pointer and we have complete shared memory between us and the remote process,
   or buf is a local pointer and internally that call serializes the memory and copies it over,
   or we don't pass a local pointer.
   Among other reasons,
   sharing memory with a process at a different privilege level is insecure,
   so we can't assume that we'll always have shared memory.
   So we need to define the aforementioned serialization.
   This makes it difficult to 

   We can have a wrapper which serializes the memory, sure.
   It's not performant, but it's fine.

   What about the internal thing, then?
   Well.
   So say we have
   =int write(process\* proc, int fd, void *buf, size_t count)=
   where the buf is a remote one.
   How do we implement that?

   ok it's fine but,
   what about say, dup2?
   we can use high fds like we said we would originally

   yeah, no non-trivial application could work like this

** hmm
*** expressiveness
   ok so we need to be more explicit about...
   the purpose is,
   the more expressive technique.

   we
   I think this line about features that are difficult to exploit is good.

   and we should dig into that in the fork-style and spawn-style sections I guess?

   spawn-style, only a short section,
   since it's just straight up impossible.

   fork-style, we can talk about at more length.

   so we should actually talk about features that are hard to work with.

   yeah, obviously. naturally.

   let's discuss fork.
**** TODO come up with things which are difficult with fork!
     ok ok ok.

     let's do this.

     and theoretically we can do this in the evaluation?
     or the examples?
     or the background?
***** netns
     making a network namespace,
     creating a socket,
     then using it in the parent.
***** others
      nested network namespaces

      pid namespaces need to be nested inherently
      (because you can't unshare them)

      so yeah nesting is a good one
      although you *can* do it with fork.
*** python
   we should also maybe say that it's a consequence of the rsyscall libc for Python.

   like.

   We are able to express direct-style process creation by using the rsyscall library,
   which allows Python programs to perform cross-process system calls.
   Our implementation strategy can be employed for any language by porting the rsyscall library.

   if we lean into the Python-ness, we don't have to make excuses for it.


   Our contribution is an implementation of direct-style process creation for Linux,
   making use of our work on the rsyscall project.
   rsyscall is a cross-process Linux syscall library, replacing libc.
   rsyscall is currently best supported on Python.
** takeaways
   talk about novel applications in the intro
   
   be clear on the requirements of the mechanism

   (I think it's clear enough right now...)

   ok so the main thing right now is that we need to revamp this "Overview and examples" section.

   and possibly merge in the process background to the examples?

   we need to genuinely evaluate fork vs rsyscall.
   and show things that we can and can't do with fork.

   ok so we have these examples...

   maybe we can just bulk them up yeah.

   current organization:

   introduction,
   background,
   overview on rsyscall,
   examples.

   what we want to insert is:
   1. genuine evaluation of fork vs rsyscall
   2. novel techniques you can do with rsyscall
   3. things one might want to do with processes which are difficult otherwise

   I guess this is all done in one way:
   go through each process-use-case,
   and do it with both fork and rsyscall,
   and show the example.

   SIGH.

   ok fine let's do it.
   let's brew up the examples/sections,
   based on the process-applications.

   i obviously need this section, it's obvious;
   it will provide a reference for others,
   and make my paper eternal.
*** DONE abstracting over resources

\begin{lstlisting}[float,language=Python,label={fds},caption={Passing down FDs}]
child = local.thread.clone()
sock = child.socket(AF.INET, SOCK.DGRAM)
# Bind the socket to a sockaddr_in;
# the sockaddr is written to memory accessible by the child with child.ptr
sock.bind(child.ptr(SockaddrIn(0, 0)))
sock.listen(10)
sock.disable_cloexec()
child.execv(executable_path, [executable_path, "--listening-socket", str(int(sock))])
\end{lstlisting}
    
    

    This is further generalized by Linux's generic tr
    
    a child process can inherit file des
    Most fundamentally,
    

    Passing down a file descriptor

    can open a file and pass it down.

In Unix, the mechanism of file descriptor inheritance
allows a process to be provided a resource by its creator,
while abstracting over the precise identity of that resource.[fn:ucspi]
For example, a process can be provided a file descriptor,
which can correspond to any file in a filesystem,
without the process being aware of what specific file it is accessing.
This is further enhanced by Unix's "everything is a file" design;
the passed file descriptor could also be a pipe, a network connection, or some other resource,
without the process knowing.
As another example,
a process can be provided a socket file descriptor on which it can call \texttt{accept} to receive connections,
without being aware of whether those connections come from the internet or from a local Unix socket.[fn:ucspi]
This abstraction mechanism is the basic principle of pipelines and redirection in the Unix shell,
but it is rarely used outside of the shell.
*** DONE also, pull anything else out of the "background on processes" section
    ok extracted - i just wanted the bit about these techniques not being usable in combination with existing services.
*** DONE concurrency
    a pipeline!

    pipelines are good.

Processes run concurrently,
which allows exploiting the parallelism of the underlying hardware.
Since processes don't share memory,
they can provide a less complex parallel programming environment
than shared-memory thread-based approaches.

A popular parallel programming environment is the Unix shell,
which obtains its parallelism by running multiple processes connected via pipes.
The Unix shell has a relatively constrained form of parallel processing,
but it's also possible to create more complex webs of parallel processes,
where, for example, one process might take multiple inputs over multiple pipes,
or produce multiple outputs,
or the processes might be arranged in a loop.

# hmm what is a good example????

# encryption/decryption maybe? some kind of ssl terminator?
# ucspi requires 6/7, so maybe... that's a good idea?
# yeah I could do some kind of SSL terminator thing?
# and then pass the socketpair to... something else..

    In listing \ref{pipe},
    we create a bidirectional connection between two processes.
    hwere the one 
    between their stdout and stderr;
    something which is not possible in a POSIX shell.

    blah blah blah.

    This is, of course, substantially more complex than the Unix shell,
    but it's also substantially more capable;
    we can pass multiple channels of communication or other resources,
    and we can freely combine these webs of processes with other features that we'll discuss below,
    including Linux namespaces.

    # two additions to the spawn-style background:
    # shell and chainloading are spawn-style, 
    # and spawn-style requires a DSL

    but it allows composition of these techniques with other

    The most basic usage of this is in the widespread distribution of software
    as executables which run in dedicated processes.
**** what example?
     let's just do some made up programs again

     try to be evocative with naming;
     two streams between two processes, for example.

     hmmmmmm.

     is made up a good idea?
     yes.

     two streams of data, maybe going different places.

     ummmm

     maybe uhhh

     ok

     maybe we can think of some way in which direct style is actually helpful..?

#+BEGIN_SRC python
def as_argument(child, fd):
  child_fd = child.inherit_fd(fd)
  child_fd.fcntl(F.SETFD, 0)
  return str(int(child_fd))

audio_pipe = local_thread.pipe()
video_pipe = local_thread.pipe()
source = local.thread.clone()
source.execve('/bin/source', ['source',
  '--audio-output', as_argument(source, audio_pipe.write),
  '--video-output', as_argument(source, video_pipe.write)])
video_sink = local.thread.clone()
video_sink.execve('/bin/video_sink', ['video_sink',
  '--video-input', as_argument(video_sink, video_pipe.read)])
audio_sink = local.thread.clone()
audio_sink.execve('/bin/audio_sink', ['audio_sink',
  '--audio-input', as_argument(audio_sink, audio_pipe.read)])
#+END_SRC

In listing \ref{pipe},
we execute a few programs in parallel,
connected by pipes.
We first create two pipes,
then inherit them down to several processes using a helper function, \texttt{as_argument}.
*** DONE customization without support
    Using some kind of user namespace + mount namespace thing?

    some other customization thing?

    TODO this one is a bit tricky

    nested clone? network namespaces? pid namespaces?

    we should show something using nested clone;

    and we should show something using shared fd tables.

    binding a random port in the child is good but you can achieve that by fd passing.
    but not if you're in a network namespace! that's good then.

    that's a useful thing, sure.

    and, another one is, say...
    running a client outside the thing?

    oh we could do this with the sandboxing thing.

    ugh the miredo thing inherits fds though, sigh.

    ok so that's not helpful.

    customization: use a mount namespace, for sure.
    and override  .. ..... something.
    doesn't really need to be real I guess.
    /etc/foo or whatever.

    oh, and we can do it as a function.
    that would be a nice demonstration of abstraction.

    this is fine.

In many systems,
it's possible to modify a process's view of nominally "global" resources.
In Unix-derived systems, this ability is most influentially provided in Plan 9[fn:plan9],
which allows each process to customize its view of the filesystem with private mounts and union directories[fn:plan9ns].
In Linux, these concepts were implemented as per-process namespaces[fn:linuxns].
Fundamentally,
this allows customizing a process's environment and therefore a program's behavior,
without having to write explicit support code for customization.
For example, Plan 9, unlike most other Unix-derived systems,
did not have a \texttt{PATH} environment variable which was searched by code in the process to find executables;
instead, each process was executed with a \texttt{/bin} directory at the root of the filesystem,
which was a union of many other directories,
and simply executed \texttt{/bin/foo} to run the program named \texttt{foo}.
In this way the set of executables provided to a process could be customized,
without any code to parse and handle \texttt{PATH} or any other executable-lookup-specific customization code.







We'll bind mount a single file, sure.

ok one awkwardness is finding a filename.

should i do uts namespaces???
that seems plausible

yeah. that's good.
or no, it's boring.

let's use mkdtemp to make a tempdir?
yeah yeah ok.

no let's hardcode a path
perfect
*** DONE sandboxing

Capability-secure sandboxing.


    Lower privilege level thing, easy.

    hmmmmmm I think I should just combine sandboxing and capsec.

    or, maybe not!

    or maybe yes.

    it's really just a combination of abstracting over resources,
    and sandboxing.
    yeah I should skip it.

    ok so what kind of sandboxing can we do?
    let's be capsecish, and pass explicit resources.
    and... otherwise just lock it down completely?
    unmount all?

    and we can say, caveat, this isn't a full sandbox, but concise for the sake of space.
    yeah and this can be a static binary

    ok so we can fexec a static binary,
    along with one explicitly passed file,
    and unmount everything,
    and boom, we're good.
**** old
The basic isolation powers of processes are used to simplify application development:
it is beneficial to have a private virtual memory space when developing a stand-alone program.
But most systems have additional mechanisms of isolation between processes,
such as different privilege levels and access to global resources,
which can be used to provide a form of sandboxing.
For example, components which may exposed to hostile network requests
can be run in a separate process, at a lower privilege level than the main program;
in this way, even if an attacker gains control over that component,
the attacker will only have access to the lower level of privileges of that component,
rather than the full privileges of the main program.

As a further development of process-based sandboxing,
the privileges of a process can be explicitly enumerated
in a capability-based security model.[fn:capsicum]
By using previously-mentioned resource passing mechanisms,
such as file descriptor inheritance or namespace manipulation,
and by disabling the process's access to global resources such as the shared filesystem,
we can enforce that all resources used by the process are passed at creation time.
**** writing
At process creation time,
we can not only pass resources and customize the process's environment,
we can also deny the process access to resources that it otherwise gets by default.
This is a key part of creating a secure sandbox for potentially malicious code.

Historically, system calls such as \texttt{chroot} were the main mechanism for doing this;
today, Linux's namespaces system is the most important component.

\begin{lstlisting}[float,language=Python,label={unmount},caption={Unmount all and run executable via fexec}]
thread = local_thread.clone(CLONE.NEWNS)
executable_fd = child.open("/bin/static_executable", O.RDONLY)
db_fd = child.open("/var/db/database.db", O.RDWR)
child.umount("/", MNT.DETACH)
db_fd.fcntl(F.SETFD, 0)
child.fexec(executable_fd, ["static_executable", "--database-fd", str(int(db_fd))])
\end{lstlisting}

In listing \ref{unmount}
we create a new child thread,
again in a new mount namespace using \texttt{CLONE_NEWNS}.
We open the executable that we'll later exec in this child thread,
as well as another file as in listing \ref{fds},
since we won't be able to do exec or open files normally after the next step.
Then we use \texttt{umount},
passing \texttt{MNT_DETACH} to perform a recursive unmount of the entire filesystem tree,
removing it all from the view of this process.
As in listing \ref{fds}, we unset the \texttt{CLOEXEC} flag from the database file descriptor,
so that it can be inherited across \texttt{exec};
we don't need to call \texttt{inherit_fd} since, in this case, the database fd was opened directly from the child.
We then exec the file we opened earlier, using \texttt{fexec},
which allows execing a file descriptor,
and also pass the database file descriptor as an argument,
as in listing \ref{fds}.
(The underlying system call for \texttt{fexec} is \texttt{execveat};
this \texttt{fexec} is a helper function.)
Note that this executable must be statically linked,
or it wouldn't work in an empty filesystem namespace,
with no libraries to dynamically link against.

By removing the filesystem tree from the view of this process,
we can run this executable with greater confidence
that it won't be able to tamper with the rest of the system.
A sandbox which is truly robust against malicious or compromised programs requires some additional steps,
but this is a substantial start.
Such a technique can also be used when a full sandbox is not relevant,
to ease reasoning about the behavior of the program being run,
and protect against bugs.

Even if the process needs additional resources,
those can be explicitly passed down through file descriptor passing,
as we do here with the database file descriptor.
This is the essence of capability-based security,
which Linux can increasingly closely approximate.
*** DONE killing child processes?
    can demonstrate using a pid namespace for that?

    yeah that's good.

    although there's the signal restriction;
    does that matter?

    no not really.
    just use sigkill if you want to kill it.

    well, it is annoying that there isns't an interface...
    actually, you've always needed to fall back to sigkill ater sigterm.
    not doing that is a joke.

    zombies, also, are kept around?
    shrug, I'm mostsly fine with that.

    ok maybe this isn't a super good idea.

    our other wacky example can be one depending on a clone.

    aha yes this is good,
    because we can demo nested clone.

    things we're doing here:

    nesting 

    using a pid namespace to clean up resources


    Namespaces can also be used to.
**** writing
    Process-local resource can also be used to control the lifetime of resources used by that process.
    Many Linux resources are not automatically cleaned up on process exit;
    a poorly coded program may allocate global resources
    and not ensure that they are cleaned up,
    leaving behind unuseds litter on the system.

    One resource which is not automatically cleaned up is processes themselves;
    if we run a program which itself spawns subprocesses,
    those subprocesses may unintentionally leak,
    and be left running on the system even after the program itself has stopped.

    We can use a pid namespace, in conjunction with nested cloning, to solve this issue.
    The lifetime of all processes in a pid namespace is tied to the first process created in it,
    the init process.
    When the init process dies,
    all other processes in the pid namespace are destroyed.

#+BEGIN_SRC python
init = local_thread.clone(CLONE.NEWPID)
grandchild = init.clone()
grandchild.execv(foo_path, [foo_path])
#+END_SRC    

    In listing \ref{pidns},
    we create a child process which we call \texttt{init},
    passing \verb|CLONE_NEWPID| to create it in a new pid namespace.
    To create another process in the pid namespace,
    we clone again, this time from \texttt{init};
    note that this is the first time we've cloned from one of our child processes.
    We exec in the grandchild,
    and we can monitor the grandchild process from \texttt{init},
    just as we would monitor \texttt{init} from its parent process.

    \texttt{init} in a pid namespace has some special powers and responsibilities.
    We can handle these responsibilities ourselves, directly from this program,
    or we can continue on by execing an init daemon in \texttt{init} to handle it for us.

    In either case, when \texttt{init} dies,
    the pid namespace will be destroyed,
    and \texttt{grandchild} will be cleaned up.
*** DONE fuse and shared fd table
    make ns thread from root thread,
    open dev fuse,
    make server thread from root thread,
    pass down dev fuse,
    mount dev fuse,
    open file,
    use in root thread.

    okaaaay.

    this should be good.

    I guess I can say, uh.

    well one attribute is Linux's ability to make usersspace filesystems
    and otherwise mock file descriptors.

    another is running a subprocess to run a service.

    yet another is sharing the file descriptor table.
    # !!! TODO !!!
    # we could just hardcode the paths of everything instead of assuming they're in scope;
    # that's probably better.

    ok so this is using a subprocess to perform a task, that's the key thing.

    fo rhtat, sharing the fd table is useful.

    (incidentally, a usersspace filesystem is useful, but not super useful)

    A library can create a child process to perform a useful service.
    This can sometimes be done more easily when the child process shares file descriptors with the parent.

#+BEGIN_SRC python
ns_child = local_thread.clone(CLONE.FILES|CLONE.NEWNS)
server_child = ns_child.clone()
await server_child.execve(fuseserver_path, [fuseserver_path, "/foo"])
fd = ns_child.open("/foo/bar", O.RDONLY)
parent_fd = local_thread.use_fd(fd)
#+END_SRC

    In listing \ref{fuse},
    we first create a child process, \texttt{ns_child}, in a new mount namespace with \texttt{CLONE_NEWNS},
    and for the first time, also pass \texttt{CLONE_FILES},
    which causes the file descriptor table to be shared between the parent process and the child process.
    We create a grandchild off of \texttt{ns_child},
    and use it to exec a FUSE filesystem,
    which will appear only in the mount namespace of \texttt{ns_child} and its descendents.
    Then we can open FUSE files in \texttt{ns_child}
    and use those files in the parent process,
    through the shared file descriptor table.

    # TODO hmm technically we could open devfuse from local_thread, that would work too.
*** misc
    need to find a space for these:
**** shared fd tables
    we can create sockets after process creation and then share them;
    like we talked about earlier,
    we can do:
    create process
    create socket
    use socket.

    we can also use the socket in the parent I guess?

    oh we can pass it down to another process in another namespace, nice, perfect

    oh yeah, actually, for that matter can we create the netns thread,
    then the client thread.

    that would be cool

    yeah that saves a thread.

    and we'll split it between the two

    perfect, and CLONE.FILES does work with newuser.

    so let's rework this

    that's complex though

    let's consider just NEWNET and NEWNS.

    so we can use the fd in the parent

    which is ifne...

    there's a complex use case, which is,
    open the file in one task in a new namespace,
    and then pass it down to another task not in that namespace.

    let's see. hm.
    this isn't necessarily something that fits into our existing use cases.

    maybe I can just skip it?

    let's just skip it for now, I'm avoiding the issue

    nope! got it!

    just mount a fuse filesystem inside,
    and use the mounted file from the outside.
    boom!
**** DONE nested clone
     This is handy, mmm.

     can we think of a way to handle this that doesn't require a bind?

     yeah, let's figure out how to get rid of the .ptr.

     we can't viably use network stuff if we do that. but I guess it's fine.

     ok so we probably should do nested clone before the clone_files thing.

     need to think of a good nested clone.
     a pid namespace, maybe?

     oh, yeah, let's do a pid namespace.
     and have our process as pid 1 in the namespace.
     that works!

     and that can be about, resource cleanup, I guess?
*** thoughts
    maybe we don't need to include each comparison in the paper?

    maybe we don't need to put the comparisons right next to these examples?
    we can do that later, in evaluation?
    although it would be helpful; we'll include one.

    ok, that seems good, seems good.

    um, except,
    the entire point of having the comparisons right adjacent,
    is so we can show off advanced techniques.
    
    OH OH OH FUSE MOUNTS YES OK

    perfect!

    ok so that's how I can use the shared fd tables!
    just mount a fuse filesystem inside,
    and use the mounted file from the outside.

    oh and we need nested clone to do that, too.
    well, to do it sanely.

    oh, hey, we can use it multiple times, actually.
    we can make the ns,
    then clone the server off the root thread,
    using the shared fuse fd.

    ok so that's a good example.
*** design
    ok so I guess we'll stay with this overview and examples design?
    let's look at other papers.

    oh i should have my end be called "conclusions"

    ok I guess I could have "overview",
    then "Applications".

    Overview can have this one create and exec example.

    Yeah, that makes sense.
** evaluation
    We quantitatively answer the following questions:

    - Does direct-style process creation on Linux allow for simpler programs than the alternatives?
      We implement and compare similar functionality using both direct-style and fork-style.

    - Does a clone-based implementation avoid the well-known performance overheads of fork?
      We evaluate clone against fork on several microbenchmarks.

    - Does a high-level Python direct-style process creation interface
      have acceptable performance relative to the Python standard library's spawn-style interface?
      We benchmark direct-style process creation against spawn-style.

*** let's do this
    OK, so the clone versus fork benchmark should be an easy one.

    Also the implementation of things with fork.

**** clone vs fork benchmark
** related work
   need a related work section I guess.

   it's just mandatory, even if it's not closely related.

   we can talk about cross-process syscall techniques,
   and about direct-style process creation.

   criu

   that other rsyscall paper about forwarding syscalls

   transparent migration of linux machines for SSI,
   such as popcorn linux,
   typically forward syscall back

   that google sandboxing thing

   io_uring

   this thing http://madchat.fr/coding/c/c.seku/SyscallProxying.pdf

   "condor's remote system calls"

   process capabilities, is something we're saying a lot
   guess I should distinguish myself from pidfd
*** outline
    couple sections of related work:

**** direct-style process creation
     As we mention in section \ref{background},
     many academic operating systems have direct-style process creation.
     We are not aware of any previous instances of direct-style process creation in a Unix-like environment.

     The closest related work known to us
     is efforts to build Unix compatibility environments, including fork-style process creation,
     on academic operating systems which natively use direct-style process creation.
     Such compatibility environments could theoretically be bypassed to perform direct-style process creation
     using the underlying primitives,
     but regular Unix system calls could not be used to create new Unix processes in a direct-style way
     in such an environment.
**** remote syscall techniques
     There are a large number of Unix-based systems which do something called "remote system calls".
     Most such systems do not allow a single program to manipulate multiple processes.
     Those systems that do allow manipulating multiple processes
     are generally oriented towards debugging and introspection,
     and are unsuitable for a general purpose system.

     Many systems use system call forwarding to implement migration in a computing cluster.
     HTCondor and Popcorn Linux are two examples.
     In both systems, processes can be live-migrated between hosts;
     when this occurs, the system will automatically forward IO-related system calls
     back to the original host.
     
     Some systems intercept system calls and forward them to another process to be implemented
     as part of a virtualization system.
     gvisor\cite{gvisor} and ViewOS\cite{viewos} are two examples.
     In both systems,
     system calls made in one process are intercepted,
     and serviced by another process.
     Also relevant is seccomp system call interception,
     which can be used to intercept system calls and emulate them in another process.

     Some systems forward system calls elsewhere for asynchronous execution.
     FlexSC is one example.
     System calls were forwarded to dedicated system call execution threads.
     The recently added Linux feature io_uring is, in some sense, another example;
     it supports sending a subset of system calls to the kernel,
     which executes them asynchronously,
     rather than blocking the thread.

     Some systems call system calls in other processes for debugging or introspection purposes.
     strace, gdb, and CRIU are examples of this.
     These systems typically uses ptrace,
     which is capable of operating on multiple processes at once.
     Unfortunately, as discussed in section \ref{implementation},
     ptrace is unsuitable as a mechanism for a general purpose system since it is not re-entrant;
     a process which is already being ptraced cannot be ptraced again,
     so using ptrace as a cross-process system call mechanism would disallow use of strace or debuggers.
**** process capabilities
     # We mention the concept of process capabilities while discussing the design of rsyscall in section \ref{overview}.
     Some notion of "process capabilities" has been recently added to Linux in the form of the pidfd API,
     which was directly inspired by Capsicum's process descriptors.
     This notion of process capability is quite limited, however;
     pidfds and process descriptors only allow sending signals to a process or waiting for its death,
     rather than exercising full control over the process.
**** capsec libc alternatives
     The Capsicum system provides a set of system calls
     which can be used to provide a capability-secure sandbox.
     Latter efforts[fn:oblivious], including CloudABI[fn:cloudabi] and WASI[fn:wasi],
     are focused primarily on backwards compatibility with the existing POSIX environment, and ease of porting existing code.
     Programs ported to the capability-secure environment
     are typically launched with an accompanying spawn-style wrapper.
     This means that capability-secure processes are created primarily by an API
     that is distinct from the capability-secure syscall API they have access to,
     which makes it harder for a capability-secure process to spawn further capability-secure processes of its own.
     Direct-style process creation could make it easier to create capability-secure processes 
     from the capability-secure syscall API,
     and reduce the need for a separate spawn-style wrapper.

     Also related is the Principle of Least Authority Shell (PLASH).
     PLASH provides a spawn-style API (in the form of a shell)
     to launch processes in a capability-secure environment.
     PLASH, like all spawn-style APIs, abstracts over the native Linux environment,
     and is therefore limited in what kind of processes it can create.
* questions
  page length? 14 too much?

  citations? do I need them now?
* things to do
  ok, let's strip out the "thread" term.
  it's too confusing,
  and I don't want to unify.
