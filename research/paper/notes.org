* papers
** rsyscall: A cross-process syscall API for Linux
** cross-host operations with rsyscall
** asynchronous IO with rsyscall thread pool (flexsc style)
* what to do after writing
1. send to labs team at TS for review
2. send to fork paper guy for review
3. send to professors as Kai says and ask about research opportunities
* Direct-style process creation on Linux
** title
"direct-style" just being a nicer way to say "imperative"

"imperative process creation" doesn't sound good because it says "imperative"
** notes
*** features to emphasize
- it covers everything you might want unlike posix_spawn
- it's immediately deployable
- it's efficient
- it has a coherent theory behind it and is relatively easy to use
** abstract
Traditional process creation interfaces are complex to use, limited in power, and difficult to abstract over.
We develop an alternative process creation interface for Linux
which allows a program to create a new process in an inert state
and then initialize it from the outside by directly operating on it
in the same style as any other operating system object.


This method of process creation results in more comprehensible programs, 
has better error handling,
is more efficient,
and is more amenable to abstraction.
Our implementation is immediately deployable without kernel modifications on any recent Linux kernel versions.
** introduction (existing process creation mechanisms)
Processes are a widespread feature of operating systems,
with substantial variation in characteristics between systems.
One of the areas of variation is in process creation mechanisms.

On most systems today,
the interfaces for process creation
can be divided into "fork-style" and "spawn-style".
A few systems use a third style, which we'll refer to as "direct-style".
*** fork-style
Fork-style interfaces are those that follow the model of fork.
A process calls fork, and a copy of that process is made,
with the return value of fork being the only differentiation between the process and its copy.
The same program continues executing in both processes.
Typically, the program will case on the return value to determine if it's running in the parent or the child,
and run process setup and initialization code if the program is running in the child,
calling various syscalls to set up the state of the child.

Fork has a number of flaws,
and much ink has been spilled detailing them.
We'll cover the most prominent briefly here,
but see Baumann 2019 for a more detailed summary.

Fork returns twice, creating two different processes which execute concurrently.
This immediately makes fork difficult to integrate;
for example, errors can't be easily communicated between the two processes.
If there is an error in the initialization of the child process,
the part of the program running in the child process
needs to use some form of inter-process communication (IPC) to communicate it back to the parent process.
The child process can run arbitrary code to make decisions about its initialization,
but that code needs to use IPC if it wishes to communicate with the rest of the program,
which is still in the parent process.

The most common complaint about fork is its poor performance.
It copies many attributes about the parent process when creating the child process,
including setting up copy-on-write memory-mappings in the child process.
This becomes slower as the parent process has more memory-mappings,
eventually taking a significant amount of time.

Multi-threaded programs generally cannot use fork safely.
Typical Unix fork implementations do not duplicate all threads in the parent process to the child process,
so, for example, an important lock might be locked at the time of the fork and then never be unlocked,
causing a deadlock.
In combination with fork's poor performance in large-memory programs,
a user of fork must think carefully
about the characteristics of the process from which fork is being called.
**** notes
expand this
- it's weird to return twice
- errors and fallbacks can't be communicated across fork
- it's fundamentally slow
- it's weird and sucks
- fork can't be used in many scenarios, such as in:
    - large memory programs
    - multithreaded programs
    - programs using unusual fork-unaware libraries such as CUDA or kernel networking bypass

What does spawn solve? performance I guess
we should make sure to clarify. hm. or something.
*** spawn-style
Spawn-style interfaces are those that follow the model of =posix_spawn= or Windows' =CreateProcess=.
All the details about the new process are provided up-front as arguments to a syscall,
which creates the new process from a clean slate, initialized with the provided details.

Since spawn-style interfaces don't copy details from the parent process,
they don't have the performance problems of fork.
However, they have significant flaws of their own.

The arguments that can be provided to a spawn-style process creation syscall
do not cover all the possible attributes that one might want to set for the new process.
Most systems have a large number of syscalls which can mutate the state of a process during its lifetime;
for a spawn-style interface to work in all scenarios,
all those possible mutations must be reproduced in the interface.

Spawn-style process creation also does not allow for conditional logic during the setup;
if the setup of the new process encounters an error at some point,
the only option is to return from the entire spawn call with an error.
Such errors returned from spawn-style calls
are typically much less informative
than the errors returned by the syscalls which directly mutate the process attributes.
In general, a spawn-style interface does not allow for conditional logic during the process setup;
a modification to the process cannot depend on the result of some other modification.
**** notes
need to list more flaws I guess
- limited number of modifiable things
- limited expressiveness (conditionals?)
- error handling (haven't included this one yet!)

seems like in multics, process creation was a privileged operation
*** direct-style
A few systems, such as KeyKOS, seL4 and other capability-based operating systems,
use another style of process creation.
In this style, a process is created by its parent in an inert state,
and then supplied with various resources,
and then started once it is fully set up.
The same mechanisms that can mutate a process while it is running,
are used to mutate the process while it is in an inert state;
in such systems, these mechanisms can be used on the process from *outside* the context of the process,
just as easily as they can be used from inside the process.

We refer to this as "direct-style" process creation,
because the parent creating the process operates on it directly and imperatively
rather than dispatching a distinct unit of code to perform setup from inside the context of the new process,
as in fork-style,
or building up a declarative specification of what the new process should look like,
as in spawn-style.

Direct-style process creation has significant advantages over fork-style and spawn-style process creation.

Unlike fork-style, the new process can be created in a clean, empty state,
which removes the performance issues of copying the parent.
The new process does not actually run any user code,
so multi-threaded programs can safely create new processes without fear of deadlocks.

Unlike spawn-style,
the new process can be manipulated with all the normal mechanisms for process manipulation,
so there is no need to create a duplicate spawn interface that allows specifying every possible attribute of a process.

Unlike both fork-style and spawn-style,
direct-style process creation operates on the process from the context of the main program,
using individual syscalls.
Thus, an error in some step of the initialization is reported to the main program like an error in any other syscall,
and coordination between a component creating a process and the rest of the program requires no IPC.
**** notes
     SEL4 has this style.
     https://docs.sel4.systems/Tutorials/threads.html
*** TODO we've made a new process creation interface that doesn't suck
We have adapted direct-style process creation for Linux.

In the rest of the paper,
we will examine the design and implementation of this style of process creation on Linux,
and demonstrate its usage.
# TODO need to clean up this, should make a nice outline here
** background on the use of processes
Why is it important to have a high-quality interface for creating processes?
Processes are already widely used;
most software is distributed as an executable which runs in a dedicated process.
This basic usage of processes can be performed with even a complex and inefficient process creation interface.
But processes have many uses beyond this simple and widespread one;
here we examine some more sophisticated applications of processes,
which benefit from a better process creation interface.
*** file descriptor inheritance allows abstracting over resources
In Unix, the mechanism of file descriptor inheritance
allows a process to be provided a resource by its creator,
while abstracting over the precise nature of that resource.
For example, a process can be provided a file descriptor which it is expected to read and write,
which can be a file in a filesystem, a pipe, a network connection, or some other resource,
without the process being aware of the type of resource it has been passed.
As another example,
a process can be provided a socket file descriptor from which it is expected to accept connections,
without being aware of whether those connections come from the internet or from a local Unix socket.
This mechanism is the basic principle of pipelines and redirection the Unix shell,
but it is rarely used outside of the shell.
**** notes
     mention ucspi

     and inetd
*** namespace modification allows customization without explicit support
In many systems,
it's possible to modify a process's view of nominally "global" resources.
In Unix-derived systems, this ability is most influentially provided in Plan 9,
which allows each process to customize its view of the filesystem with private mounts and union directories.
In Linux, these concepts were implemented as per-process namespaces.
In its most basic application,
this allows customizing the environment of a process without having to write explicit support code for customization.
For example, Plan 9, unlike most other Unix-derived systems,
did not have a =PATH= environment variable which was searched by code in the process to find executables;
instead, each process was executed with a =/bin= directory at the root of the filesystem,
which was a union of many other directories,
and simply executed =/bin/foo= to run the program named =foo=.
In this way a process could get a customized set of executables,
without needing code to parse and handle =PATH=.
*** TODO privilege separation allows sandboxing
The basic isolation powers of processes are used to simplify application development:
it is beneficial to have a private virtual memory space when developing a stand-alone program.
But most systems have additional mechanisms of isolation between processes,
such as different privilege levels and access to global resources,
which can be used to provide a form of sandboxing.
For example, components which may exposed to hostile network requests
can be run in a separate process, at a lower privilege level than the main program;
in this way, even if an attacker gains control over that component,
the attacker will only have access to the lower level of privileges of that component,
rather than the full privileges of the main program.
*** robust privilege separation and resource privion allows capability-based-security
As a further, more robust development of process-based sandboxing,
the privileges of a process can be explicitly enumerated
in a capability-based security model.
By using previously-mentioned resource passing mechanisms,
such as file descriptor inheritance or namespace manipulation,
and by disabling the process's access to global resources such as the shared filesystem,
we can enforce that all resources used by the process are passed at creation time.
**** notes
     we separate "regular" sandboxing and capability-based security
     because lots people won't understand they're the same thing

     mention capsicum
*** non-shared-memory concurrency allows exploiting parallelism in a simple way
# TODO not sure about this one
Processes run concurrently,
which allows exploiting parallelism in the hardware.
Since processes don't share memory,
they can provide a less complex parallel programming environment
than shared-memory thread-based approaches.
The most popular parallel programming environment in existence today is the Unix shell,
which obtains its parallelism by running multiple processes connected via pipes.
The Unix shell has a relatively constrained form of parallel processing,
but it's also possible to create more complex webs of parallel processes,
where, for example, one process might take multiple inputs over multiple pipes,
or produce multiple outputs.
**** notes
     really, non-shared-memory concurrency is a better fit with reality

     processors don't share memory! they should explicitly copy, that's reality

     where's that dang datastructure server paper
*** services?
    don't think i really want this section

 - Failure monitoring? Concurrency control? Concurrency in general? Service-oriented distributed systems?
   Shared-nothing message-passing concurrency? (aka "distributed systems")

*** conclusion
These techniques, and more, are available through the process interface.
Most software would benefit from abstraction over resources, sandboxing, and parallelism.
Yet these features of processes are used only rarely.
There are multiple reasons for this,
but one of the primary reasons is the complexity of current process creation interfaces.

Many of these techniques are used today by specialized software and services.
Often, such software only allows use of one of these techniques;
for example, the Unix shell allows piping together process, but not namespacing them;
container systems allow sandboxing processes, but not piping them together.
By delegating these features to specific separate services,
we lose the ability to use them in combination.

By improving the process creation interface,
we can make it possible both for programs to directly manipulate processes to use these techniques,
and to use and share composable libraries which use these techniques.
We believe this potential justifies the investment of substantial effort
into improving the process creation interface.
**** notes
should establish more that applications don't use these features

- We delegate many of these features to specific separate programs/servers to abstract over them,
  which means we can't use these features in combination.
  - shells, container engines, process supervisors
- If we make it simpler to create processes, we can increase our usage of these features, including in combination
  - This will also make it possible to replace separate programs running as system servers, with libraries
*** notes
    remember: talk about the features we use in the demos in this process-features section!
    and likewise, demonstrate the features we talk about in this section, in the demos sectioN!
** overview/example
   To implement direct-style process creation on Linux,
   we need to be able to call syscalls which operate on a child process,
   from a program which does not run in the child process.
   Given a system for such cross-process syscalls,
   we can create a child process in a sufficiently inert state using existing Linux functionality,
   and then mutate it through various syscalls,
   until it reaches the desired state,
   at which point we can call =execve= on the child process to start it running.
   After that, the child process functions like any other child process,
   and can be monitored using normal Linux child monitoring syscalls,
   such as =waitid=.

   The API for such cross-process syscalls depends on the language;
   in an object-oriented language,
   a syscall naturally takes the form of a method on an object containing a handle for a process.
   When a process is created,
   an object is returned,
   upon which exist methods for all Linux syscalls,
   and which perform those syscalls such that they manipulate the specific process wrapped by this object.
   A program written in an object-oriented language
   creating processes in direct-style
   is then a normal imperative program creating and mutating objects.

   We'll give all our examples of direct-style process creation in object-oriented Python;
   some Python-specific syntax has been removed for clarity,
   but the examples are otherwise real working code.
*** Terminology: A thread is a process controlled by a single program
    From the perspective of our programs,
    there are multiple processes which are under its control.
    "A process which is under my control" is a mouthful;
    we use the term "thread" to refer to all such controlled processes,
    including the main "thread" on which the program is running.
    On Linux, the shared-memory "threads" provided by libraries such as pthreads
    are implemented as processes,
    and their lifetime and execution is completely controlled by a single program;
    the same is true of our controlled-process "threads".
    The most significant difference is that our "threads" do not run their own code concurrently with the main program;
    nevertheless our "threads" do provide the opportunity for parallel execution of system calls,
    and so the terminology provides useful intuition.
    We will therefore use the term "thread" to refer to these controlled processes throughout the rest of the paper.
*** basic example
In \listing{basic}, we simply create a new process under our control (a thread)
and immediately exec a binary with it.
As exceptions are used for error-handling in the Python API,
there is no need for error-checking code.

#+BEGIN_SRC python
# Use the clone syscall in the local thread to create a new thread;
# we use a wrapper that supplies defaults for all arguments.
child = local.thread.clone()
# Call execve to run a different executable in the child thread;
# We pass the executable path as the first argument in the argument list, as is traditional.
# We use a wrapper that defaults envp to an unchanged environment, so we don't pass envp.
child.execve(hello_path, [hello_path])
#+END_SRC
**** introducing
thr

we refer to processes as threads.

hmm maybe we shouldn't
no it's fine


clone

not using:
execve returining the child process

not actually waiting on the child.
*** passing down fds
In \listing{fds}, we create a new thread,
then create a listening socket bound to a random port in that thread,
then call exec, 
passing down the socket by disabling cloexec and passing its file descriptor number as an argument to the new program.

File descriptors, here, are object oriented and have relevant syscalls as methods.
They make syscalls in the process they are created in by default;
we can create more objects referring to the same file descriptor from different processes
if we want to make the syscalls from another process.

#+BEGIN_SRC python
child = local.thread.clone()
sock = child.socket(AF.INET, SOCK.DGRAM)
# bind the socket to a sockaddr_in;
# the sockaddr is allocated in memory with child.ptr and is garbage collected
sock.bind(child.ptr(SockaddrIn(0, 0)))
sock.listen(10)
sock.disable_cloexec()
child.execve(executable_path, [executable_path, "--listening-socket", str(int(sock))])
#+END_SRC
**** introducing
disable_cloexec
socket creation
using int on sock
.ptr

not using:
.args
as_argument
*** piping
In \listing{pipe},
we do the same as the Unix shell pipeline "yes | head -n 15".
We create a pipe,
then create two threads,
connect them with a pipe,
and exec a different program in each thread.

After a process is created with clone,
it may have inherited file descriptors;
here we inherit the pipe.
We make this inheritance explicit with =inherit_fd=,
a helper method on our thread object,
which takes a file descriptor from a different thread
and performs a runtime check that the file descriptor actually was inherited.
If so, it returns a new handle to the file descriptor which performs syscalls from the new thread.

Then we simply =dup2= as normal to replace child1's stdout with the write end of the pipe;
=dup2= disables CLOEXEC by default on the target.

#+BEGIN_SRC python
# create the pipe
pipe = local.thread.pipe()
child1 = local.thread.clone()
# inherit the write-end of the pipe to child1, and replace child1.stdout with it
child1.inherit_fd(pipe.write).dup2(child1.stdout)
child1_proc = child1.execve(yes_path, [yes_path])
child2 = local.thread.clone()
# inherit the read-end of the pipe to child2, and replace child2.stdin with it
child2.inherit_fd(pipe.read).dup2(child2.stdin)
child2_proc = child2.execve(head_path, [head_path, "-n", "15"])
#+END_SRC
**** introduce
dup2
pipe

not using:
syscalls returning the buffer passed into them
malloc
*** mount namespace
In \listing{mount},
we make a new mount namespace and rearrange the filesystem tree for the child process.
We bind-mount /proc at /proc inside the chroot directory,
chroot into the directory,
and exec an executable which will run inside the chroot.

#+BEGIN_SRC python
child = local.thread.clone(CLONE.NEWUSER|CLONE.NEWNS)
child.mkdir(rootdir/"proc")
child.mount(Path("/proc"), rootdir/"proc", "", MS.BIND, "")
child.chroot(rootdir)
child.execve(executable_path, [executable_path])
#+END_SRC
**** introduce
namespaces

slashes in paths

do we really need this? yeah it's nice, shrug
*** nested clone and network namespace
In \listing{nested},
we make a process (=ns_thread=) in a new network namespace.
Then, we create two more child processes of =ns_thread=,
which are also in the new network namespace.
This nested creation of child processes is fully supported,
like all other syscalls,
and allows us to set up complex graphs of processes and namespaces.

We bind to a privileged port on localhost inside the namespace,
and create one child to listen on that socket,
and another child to connect to it.

#+BEGIN_SRC python
ns_thread = local.thread.clone(CLONE.NEWNET|CLONE.NEWUSER)

listening_child = ns_thread.clone()
sock = listening_child.socket(AF.INET, SOCK.DGRAM)
sockaddr = SockaddrIn(22, "127.0.0.1")
sock.bind(listening_child.ptr(sockaddr))
sock.listen(10)
sock.disable_cloexec()
child.execve(server_path, [server_path, "--listening-socket", str(int(sock))])

connecting_child = ns_thread.clone()
child.execve(client_path, [client_path, "--connect-address", str(sockaddr.address) + ":" + str(sockaddr.port)])
#+END_SRC
**** introduce
network namespace
nested clone

Nested clone example: client and server inside net namespace using fixed localhost port
*** TODO miredo
In \listing{miredo},
we show non-trivial code for launching a real application:
the Miredo IPv6 tunneling software.
We use a few helper functions in this listing to keep the attention focused on the interesting parts.

Miredo is separated into two components, a privileged process which sets up network interfaces,
and an unprivileged process which talks to the network.
With minimal modifications to Miredo,
we launch Miredo entirely unprivileged inside a user namespace and network namespace,
with all resources created outside and explicitly passed in.

#+BEGIN_SRC python
### create socket outside network namespace that Miredo will use for internet access
inet_sock = local.thread.socket(AF.INET, SOCK.DGRAM)
inet_sock.bind(local.thread.ptr(SockaddrIn(0, 0)))
# set some miscellaneous additional sockopts that Miredo wants
set_miredo_sockopts(local.thread, inet_sock)
### create main network namespace thread
ns_thread = local.thread.clone(CLONE.NEWNET|CLONE.NEWUSER)
### create in-network-namespace raw INET6 socket which Miredo will use to relay pings
icmp6_fd = ns_thread.socket(AF.INET6, SOCK.RAW, IPPROTO.ICMPV6)
### create in-network-namespace socket which Miredo will use for unassociated Ifreq ioctls
reqsock = ns_thread.socket(AF.INET, SOCK.STREAM)
### create and set up the TUN interface
tun_fd, tun_index = make_tun(ns_thread, "miredo", reqsock)
### create socketpair which Miredo will use to communicate between privileged process and Teredo client
privproc_pair = ns_thread.socketpair(AF.UNIX, SOCK.STREAM)
### start up privileged process which manipulates the network setup in the namespace
privproc_thread = ns_thread.clone()
# preserve NET_ADMIN capability over exec so that privproc can manipulate the TUN interface
# helper function used because manipulating Linux ambient capabilities is fairly verbose
add_to_ambient_caps(privproc_thread, {CAP.NET_ADMIN})
# privproc expects to communicate with the main client over stdin and stdout
privproc_side = privproc_thread.inherit_fd(privproc_pair.first)
privproc_side.dup2(privproc_thread.stdin)
privproc_side.dup2(privproc_thread.stdout)
privproc_child = privproc_thread.execve(miredo_privproc_executable_path, [
    miredo_privproc_executable_path, str(tun_index)
])
### start up Miredo client process which communicates over the internet to implement the tunnel
# the client process doesn't need to be in the same network namespace, since it is passed all
# the resources it needs as fds at startup.
client_thread = ns_thread.clone(CLONE.NEWUSER|CLONE.NEWNET|CLONE.NEWNS|CLONE.NEWPID)
# lightly sandbox by unmounting everything except for the executable and its deps (known via package manager)
unmount_everything_except(client_thread, miredo_exec.run_client.executable_path)
# a helper function for preparing the fds that are passed as command line arguments
async def pass_fd(fd: FileDescriptor) -> str:
    client_thread.inherit_fd(fd).disable_cloexec()
    return str(int(fd))
client_child = client_thread.execve(miredo_client_executable_path, [
    miredo_client_executable_path,
    pass_fd(inet_sock), pass_fd(tun_fd), pass_fd(reqsock),
    pass_fd(icmp6_fd), pass_fd(privproc_pair.second),
    "teredo.remlab.net", "teredo.remlab.net"
])
#+END_SRC

**** introduces
socketpair
several helper functions
**** notes
ok so maybe I should remove some of the unnecessary networking stuff?
actually we can come back to this later

ok I feel like we need more detailed description or something.

actually it would be strongly beneficial to show what the corresponding fork-based code looks like.
ah maybe I shouldn't actually do the gloating "wow we're so simple"

maybe I should do a nested clone before this? yeah for sure actually.

OK so that's a big, clear todo:
do a nested clone before here.
what's the use of nested clones? well, namespaces are one.

guess I could do that in the mount namespace?
I could do a pipe in the mount namespace.
i could intro some other namespace; newnet or newpid maybe

Nested clone example: client and server inside net namespace using fixed localhost port
** implementation
*** basics about rsyscall
Our main need for implementing direct-style process creation
is a robust system for cross-process syscalls.
We provided this in the rsyscall project.
rsyscall is a toolkit for cross-process syscalls on Linux,
with several language-specific library implementations.

In this section, we'll give a brief overview of rsyscall,
and focus on implementation issues specific to process creation.
# no need to mention more detailed papers here

rsyscall can be conceptually divided in two parts:
the basic cross-process syscall primitive,
and a language-specific library built on top
to handle the complexities of manipulating resources across multiple processes.
The Python language-specific library has already been demonstrated above.
Such libraries only need to be able to call syscalls and explicitly specify a process in some way;
they are, for the most part, agnostic to how the cross-process syscall is implemented.

Using the Python library as an example,
it provides Python wrappers for Linux system calls and structs
which are type-safe using Python 3 type annotations and runtime checks
while still providing low-abstraction access to a large subset of native Linux functionality.
It also provides garbage collection for remote file descriptors, memory and other resources.
Such features are independent of the precise implementation of the cross-process syscall primitive.

On Linux x86_64, a syscall is specified by a syscall number plus six register-sized arguments;
a syscall returns one register-sized value.
rsyscall's default implementation of cross-process syscalls sends those seven integers over a pipe,
and waits for a response on another pipe.
Processes are created running an infinite loop which, at each iteration,
reads a syscall request off the pipe,
performs that syscall,
and writes the return value back over the return pipe.
In this way, a cross-process syscall works much like a very primitive remote procedure call.

Many syscalls either take or return pointers to memory,
and require the caller to read or write that memory to provide arguments or receive results.
Therefore, an rsyscall library needs a way to access memory in the target process.
We implement this through another set of pipes,
by explicitly copying memory into and out of those pipes using the =read= and =write= system calls.
When we wish to read =N= bytes of memory at address =A= in the target process,
we first perform a =write(memory_pipe, A, N)= in the target process,
and then read that data off the other end of the pipe in the parent process.
When we wish to write =N= bytes of data at address =A= in the target process,
we first write that data to the pipe in the parent process,
then perform a =read(memory_pipe, A, N)= in the target process to copy that data from the pipe into memory.

ptrace provides an alternative means to perform arbitrary actions on other processes.
However, among other issues, it has the unavoidable substantial disadvantage of not permitting multiple ptracers.
A ptrace-based implementation would prevent using strace or gdb on rsyscall-controlled processes,
which is an unacceptable limitation for a general-purpose utility.

The =process_vm_readv= and =process_vm_writev= system calls
allow the caller to read and write memory from the virtual address space of other processes.
However, they require that the caller have specific credentials relative to the process being accessed,
which may not always be the case.
Additionally, these system calls are disabled if ptrace is disabled system-wide,
which is a niche but possible system configuration.
To ensure that rsyscall can be used for arbitrary purposes and on arbitrary systems, we avoided these calls.
**** notes
     maybe it would be beneficial to separate
     "rsyscall the basic cross-process syscall primitive"
     from
     "rsyscall the python library".

     
*** clone
Now that we've established the basic operations which rsyscall provides,
let's consider the specific issues related to process creation and initialization.

There are three Linux system calls which create processes:
=fork=, =vfork= and =clone=.
=clone= provides a superset of the functionality of the other two,
so we focused our attention on =clone=.

=clone= (along with =fork=) creates a new process
which immediately starts executing at the next instruction after the syscall instruction,
in parallel with the parent process,
with its registers in generally the same state as the parent process.[fn:glibc]
In the style of Plan 9's =fork= syscall, which inspired =clone=,
=clone= takes a mask of flags which determines whether several attributes of the new process
are either shared with, or copied from, the parent process.

=clone= only lets us change the stack register for the new process.
We would like to be able to set arbitrary registers for the new process,
so that we can control where it begins executing and the stack it executes on.
Fortunately, changing the stack is sufficient.

We ensure that the next instruction executed after any syscall
is (in x86 terms) a =RET=;
this is always the case, so we have no need to special case the execution of =clone=.
Since we control the stack of the new process,
the =RET= will jump to a code address that we control.
We can then supply additional arguments to this code
by putting them on the stack.

We typically cause the new process to jump to a trampoline provided by the rsyscall library
which sets all registers to values found on the stack
and then jumps to another address.[fn:rop]
With this trampoline,
we can provide a helper Python function that,
when given a function pointer following C calling conventions, and some arguments,
will prepare a stack for a call to clone such that the new process will call that function with those arguments.

With our new ability to call arbitrary C-compatible functions,
we can now call =clone= so that it launches a process running our infinite syscall loop,
which is implented in C and, as described in the previous section,
uses two pipes passed as arguments to receive syscall requests and respond with syscall results.

# TODO is this section necessary?
The addresses of these functions and trampolines are discovered through a linking procedure.
When the process being created is in the same address space as the main process which is running user code,
the location of the rsyscall library in memory, and the addresses of code within it,
are known through normal language-specific linking mechanisms.
However, when a process is created with a different address space,
such as when we establish a connection to a process after it's been started,
we need to perform linking to learn the addresses of functions.
This linking procedure is performed while bootstrapping the connection,
and involves the target process sending a table of important addresses to the connecting process.

After using =clone= to create a new process running our syscall loop,
most system calls can be called as normal.
The new process can be modified freely through chdir, dup2, and other system calls.
Out of system calls related to process creation,
only =execve= and =unshare= need substantial further attention.
*** execve
Eventually, most programs will want to call =execve= in the processes they create.
=execve= is unusual and requires careful design,
because when it is successful, it does not return.
Therefore we need a way to determine if =execve= is successful;
naively waiting for a response to the syscall request will leave us waiting forever.

One traditional means to detect a successful =execve= is to create a pipe before forking,
ensure both ends are marked =O_CLOEXEC=,
perform the fork,
call =execve= in the child,
close the write end of the pipe,
and wait for EOF on the read end.
If the child process has neither successfully called =execve=, nor exited for some other reason,
then the write end of the pipe will still be open in the child process's fd table,
and the read end of the pipe will not return EOF.
But once the child process calls =execve= successfully,
=O_CLOEXEC= will cause the write end of the pipe to be closed,
and the read end of the pipe will return EOF.

This trick works well with =fork=;
but it's not general enough to work with =clone=.
Child processes can be created with the =CLONE_FILES= flag passed to =clone=,
which causes the parent process and child process to share a single fd table.
This means that when the parent process closes the write end of the pipe,
it will also be closed in the child process,
and the read end of the pipe will immediately return EOF,
regardless of whether the child has called =execve= or exited.

Fortunately, there is an alternative solution, which does work with =CLONE_FILES=.
The =ctid= argument to =clone= specifies a memory address which,
when the =CLONE_CHILD_CLEARTID= flag is set,
the kernel sets to zero when the child exits or execs,
and then, crucially, performs a futex wakeup on.
More specifically,
the kernel clears and does a futex wakeup on =ctid= when the child process leaves its current address space;
this precisely coincides with exiting or execing,
since those are the only way to change address space in Linux as of this writing.

A futex is a Linux-specific feature,
which is generally used for the implementation of userspace shared-memory synchronization constructs,
such as mutexes and condition variables.
The relevant detail for us here is that we can wait on an address
until a futex wakeup is performed on that address;
that means we can wait on =ctid= until the futex wakeup is performed,
and in this way get notified of the child process calling =execve=.

Unfortunately, futexes in current Linux integrate poorly:
There is no way for a single process to wait for more than one futex at a time,
and no way to monitor a futex with file-descriptor-monitoring syscalls such as =poll=.
The best we can do is create a dedicated child process for each futex we want to wait on,
and have this child process exit when the futex has a wakeup.
Monitoring child processes can be straightforwardly integrated into an event loop.

While slightly complex to implement, this solution works well.
We provide =ctid= whenever we call =clone=,
and set up a process to wait on that futex.
Then, when we call =execve=,
we wait for either the =execve= to return an error or the futex process to exit,
whichever comes first.
If the futex process exits,
and the child process doesn't itself exit,
we know that the child has successfully called =execve=.

If the futex process and child process both exit,
it's ambiguous whether the child process successfully called =execve=;
this ambiguity is unfortunate, but it is also present in the pipe-based approach.
This is, we believe, the best solution currently available.

We would prefer for Linux to natively provide functionality to wait for a child's =execve=.
Some other Unix-like systems provide this;
kqueue, on FreeBSD, allows waiting for exec in arbitrary processes through kqueue's =EVFILT_PROC=.
One approach for Linux would be to add a new =clone= flag to opt-in to receiving =WEXECED= events through =waitid=;
note that a =waitid= flag alone is not sufficient,
since it's necessary to receive =SIGCHLD= signals for the =WEXECED= event if waiting for it from an event loop.
Alternatively, some way to wait for futex wakeups through a file descriptor could be added,
so we can use file-descriptor-monitoring syscalls to wait for the =ctid= futex;
such a feature used to exist in the form of =FUTEX_FD=,
but was removed from Linux long ago due to race conditions in its design.
*** TODO unshared file descriptor tables
As mentioned in the previous section,
the =CLONE_FILES= flag can be passed to =clone=.
When this flag is passed,
the file descriptor table is shared between the parent process and child process.
This is simple to model,
and convenient for many purposes;
for example, the child process might be in a different network namespace from the parent,
and the shared file descriptor table would allow the child to bind a socket
and the parent to use it.

The real difficulty is when =CLONE_FILES= is not passed to =clone=.
Then, we get the same behavior as =fork=:
The new process has a new file descriptor table,
containing copies of all the file descriptors existing in the parent at the time of the system call.
This same behavior can also be triggered after process creation by calling =unshare(CLONE_FILES)= or =execve=;
if =unshare(CLONE_FILES)= or =execve=
are called in a process currently sharing its file descriptor table with another process,
then after the call that process will have a new, private file descriptor table,
again with a copy of all the file descriptors existing at the time of the system call.

The new file descriptor table containing a copy of all the file descriptors in the old table
is known as "file descriptor inheritance".
The new table will contain private file descriptors from libraries running in the old table.
Leaving these file descriptors open in the new table is a form of resource leakage,
but it also will cause erroneous behavior.
For example, it's a common practice to close the write end of a pipe
and expect an EOF on the read end;
if the write end is copied into the new file descriptor table before being closed,
and the write end is never closed in the new table,
the read end will never get an EOF.

This issue is well-known,
and the traditional fix is the =CLOEXEC= file descriptor flag.
If this flag is set on a file descriptor,
it will not be copied into the new file descriptor table when one is created;
file descriptor inheritance will not happen for this file descriptor.
This flag can be set on all private file descriptors,
so that they will not be inherited.

If a program does want a file descriptor to be inherited,
it can unset =CLOEXEC= on that file descriptor,
create the new file descriptor table,
and then set =CLOEXEC= again.
It's important that this only be done on file descriptors in processes
that don't have a shared file descriptor table;
otherwise, other processes sharing that file descriptor might call =execve= while =CLOEXEC= is unset,
and unexpectedly inherit the file descriptor.

Unfortunately, =CLOEXEC= only works when a new file descriptor table is created by an =execve=;
hence the typical explanation of its functionality as
"closing CLOEXEC-marked file descriptors when exec is called".
As we noted earlier, file descriptor tables can also be created by =unshare(CLONE_FILES)= or by =clone=.
In many cases, processes that call =unshare(CLONE_FILES)= or =clone= may be short-lived,
or may call =execve= soon thereafter, removing the problem.
But if they are long-lived and don't call =execve=,
the file descriptors inherited into those file descriptor tables
will cause the same resource leakage and incorrect behavior
as file descriptors inherited across =execve=.

The solution is straightforward.
After a call to =unshare(CLONE_FILES)= or =clone=,
we can simply explicitly close all =CLOEXEC= file descriptors.
There is no system call to do this,
but there are multiple mechanism we can use to loop over =CLOEXEC= file descriptors and call =close= on them.
Like with =execve=,
we can inherit a file descriptor into a new file descriptor table by unsetting =CLOEXEC= before creating the new table.

This works well,
but there's one more relevant aspect.
As we mentioned,
it's important that =CLOEXEC= only be unset on file descriptors in processes
that don't have a shared file descriptor table.
Following this rule strictly doesn't allow a process which already shares a file descriptor table
to create a new file descriptor table and selectively inheriting file descriptors into that table.
The imperative process of unsetting =CLOEXEC= and then setting it again
is unsuitable for shared file descriptor tables.

We would prefer to be able to explicitly list =CLOEXEC= fds
which should be inherited into the new file descriptor table
rather than closed.
Fortunately, since we are manually closing the =CLOEXEC= fds in the first place,
we can implement this ourselves:
We simply pass a list of fds as an argument to the userspace function
which closes =CLOEXEC= fds,
and change the function to ignore those fds when closing.
If this functionality was implemented with a system call,
it would be likewise essential that it take a list of fds to exclude from closing.

Note that this is not the same as explicitly listing all fds which should be copied into the new file descriptor table.
Non-=CLOEXEC= fds should still be implicitly inherited;
this is a traditional Unix feature which is useful in a wide variety of situations,
much the same as implicit inheritance of environment variables.

This explicit list of =CLOEXEC= fds which should be inherited is built up by calls to =inherit_fd=;
the programs in the examples section demonstrate its use.
The actual closing of =CLOEXEC= fds doesn't happen, in the Python library,
until the user explicitly triggers it or a file descriptor garbage collection happens.
This is how the feature of inheritance is explicitly exposed to the user.

# TODO um we earlier just said it was really important to close these things
# maybe we should focus on the perspective which emphasizes inherit_fd?
# and emphasizes explicitly cleaning up the fd space
# focus on inheritance
# one thing I do like about this version is that we're very clear about creating new file descriptor tables.
# OK!
# so there are three issues:
# inherit_fd, do_cloexec, and do_cloexec_except
# let's focus on inherit_fd.
# the notion is, we have all fds.
# so we can inherit.
# but, we do need to clean up the fd table afterwards.
**** outline
     we'll cover inheritance through a clone_files making a new fd table,
     then doing a do_cloexec or exec to clear it out.

     and maybe details of unshare?

     yeah, we'll focus on unshare.
     basically just talk about unshare.
**** closefrom
This issue and solution have a surface similarity to the =closefrom= function provided on some Unix systems;
=closefrom= allows closing all file descriptors with an numeric value greater than a specified integer.
Unlike =closefrom=, explicitly closing only =CLOEXEC= file descriptors
preserves the ability to opt-in to inheritance when desired.
=closefrom= lets the program special-case stdin/stdout/stderr as inherited,
while not permitting anything else, such as higher fds, to be inherited.
This is not recommended for a program that is part of a general-purpose system;
inheritance of useful fds to unaware programs is a useful, if niche, feature,
which can be used for many of the same purposes as inheritance of environment variables.
Thus, we close fds based on whether they have =CLOEXEC= set,
not based on their numeric value.
**** fexecve
An explicit list of =CLOEXEC= fds which should be copied rather than closed
would be useful for =execve= as well.
For most fds which are inherited by disabling =CLOEXEC=,
the new process immediately sets =CLOEXEC= again;
an explicit list of passed fds would be a useful, but not essential, convenience.
One case where such a list is essential is with =fexecve= functionality;
that is, executing an executable specified by a file descriptor rather than a path.
The file normally is not needed by the new program,
but if the program is a script,
then the file *is* needed,
so that the script interpreter can read it.
This forces the program running an executable to know how the executable is implemented,
which is an undesirable break of abstraction.
The only robust way to use =fexecve=, then,
is to always pass the file descriptor down across the exec;
if this is done by unsetting =CLOEXEC=,
this will usually result in a leaked file descriptor.
** evaluation (or, measurement)
something?

what if i just skip this section lol

let's do that.
skipping this!

ugh we'll have to do it eventually though,
but not now, I don't want to.
*** performance I guess?
can mmap a bunch of stuff and show that without having to copy page tables, it's faster

can show a chart even, two lines, speed with and without, nice

We are more interested in complexity.
We have not optimized our implementation for performance.

nevertheless, [chart]
*** rewrite something?
    maybe I should try implementing Python's popen/subprocess with rsyscall?
    But that's not fair, theirs is portable.

    lol I can't do it anyway, unless I want to wrap rsyscall fds with Python fds
    i guess I could do that hmmmt
** future work
*** rsyscall other uses
Most avenues of future work focus on rsyscall.
rsyscall was not developed solely for the purpose of this paper,
and it has many uses unrelated to direct-style process creation,
such as asynchronous system calls, exceptionless system calls[fn:flexsc], cross-host operations, among others.
We are actively exploring such directions.
*** non-OO APIs for other languages
[do I really need this section?]
*** portability? other Unix systems
We believe similar techniques as outlined in this paper
could be applied to other systems to provide direct-style process creation for them.
We are not actively exploring this,
as our focus is on Linux,
but others may wish to explore porting this strategy to other operating systems.
*** direct kernel support
rsyscall's cross-process syscalls can be performed entirely in userspace,
which has substantial benefits for deployability.
Nevertheless, direct kernel support for creating a stub process and performing syscalls in the context of that process
may provide efficiency benefits, as well as reducing userspace-visible complexity.
*** TODO kernel support
    # let's concretely say again the features we want;
    # it's just the exec-detecing and do_cloexec_except stuff
Some aspects of direct-style process creation are difficult to achieve on Linux;
see the implementation section for more.
We would benefit from some kernel features;
exploring how to implement these features in a generally useful and upstreamable way
is an important direction for future work.
*** large scale open source usage
We have made substantial use of the techniques described in this paper
in proprietary software at Two Sigma.
While this gives us personally greater confidence in these techniques,
it would be better to use them in a publicly available, open source system.
Either porting an existing system to use these techniques,
or using these techniques to create a substantial new system from scratch,
would aid adoption of these techniques.
** conclusion
Direct-style process creation is much less known and much less used than fork-style and spawn-style.
We have implemented direct-style process creation for Linux.
Our implementation is immediately deployable on today's Linux systems.
We have discussed various applications of processes,
and demonstrated the use of Linux direct-style process creation
to implement them.
We hope that this work will help encourage more use of the process abstraction,
which, though widespread,
is still not used to its full potential.
* Footnotes

[fn:rop]
This is also a generally useful utility for hackers performing return-oriented-programming attacks;
but similar functionality exists in any standards-compliant C library,
so there is no increase in attack surface.

[fn:glibc]
Note that =glibc= defines a wrapper for the raw kernel syscall;
we are here talking about the kernel syscall.
