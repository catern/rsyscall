* papers
** rsyscall: A cross-process syscall API for Linux
** cross-host operations with rsyscall
** asynchronous IO with rsyscall thread pool (flexsc style)
* what to do after writing
1. send to labs team at TS for review
2. send to fork paper guy for review
3. send to professors as Kai says and ask about research opportunities
* Direct-style process creation on Linux
** title
"direct-style" just being a nicer way to say "imperative"

"imperative process creation" doesn't sound good because it says "imperative"
** notes
*** features to emphasize
- it covers everything you might want unlike posix_spawn
- it's immediately deployable
- it's efficient
- it has a coherent theory behind it and is relatively easy to use
** abstract
Traditional process creation interfaces are complex to use, limited in power, and difficult to abstract over.
We develop an alternative process creation interface for Linux
which allows a program to create a new process in an inert state
and then initialize it from the outside by directly operating on it
in the same style as any other operating system object.


This method of process creation results in more comprehensible programs, 
has better error handling,
is more efficient,
and is more amenable to abstraction.
Our implementation is immediately deployable without kernel modifications on any recent Linux kernel versions.
** introduction (existing process creation mechanisms)
Processes are a widespread feature of operating systems,
with substantial variation in characteristics between systems.
One of the areas of variation is in process creation mechanisms.

On most systems today,
the interfaces for process creation
can be divided into "fork-style" and "spawn-style".
A few systems use a third style, which we'll refer to as "direct-style".
*** fork-style
Fork-style interfaces are those that follow the model used by Unix's fork[fn:fork].
A process calls fork, and a copy of that process is made,
with the return value of fork being the only differentiation between the process and its copy.
The same program continues executing in both processes.
Typically, the program will case on the return value to determine if it's running in the parent or the child,
and run process setup and initialization code if the program is running in the child,
calling various syscalls to set up the state of the child.

Fork has a number of flaws,
and much ink has been spilled detailing them.
We'll cover the most prominent briefly here,
but see Baumann 2019[fn:forkroad] for a more detailed evaluation.

Fork returns twice, creating two different processes which execute concurrently.
This immediately makes fork difficult to integrate;
for example, errors can't be easily communicated between the two processes.
If there is an error in the initialization of the child process,
the part of the program running in the child process
needs to use some form of inter-process communication (IPC) to communicate it back to the parent process.
The child process can run arbitrary code to make decisions about its initialization,
but that code needs to use IPC if it wishes to communicate with the rest of the program,
which is still in the parent process.

The most common complaint about fork is its poor performance[fn:forkroad].
It copies many attributes about the parent process when creating the child process,
including setting up copy-on-write memory-mappings in the child process.
This becomes slower as the parent process has more memory-mappings,
eventually taking a significant amount of time.

Multi-threaded programs generally cannot use fork safely.
Typical Unix fork implementations do not duplicate all threads in the parent process to the child process,
so, for example, an important lock might be locked at the time of the fork and then never be unlocked,
causing a deadlock.
In combination with fork's poor performance in large-memory programs,
a user of fork must think carefully
about the characteristics of the process from which fork is being called.
**** notes
expand this
- it's weird to return twice
- errors and fallbacks can't be communicated across fork
- it's fundamentally slow
- it's weird and sucks
- fork can't be used in many scenarios, such as in:
    - large memory programs
    - multithreaded programs
    - programs using unusual fork-unaware libraries such as CUDA or kernel networking bypass

What does spawn solve? performance I guess
we should make sure to clarify. hm. or something.
*** spawn-style
Spawn-style interfaces are those that follow the model used by =posix_spawn=[fn:posix_spawn]
and Windows' =CreateProcess=[fn:create_process].
All the details about the new process are provided up-front as arguments to a syscall,
which creates the new process from a clean slate, initialized with the provided details.

Since spawn-style interfaces don't copy details from the parent process,
they don't have the performance problems of fork.
However, they have significant flaws of their own.

The arguments that can be provided to a spawn-style process creation syscall
do not cover all the possible attributes that one might want to set for the new process.
Most systems have a large number of syscalls which can mutate the state of a process during its lifetime;
for a spawn-style interface to work in all scenarios,
all those possible mutations must be reproduced in the interface.

Spawn-style process creation also does not allow for conditional logic during the setup;
if the setup of the new process encounters an error at some point,
the only option is to return from the entire spawn call with an error.
Such errors returned from spawn-style calls
are typically much less informative
than the errors returned by the syscalls which directly mutate the process attributes.
In general, a spawn-style interface does not allow for conditional logic during the process setup;
a modification to the process cannot depend on the result of some other modification.
**** notes
need to list more flaws I guess
- limited number of modifiable things
- limited expressiveness (conditionals?)
- error handling (haven't included this one yet!)

seems like in multics, process creation was a privileged operation
*** direct-style
A few systems, such as KeyKOS[fn:keykos], seL4[fn:sel4]
and some other operating systems[fn:exokernel] [fn:fuschia] [fn:singularity],
use another style of process creation.
In this style, a process is created by its parent in an inert state,
and then supplied with various resources,
and then started once it is fully set up.
The same mechanisms that can mutate a process while it is running,
are used to mutate the process while it is in an inert state;
in such systems, these mechanisms can be used on the process from *outside* the context of the process,
just as easily as they can be used from inside the process.

We refer to this as "direct-style" process creation,
because the parent creating the process operates on it directly and imperatively
rather than dispatching a distinct unit of code to perform setup from inside the context of the new process,
as in fork-style,
or building up a declarative specification of what the new process should look like ahead of time,
as in spawn-style.

Direct-style process creation has significant advantages over fork-style and spawn-style process creation.

Unlike fork-style, the new process can be created in a clean, empty state,
which removes the performance issues of copying the parent.
The new process does not actually run any user code,
so multi-threaded programs can safely create new processes without fear of deadlocks.

Unlike spawn-style,
the new process can be manipulated with all the normal mechanisms for process manipulation,
so there is no need to create a duplicate spawn interface that allows specifying every possible attribute of a process.

Unlike both fork-style and spawn-style,
direct-style process creation operates on the process from the context of the main program,
using individual syscalls.
Thus, an error in some step of the initialization is reported to the main program like an error in any other syscall,
and coordination between a component creating a process and the rest of the program requires no IPC.
**** notes
     SEL4 has this style.
     https://docs.sel4.systems/Tutorials/threads.html
*** TODO we've made a new process creation interface that doesn't suck
We have adapted direct-style process creation for Linux.

In the rest of the paper,
we will examine the design and implementation of this style of process creation on Linux,
and demonstrate its usage.
# TODO need to clean up this, should make a nice outline here
** background on the use of processes
Why is it important to have a high-quality interface for creating processes?
Processes are already widely used;
most software is distributed as an executable which runs in a dedicated process.
This basic usage of processes can be performed with even a complex and inefficient process creation interface.
But processes have many uses beyond this simple and widespread one;
here we examine some more sophisticated applications of processes,
which benefit from a better process creation interface.
*** file descriptor inheritance allows abstracting over resources
In Unix, the mechanism of file descriptor inheritance
allows a process to be provided a resource by its creator,
while abstracting over the precise identity of that resource.[fn:ucspi]
For example, a process can be provided a file descriptor,
which can correspond to any file in a filesystem,
without the process being aware of what specific file it is accessing.
This is further enhanced by Unix's "everything is a file" design;
the passed file descriptor could also be a pipe, a network connection, or some other resource,
without the process knowing.
As another example,
a process can be provided a socket file descriptor on which it can call =accept= to receive connections,
without being aware of whether those connections come from the internet or from a local Unix socket.[fn:ucspi]
This abstraction mechanism is the basic principle of pipelines and redirection in the Unix shell,
but it is rarely used outside of the shell.
**** notes
     mention ucspi

     and inetd

     hmm should I talk about socket activation? lazily starting processes?

     TODO need more cites for this, it's a common concept.
*** namespace modification allows customization without explicit support
In many systems,
it's possible to modify a process's view of nominally "global" resources.
In Unix-derived systems, this ability is most influentially provided in Plan 9[fn:plan9],
which allows each process to customize its view of the filesystem with private mounts and union directories[fn:plan9ns].
In Linux, these concepts were implemented as per-process namespaces[fn:linuxns].
Fundamentally,
this allows customizing a process's environment and therefore a program's behavior,
without having to write explicit support code for customization.
For example, Plan 9, unlike most other Unix-derived systems,
did not have a =PATH= environment variable which was searched by code in the process to find executables;
instead, each process was executed with a =/bin= directory at the root of the filesystem,
which was a union of many other directories,
and simply executed =/bin/foo= to run the program named =foo=.
In this way the set of executables provided to a process could be customized,
without any code to parse and handle =PATH= or any other executable-lookup-specific customization code.
*** TODO privilege separation allows sandboxing
The basic isolation powers of processes are used to simplify application development:
it is beneficial to have a private virtual memory space when developing a stand-alone program.
But most systems have additional mechanisms of isolation between processes,
such as different privilege levels and access to global resources,
which can be used to provide a form of sandboxing.
For example, components which may exposed to hostile network requests
can be run in a separate process, at a lower privilege level than the main program;
in this way, even if an attacker gains control over that component,
the attacker will only have access to the lower level of privileges of that component,
rather than the full privileges of the main program.
**** notes
     hmm guess I can just find some paper about privile separation in Unix?
*** robust privilege separation and resource privion allows capability-based-security
As a further, more robust development of process-based sandboxing,
the privileges of a process can be explicitly enumerated
in a capability-based security model.[fn:capsicum]
By using previously-mentioned resource passing mechanisms,
such as file descriptor inheritance or namespace manipulation,
and by disabling the process's access to global resources such as the shared filesystem,
we can enforce that all resources used by the process are passed at creation time.
**** notes
     we separate "regular" sandboxing and capability-based security
     because lots people won't understand they're the same thing

     mention capsicum
*** non-shared-memory concurrency allows exploiting parallelism in a simple way
# TODO not sure about this one
Processes run concurrently,
which allows exploiting parallelism in the hardware.
Since processes don't share memory,
they can provide a less complex parallel programming environment
than shared-memory thread-based approaches.
The most popular parallel programming environment in existence today is the Unix shell,
which obtains its parallelism by running multiple processes connected via pipes.
The Unix shell has a relatively constrained form of parallel processing,
but it's also possible to create more complex webs of parallel processes,
where, for example, one process might take multiple inputs over multiple pipes,
or produce multiple outputs.
**** notes
     really, non-shared-memory concurrency is a better fit with reality

     processors don't share memory! they should explicitly copy, that's reality

     where's that dang datastructure server paper

     I should be able to find plenty of cites suggesting that process-based concurrency is superior
*** services?
    don't think i really want this section

 - Failure monitoring? Concurrency control? Concurrency in general? Service-oriented distributed systems?
   Shared-nothing message-passing concurrency? (aka "distributed systems")

*** conclusion
These techniques, and more, are available through the process interface.
Most software would benefit from abstraction over resources, sandboxing, and parallelism.
Yet these features of processes are used only rarely.
There are multiple reasons for this,
but one of the primary reasons is the complexity of current process creation interfaces.

Many of these techniques are used today by specialized software and services.
Often, such software only allows use of one of these techniques;
for example, the Unix shell allows piping together process, but not namespacing them;
container systems allow sandboxing processes, but not piping them together.
By delegating these features to specific separate services,
we lose the ability to use them in combination.

By improving the process creation interface,
we can make it possible both for programs to directly manipulate processes to use these techniques,
and to use and share composable libraries which use these techniques.
We believe this potential justifies the investment of substantial effort
into improving the process creation interface.
**** notes
should establish more that applications don't use these features

- We delegate many of these features to specific separate programs/servers to abstract over them,
  which means we can't use these features in combination.
  - shells, container engines, process supervisors
- If we make it simpler to create processes, we can increase our usage of these features, including in combination
  - This will also make it possible to replace separate programs running as system servers, with libraries
*** notes
    remember: talk about the features we use in the demos in this process-features section!
    and likewise, demonstrate the features we talk about in this section, in the demos sectioN!
** overview/example
   To implement direct-style process creation on Linux,
   we need to be able to call syscalls which operate on a child process,
   from a program which does not run in the child process.
   Given a system for such cross-process syscalls,
   we can create a child process in a sufficiently inert state using existing Linux functionality,
   and then mutate it through various syscalls,
   until it reaches the desired state,
   at which point we can call =execve= on the child process to start it running.
   After that, the child process functions like any other child process,
   and can be monitored using normal Linux child monitoring syscalls,
   such as =waitid=.

   The API for such cross-process syscalls depends on the language;
   in an object-oriented language,
   a syscall naturally takes the form of a method on an object containing a handle for a process.
   When a process is created,
   an object is returned,
   upon which exist methods for all Linux syscalls,
   and which perform those syscalls such that they manipulate the specific process wrapped by this object.
   A program written in an object-oriented language
   creating processes in direct-style
   is then a normal imperative program creating and mutating objects.

   We'll give all our examples of direct-style process creation in object-oriented Python;
   some Python-specific syntax has been removed for clarity,
   but the examples are otherwise real working code.
*** Terminology: A thread is a process controlled by a single program
    From the perspective of our programs,
    there are multiple processes which are under its control.
    "A process which is under my control" is a mouthful;
    we use the term "thread" to refer to all such controlled processes,
    including the main "thread" on which the program is running.
    On Linux, the shared-memory "threads" provided by libraries such as pthreads
    are implemented as processes,
    and their lifetime and execution is completely controlled by a single program;
    the same is true of our controlled-process "threads".
    The most significant difference is that our "threads" do not run their own code concurrently with the main program;
    nevertheless our "threads" do provide the opportunity for parallel execution of system calls,
    and so the terminology provides useful intuition.
    We will therefore use the term "thread" to refer to these controlled processes throughout the rest of the paper.
*** basic example
In \listing{basic}, we simply create a new process under our control (a thread)
and immediately exec a binary with it.
As exceptions are used for error-handling in the Python API,
there is no need for error-checking code.

#+BEGIN_SRC python
# Use the clone syscall in the local thread to create a new thread;
# we use a wrapper that supplies defaults for all arguments.
child = local.thread.clone()
# Call execve to run a different executable in the child thread;
# We pass the executable path as the first argument in the argument list, as is traditional.
# We use a wrapper that defaults envp to an unchanged environment, so we don't pass envp.
child.execve(hello_path, [hello_path])
#+END_SRC
**** introducing
thr

we refer to processes as threads.

hmm maybe we shouldn't
no it's fine


clone

not using:
execve returining the child process

not actually waiting on the child.
*** passing down fds
In \listing{fds}, we create a new thread,
then create a listening socket bound to a random port in that thread,
then call exec, 
passing down the socket by disabling cloexec and passing its file descriptor number as an argument to the new program.

File descriptors, here, are object oriented and have relevant syscalls as methods.
They make syscalls in the process they are created in by default;
we can create more objects referring to the same file descriptor from different processes
if we want to make the syscalls from another process.

#+BEGIN_SRC python
child = local.thread.clone()
sock = child.socket(AF.INET, SOCK.DGRAM)
# bind the socket to a sockaddr_in;
# the sockaddr is allocated in memory with child.ptr and is garbage collected
sock.bind(child.ptr(SockaddrIn(0, 0)))
sock.listen(10)
sock.disable_cloexec()
child.execve(executable_path, [executable_path, "--listening-socket", str(int(sock))])
#+END_SRC
**** introducing
disable_cloexec
socket creation
using int on sock
.ptr

not using:
.args
as_argument
*** piping
In \listing{pipe},
we do the same as the Unix shell pipeline "yes | head -n 15".
We create a pipe,
then create two threads,
connect them with a pipe,
and exec a different program in each thread.

After a process is created with clone,
it may have inherited file descriptors;
here we inherit the pipe.
We make this inheritance explicit with =inherit_fd=,
a helper method on our thread object,
which takes a file descriptor from a different thread
and performs a runtime check that the file descriptor actually was inherited.
If so, it returns a new handle to the file descriptor which performs syscalls from the new thread.

Then we simply =dup2= as normal to replace child1's stdout with the write end of the pipe;
=dup2= disables CLOEXEC by default on the target.

#+BEGIN_SRC python
# create the pipe
pipe = local.thread.pipe()
child1 = local.thread.clone()
# inherit the write-end of the pipe to child1, and replace child1.stdout with it
child1.inherit_fd(pipe.write).dup2(child1.stdout)
child1_proc = child1.execve(yes_path, [yes_path])
child2 = local.thread.clone()
# inherit the read-end of the pipe to child2, and replace child2.stdin with it
child2.inherit_fd(pipe.read).dup2(child2.stdin)
child2_proc = child2.execve(head_path, [head_path, "-n", "15"])
#+END_SRC
**** introduce
dup2
pipe

not using:
syscalls returning the buffer passed into them
malloc
*** mount namespace
In \listing{mount},
we make a new mount namespace and rearrange the filesystem tree for the child process.
We bind-mount /proc at /proc inside the chroot directory,
chroot into the directory,
and exec an executable which will run inside the chroot.

#+BEGIN_SRC python
child = local.thread.clone(CLONE.NEWUSER|CLONE.NEWNS)
child.mkdir(rootdir/"proc")
child.mount(Path("/proc"), rootdir/"proc", "", MS.BIND, "")
child.chroot(rootdir)
child.execve(executable_path, [executable_path])
#+END_SRC
**** introduce
namespaces

slashes in paths

do we really need this? yeah it's nice, shrug
*** nested clone and network namespace
In \listing{nested},
we make a process (=ns_thread=) in a new network namespace.
Then, we create two more child processes of =ns_thread=,
which are also in the new network namespace.
This nested creation of child processes is fully supported,
like all other syscalls,
and allows us to set up complex graphs of processes and namespaces.

We bind to a privileged port on localhost inside the namespace,
and create one child to listen on that socket,
and another child to connect to it.

#+BEGIN_SRC python
ns_thread = local.thread.clone(CLONE.NEWNET|CLONE.NEWUSER)

listening_child = ns_thread.clone()
sock = listening_child.socket(AF.INET, SOCK.DGRAM)
sockaddr = SockaddrIn(22, "127.0.0.1")
sock.bind(listening_child.ptr(sockaddr))
sock.listen(10)
sock.disable_cloexec()
child.execve(server_path, [server_path, "--listening-socket", str(int(sock))])

connecting_child = ns_thread.clone()
child.execve(client_path, [client_path, "--connect-address", str(sockaddr.address) + ":" + str(sockaddr.port)])
#+END_SRC
**** introduce
network namespace
nested clone

Nested clone example: client and server inside net namespace using fixed localhost port
*** TODO miredo
In \listing{miredo},
we show non-trivial code for launching a real application:
the Miredo IPv6 tunneling software.
We use a few helper functions in this listing to keep the attention focused on the interesting parts.

Miredo is separated into two components, a privileged process which sets up network interfaces,
and an unprivileged process which talks to the network.
With minimal modifications to Miredo,
we launch Miredo entirely unprivileged inside a user namespace and network namespace,
with all resources created outside and explicitly passed in.

#+BEGIN_SRC python
### create socket outside network namespace that Miredo will use for internet access
inet_sock = local.thread.socket(AF.INET, SOCK.DGRAM)
inet_sock.bind(local.thread.ptr(SockaddrIn(0, 0)))
# set some miscellaneous additional sockopts that Miredo wants
set_miredo_sockopts(local.thread, inet_sock)
### create main network namespace thread
ns_thread = local.thread.clone(CLONE.NEWNET|CLONE.NEWUSER)
### create in-network-namespace raw INET6 socket which Miredo will use to relay pings
icmp6_fd = ns_thread.socket(AF.INET6, SOCK.RAW, IPPROTO.ICMPV6)
### create in-network-namespace socket which Miredo will use for unassociated Ifreq ioctls
reqsock = ns_thread.socket(AF.INET, SOCK.STREAM)
### create and set up the TUN interface
tun_fd, tun_index = make_tun(ns_thread, "miredo", reqsock)
### create socketpair which Miredo will use to communicate between privileged process and Teredo client
privproc_pair = ns_thread.socketpair(AF.UNIX, SOCK.STREAM)
### start up privileged process which manipulates the network setup in the namespace
privproc_thread = ns_thread.clone()
# preserve NET_ADMIN capability over exec so that privproc can manipulate the TUN interface
# helper function used because manipulating Linux ambient capabilities is fairly verbose
add_to_ambient_caps(privproc_thread, {CAP.NET_ADMIN})
# privproc expects to communicate with the main client over stdin and stdout
privproc_side = privproc_thread.inherit_fd(privproc_pair.first)
privproc_side.dup2(privproc_thread.stdin)
privproc_side.dup2(privproc_thread.stdout)
privproc_child = privproc_thread.execve(miredo_privproc_executable_path, [
    miredo_privproc_executable_path, str(tun_index)
])
### start up Miredo client process which communicates over the internet to implement the tunnel
# the client process doesn't need to be in the same network namespace, since it is passed all
# the resources it needs as fds at startup.
client_thread = ns_thread.clone(CLONE.NEWUSER|CLONE.NEWNET|CLONE.NEWNS|CLONE.NEWPID)
# lightly sandbox by unmounting everything except for the executable and its deps (known via package manager)
unmount_everything_except(client_thread, miredo_exec.run_client.executable_path)
# a helper function for preparing the fds that are passed as command line arguments
async def pass_fd(fd: FileDescriptor) -> str:
    client_thread.inherit_fd(fd).disable_cloexec()
    return str(int(fd))
client_child = client_thread.execve(miredo_client_executable_path, [
    miredo_client_executable_path,
    pass_fd(inet_sock), pass_fd(tun_fd), pass_fd(reqsock),
    pass_fd(icmp6_fd), pass_fd(privproc_pair.second),
    "teredo.remlab.net", "teredo.remlab.net"
])
#+END_SRC

**** introduces
socketpair
several helper functions
**** notes
ok so maybe I should remove some of the unnecessary networking stuff?
actually we can come back to this later

ok I feel like we need more detailed description or something.

actually it would be strongly beneficial to show what the corresponding fork-based code looks like.
ah maybe I shouldn't actually do the gloating "wow we're so simple"

maybe I should do a nested clone before this? yeah for sure actually.

OK so that's a big, clear todo:
do a nested clone before here.
what's the use of nested clones? well, namespaces are one.

guess I could do that in the mount namespace?
I could do a pipe in the mount namespace.
i could intro some other namespace; newnet or newpid maybe

Nested clone example: client and server inside net namespace using fixed localhost port
** implementation
*** basics about rsyscall
Our main need for implementing direct-style process creation
is a robust system for cross-process syscalls.
We provided this in the rsyscall project.
rsyscall is a toolkit for cross-process syscalls on Linux,
with several language-specific library implementations.

In this section, we'll give a brief overview of rsyscall,
and focus on implementation issues specific to process creation.

rsyscall can be conceptually divided in two parts:
the basic cross-process syscall primitive,
and a language-specific library built on top
to handle the complexities of manipulating resources across multiple processes.
The Python language-specific library has already been demonstrated above.
Such libraries only need to be able to call syscalls and explicitly specify a process in some way;
they are, for the most part, agnostic to how the cross-process syscall is implemented.

Using the Python library as an example,
it provides Python wrappers for Linux system calls and structs
which are type-safe using Python 3 type annotations and runtime checks
while still providing low-abstraction access to a large subset of native Linux functionality.
It also provides garbage collection for remote file descriptors, memory and other resources.
Such features are independent of the precise implementation of the cross-process syscall primitive.

On Linux x86_64, a syscall is specified by a syscall number plus six register-sized arguments;
a syscall returns one register-sized value.
rsyscall's default implementation of cross-process syscalls sends those seven integers over a pipe,
and waits for a response on another pipe.
Processes are created running an infinite loop which, at each iteration,
reads a syscall request off the pipe,
performs that syscall,
and writes the return value back over the return pipe.
In this way, a cross-process syscall works much like a very primitive remote procedure call.

Many syscalls either take or return pointers to memory,
and require the caller to read or write that memory to provide arguments or receive results.
Therefore, an rsyscall library needs a way to access memory in the target process.
We implement this through another set of pipes,
by explicitly copying memory into and out of those pipes using the =read= and =write= system calls.
When we wish to read =N= bytes of memory at address =A= in the target process,
we first perform a =write(memory_pipe, A, N)= in the target process,
and then read that data off the other end of the pipe in the parent process.
When we wish to write =N= bytes of data at address =A= in the target process,
we first write that data to the pipe in the parent process,
then perform a =read(memory_pipe, A, N)= in the target process to copy that data from the pipe into memory.

ptrace provides an alternative means to perform arbitrary actions on other processes.
However, among other issues, it has the unavoidable substantial disadvantage of not permitting multiple ptracers.
A ptrace-based implementation would prevent using strace or gdb on rsyscall-controlled processes,
which is an unacceptable limitation for a general-purpose utility.

The =process_vm_readv= and =process_vm_writev= system calls
allow the caller to read and write memory from the virtual address space of other processes.
However, they require that the caller have specific credentials relative to the process being accessed,
which may not always be the case.
Additionally, these system calls are disabled if ptrace is disabled system-wide,
which is a niche but possible system configuration.
To ensure that rsyscall can be used for arbitrary purposes and on arbitrary systems, we avoided these calls.
**** notes
     maybe it would be beneficial to separate
     "rsyscall the basic cross-process syscall primitive"
     from
     "rsyscall the python library".

     
*** clone
Now that we've established the basic operations which rsyscall provides,
let's consider the specific issues related to process creation and initialization.

There are three Linux system calls which create processes:
=fork=, =vfork= and =clone=.
=clone= provides a superset of the functionality of the other two,
so we focused our attention on =clone=.

=clone= (along with =fork=) creates a new process
which immediately starts executing at the next instruction after the syscall instruction,
in parallel with the parent process,
with its registers in generally the same state as the parent process.[fn:glibc]
In the style of Plan 9's =fork= syscall[fn:rfork], which inspired =clone=,
=clone= takes a mask of flags which determines whether several attributes of the new process
are either shared with, or copied from, the parent process.

=clone= only lets us change the stack register for the new process.
We would like to be able to set arbitrary registers for the new process,
so that we can control where it begins executing and the stack it executes on.
Fortunately, changing the stack is sufficient.

We ensure that the next instruction executed after any syscall
is (in x86 terms) a =RET=;
this is always the case, so we have no need to special case the execution of =clone=.
Since we control the stack of the new process,
the =RET= will jump to a code address that we control.
We can then supply additional arguments to this code
by putting them on the stack.

We typically cause the new process to jump to a trampoline provided by the rsyscall library
which sets all registers to values found on the stack
and then jumps to another address.[fn:rop]
With this trampoline,
we can provide a helper Python function that,
when given a function pointer following C calling conventions, and some arguments,
will prepare a stack for a call to clone such that the new process will call that function with those arguments.

With our new ability to call arbitrary C-compatible functions,
we can now call =clone= so that it launches a process running our infinite syscall loop,
which is implented in C and, as described in the previous section,
uses two pipes passed as arguments to receive syscall requests and respond with syscall results.

# TODO is this section necessary?
The addresses of these functions and trampolines are discovered through a linking procedure.
When the process being created is in the same address space as the main process which is running user code,
the location of the rsyscall library in memory, and the addresses of code within it,
are known through normal language-specific linking mechanisms.
However, when a process is created with a different address space,
such as when we establish a connection to a process after it's been started,
we need to perform linking to learn the addresses of functions.
This linking procedure is performed while bootstrapping the connection,
and involves the target process sending a table of important addresses to the connecting process.

After using =clone= to create a new process running our syscall loop,
most system calls can be called as normal.
The new process can be modified freely through chdir, dup2, and other system calls.
Out of system calls related to process creation,
only =execve= and =unshare= need substantial further attention.
*** execve
Eventually, most programs will want to call =execve= in the processes they create.
=execve= is unusual and requires careful design,
because when it is successful, it does not return.
Therefore we need a way to determine if =execve= is successful;
naively waiting for a response to the syscall request will leave us waiting forever.

One traditional means to detect a successful =execve= is to create a pipe before forking,
ensure both ends are marked =O_CLOEXEC=,
perform the fork,
call =execve= in the child,
close the write end of the pipe,
and wait for EOF on the read end.
If the child process has neither successfully called =execve=, nor exited for some other reason,
then the write end of the pipe will still be open in the child process's fd table,
and the read end of the pipe will not return EOF.
But once the child process calls =execve= successfully,
=O_CLOEXEC= will cause the write end of the pipe to be closed,
and the read end of the pipe will return EOF.

This trick works well with =fork=;
but it's not general enough to work with =clone=.
Child processes can be created with the =CLONE_FILES= flag passed to =clone=,
which causes the parent process and child process to share a single fd table.
This means that when the parent process closes the write end of the pipe,
it will also be closed in the child process,
and the read end of the pipe will immediately return EOF,
regardless of whether the child has called =execve= or exited.

Fortunately, there is an alternative solution, which does work with =CLONE_FILES=.
The =ctid= argument to =clone= specifies a memory address which,
when the =CLONE_CHILD_CLEARTID= flag is set,
the kernel sets to zero when the child exits or execs,
and then, crucially, performs a futex wakeup on.
More specifically,
the kernel clears and does a futex wakeup on =ctid= when the child process leaves its current address space;
this precisely coincides with exiting or execing,
since those are the only way to change address space in Linux as of this writing.

A futex is a Linux-specific feature,
which is generally used for the implementation of userspace shared-memory synchronization constructs,
such as mutexes and condition variables.
The relevant detail for us here is that we can wait on an address
until a futex wakeup is performed on that address;
that means we can wait on =ctid= until the futex wakeup is performed,
and in this way get notified of the child process calling =execve=.

Unfortunately, futexes in current Linux integrate poorly:
There is no way for a single process to wait for more than one futex at a time,
and no way to monitor a futex with file-descriptor-monitoring syscalls such as =poll=.
The best we can do is create a dedicated child process for each futex we want to wait on,
and have this child process exit when the futex has a wakeup.
Monitoring child processes can be straightforwardly integrated into an event loop.

While slightly complex to implement, this solution works well.
We provide =ctid= whenever we call =clone=,
and set up a process to wait on that futex.
Then, when we call =execve=,
we wait for either the =execve= to return an error or the futex process to exit,
whichever comes first.
If the futex process exits,
and the child process doesn't itself exit,
we know that the child has successfully called =execve=.

If the futex process and child process both exit,
it's ambiguous whether the child process successfully called =execve=;
this ambiguity is unfortunate, but it is also present in the pipe-based approach.
This is, we believe, the best solution currently available.

We would prefer for Linux to natively provide functionality to wait for a child's =execve=.
Some other Unix-like systems provide this;
kqueue, on FreeBSD, allows waiting for exec in arbitrary processes through kqueue's =EVFILT_PROC=.
One approach for Linux would be to add a new =clone= flag to opt-in to receiving =WEXECED= events through =waitid=;
note that a =waitid= flag alone is not sufficient,
since it's necessary to receive =SIGCHLD= signals for the =WEXECED= event if waiting for it from an event loop.
Alternatively, some way to wait for futex wakeups through a file descriptor could be added,
so we can use file-descriptor-monitoring syscalls to wait for the =ctid= futex;
such a feature used to exist in the form of =FUTEX_FD=,
but was removed from Linux long ago due to race conditions in its design.
*** managing file descriptor tables
As mentioned in the previous section,
the =CLONE_FILES= flag can be passed to =clone=.
When this flag is passed,
the file descriptor table is shared between the parent process and child process.
The same file descriptors are open in both processes at the same numbers,
and if new file descriptors are opened in either process,
they are also visible in the other process.
This is simple to model,
and convenient for many purposes;
for example, the child process might be in a different network namespace from the parent,
and the shared file descriptor table would allow the child to bind a socket
and the parent to use it.

If =CLONE_FILES= is not passed to =clone=,
then =clone= has the same behavior as =fork=:
The new process has a new file descriptor table,
containing copies of all the file descriptors existing in the parent at the time of the system call.
This same behavior can also be triggered after process creation by calling =unshare(CLONE_FILES)= or =execve=;
if =unshare(CLONE_FILES)= or =execve= (ignoring =CLOEXEC=, which we'll discuss later),
are called in a process currently sharing its file descriptor table with another process,
then after the call that process will have a new, private file descriptor table,
again with a copy of all the file descriptors existing at the time of the system call.

After a system call has copied all the file descriptors in the old table into the new table,
we need to decide which file descriptors we want to keep open in the new table,
and which file descriptors should be closed.
Keeping some file descriptors from the old table in the new table
is referred to as "file descriptor inheritance".
**** inheriting file descriptors
The rule about which file descriptors stay open is simple:
We want to keep a file descriptor open in the new table
if there is a process using that file descriptor.
We track which processes are using which file descriptors as part of our file descriptor garbage collection system.
File descriptors are used through garbage-collected handles,
each of which is associated with a process.
If there is an existing handle for a (process, file descriptor number) combination,
this means that the file descriptor with that number in that process's file descriptor table is in use by that process.

This makes inheritance simple in the case of =unshare(CLONE_FILES)=.
Every file descriptor in use by a process
is referenced by a (process, file descriptor number) combination;
these handles are still valid after a call to =unshare(CLONE_FILES)=,
they simply now refer to the new file descriptor in the process's new file descriptor table.
So, these file descriptors will automatically be kept open;
that is, they will be automatically inherited into the new table.
This same mechanism can work across =execve=,
if control over the process is re-established after the =execve=,
and if we carefully manage =CLOEXEC=.

When a process is newly created,
no file descriptor handles yet exist which involve that process.
Once a handle is created with that process and some file descriptor number,
the referenced file descriptor will be inherited into (that is, kept open in) the new file descriptor table.
These handles can be created through the =inherit_fd= function.

At the time of creating a new file descriptor table,
we perform some bookkeeping:
We make a list of all the file descriptors that existed in the old table
at the time of the creation of the new table.
These file descriptors are the ones which were copied into the new table.

The =inherit_fd= function uses this list.
It takes as arguments a new process and a file descriptor handle from another process,
and checks (with the list) that that file descriptor was copied into the new process's file descriptor table
and hasn't since been closed.
If so, it creates a new handle for that (process, file descriptor number) combination.
This causes the file descriptor to stay open in the new file descriptor table,
and it can be used in the new process through the handle.
**** closing file descriptors after inheritance
After file descriptor inheritance is complete,
we must promptly close other file descriptors we don't want to inherit.
Leaving these file descriptors open in the new table is a form of resource leakage.
It can also cause erroneous behavior.
For example, it's a common practice to close the write end of a pipe
and expect an EOF on the read end;
if the write end is copied into the new file descriptor table before being closed,
and the write end is never closed in the new table,
the read end will never get an EOF.

However, we can't simply close all other file descriptors.
The possibility of implicit inheritance of file descriptors is a traditional Unix feature,
which is useful in a wide variety of situations,
in much the same way as implicit inheritance of environment variables;
it can allow a resource to be passed down a process hierarchy without intervening programs being aware.

Here is where =CLOEXEC= becomes relevant:
=CLOEXEC=, in practice,
is a tag, set by userspace, for file descriptors which should not be implicitly inherited.
If =CLOEXEC= is set on a file descriptor,
we should close that file descriptor if we don't explicitly want to inherit it;
if =CLOEXEC= is not set, we should not close the file descriptor,
but instead we should allow it to be implicitly inherited.

This interpretation of =CLOEXEC= is a consequence of =CLOEXEC='s primary purpose:
Managing inheritance of file descriptors over =execve=,
in programs where the caller of =execve=
doesn't know all the file descriptors that may have been opened by the rest of the program;
this describes, in practice, all programs which call =execve=.
When =CLOEXEC= is set on a file descriptor,
it will not be copied into the new file descriptor table created after =execve=.
Thus =CLOEXEC= is a way of saying, before an =execve=,
that this file descriptor will not be used by the new program after an =execve=,
and therefore should not be inherited.
Since, in a general purpose program, =execve= may be called at any time,
libraries must have =CLOEXEC= set correctly at all times.
So we can check =CLOEXEC= at any time to see whether a file descriptor should be inherited or not,
even if the file descriptor is used by a library unrelated to rsyscall.

The implementation of closing non-inherited file descriptors is then simple:
We close all file descriptors which have =CLOEXEC= set
and which aren't referenced by an rsyscall file descriptor handle.
We also clear the list of copied file descriptors which =inherit_fd= uses.
We do this closing operation in userspace;
a syscall to perform this would be a useful addition to Linux.

=CLOEXEC= is set by default on all file descriptors opened by rsyscall,
though it may be unset by a user program.
Many user programs will unset =CLOEXEC= on some file descriptors immediately before calling =execve=
so that the executable they run will inherit those file descriptors.
The new program run by the =execve= will typically immediately set =CLOEXEC= again.

An additional argument to the =execve= syscall
which allows specifying an explicit list of file descriptor numbers to inherit despite the =CLOEXEC= flag being set
would allow programs to avoid this pointless behavior of un-setting and then immediately re-setting =CLOEXEC=.
It would also, more significantly, allow programs to inherit file descriptors across =execve=
while the file descriptor table is shared;
unsetting =CLOEXEC= while the file descriptor table is shared will cause race conditions,
because the other processes sharing the file descriptor table might call =execve= at any time,
and so the file descriptor table must first be unshared before unsetting =CLOEXEC= and calling =execve=.
** TODO evaluation
*** performance ideas
Can mmap a bunch of stuff in a process and then create some children,
and show that without having to copy page tables (due to CLONE_VM), it's faster.
Can make the classic chart of fork vs glibc's posix_spawn vs direct-style.
*** abstraction ideas
    implementing Python's popen/subprocess with rsyscall?
    But that's not fair, theirs is portable, we'll be way simpler for free.

    Rewrite some existing system with rsyscall?
    Write a shell?
** Future work
*** rsyscall other uses
Most avenues of future work focus on rsyscall.
rsyscall was not developed solely for the purpose of this paper,
and it has many uses unrelated to direct-style process creation,
such as asynchronous system calls, exceptionless system calls[fn:flexsc], cross-host operations, among others.
We are actively exploring such applications,
as well as broadening rsyscall's language support.
*** direct kernel support
rsyscall's cross-process syscalls can be performed entirely in userspace,
which has substantial benefits for deployability.
Nevertheless, direct kernel support for creating a stub process and performing syscalls in the context of that process
may provide efficiency benefits, as well as reducing userspace-visible complexity.
*** kernel support
Several other aspects of our implementation would be improved by kernel support.
We discussed these in the implementation section;
in brief, we would most benefit from kernel support for
detecting when a child process finishes =execve=,
closing all =CLOEXEC= file descriptors except for an explicitly specified list,
and explicitly specifying a list of =CLOEXEC= file descriptors to inherit over =excve=.
Implementing these features in the kernel in a generally useful way, and upstreaming them,
is an important direction for our future work.
*** portability? other Unix systems
Other non-Linux systems
could adopt the techniques of this paper
to provide direct-style process creation.
Currently, our focus is on Linux,
but others may wish to explore porting these techniques to other operating systems.
*** large scale open source usage
We have made use of the techniques described in this paper
in proprietary software at Two Sigma.
While this gives us personally greater confidence in these techniques,
it would be better to use them in a publicly available, open source system.
Either porting an existing system to use these techniques,
or using these techniques to create a substantial new system from scratch,
would provide a meaningful demonstration of the viability of these techniques.
*** file descriptor lifetime management                            :noexport:
    # TODO this is hard to explain, probably best to drop this
Keeping file descriptors open if and only if there is a specific process using that file descriptor
is not the only possibility.
Keep in mind that "a process using the file descriptor" is a slight abuse of terminology;
processes don't use file descriptors, programs do, and in our system there is only one program.
To reflect this, we could instead more directly use file descriptors in a process-agnostic way;
this would support the creation of objects
which work transparently across multiple processes which share a file descriptor table.
Such objects would automatically use the relevant process to perform the syscalls for any specific operation.
A process-centric view of file descriptors instead forces each object to be associated with one process.
Nevertheless, we found that the process-centric perspective better matches the existing intuitions of users,
especially those with prior experience in programming with processes.
We hope that future systems for multi-process programming
might explore an object-centric approach for managing resources.
** conclusion
Direct-style process creation is much less known and much less used than fork-style and spawn-style.
We have implemented direct-style process creation for Linux.
Our implementation is immediately deployable on today's Linux systems.
We have discussed various applications of processes,
and demonstrated the use of Linux direct-style process creation
to implement them.
We hope that this work will help encourage more use of the process abstraction,
which, though widespread,
is still not used to its full potential.

* Footnotes

[fn:linuxns]
TODO something about namespaces?
maybe cite namespaces(7)?

[fn:plan9ns]
http://doc.cat-v.org/plan_9/4th_edition/papers/names

[fn:rfork]
http://man.cat-v.org/plan_9/2/fork

[fn:ucspi]
https://cr.yp.to/proto/ucspi.txt

[fn:fork]
https://pdfs.semanticscholar.org/ee8b/ece5d6d3270df9a22211aeaa84919a9251b3.pdf

[fn:create_process]
https://docs.microsoft.com/en-us/windows/win32/api/processthreadsapi/nf-processthreadsapi-createprocessa

[fn:posix_spawn]
https://pubs.opengroup.org/onlinepubs/9699919799/functions/posix_spawn.html

[fn:singularity]
https://www.microsoft.com/en-us/research/publication/deconstructing-process-isolation/

[fn:exokernel]
https://www.semanticscholar.org/paper/The-exokernel-operating-system-architecture-Engler/071b014e255ed65b6e933db22882e0532e4cff1c

[fn:fuschia]
https://fuchsia.dev/fuchsia-src/reference/syscalls/process_create

[fn:plan9]
https://www.semanticscholar.org/paper/Plan-9-from-Bell-Labs-Pike-Presotto/d84747ac8b31be455670c20fe975a2f4dcaf7f7e
paper pdf is wrong, real pdf is
https://www.usenix.org/legacy/publications/compsystems/1995/sum_pike.pdf

[fn:sel4]
https://dl.acm.org/doi/10.1145/2893177

[fn:keykos]
https://www.researchgate.net/publication/221397234_The_KeyKOS_nanokernel_architecture

[fn:forkroad]
https://www.microsoft.com/en-us/research/publication/a-fork-in-the-road/

[fn:flexsc]
https://www.semanticscholar.org/paper/FlexSC%3A-Flexible-System-Call-Scheduling-with-System-Soares-Stumm/7bdd86e6b294870bd1e1349b3b4bed80f655044e

[fn:fdnumber]
In this paper, we are somewhat loose about using the term "file descriptor"
when we really mean "file descriptor number".
A file descriptor exists in a specific file descriptor table;
when a process switches tables,
it can use the same file descriptor number to refer to the same "open file description",
but the file descriptor establishing the link between the two is different.
File descriptors have only one bit of state which is independent from the "open file description":
The CLOEXEC flag.

[fn:rop]
This is also a generally useful utility for hackers performing return-oriented-programming attacks;
but similar functionality exists in any standards-compliant C library,
so there is no increase in attack surface.

[fn:glibc]
Note that =glibc= defines a wrapper for the raw kernel syscall;
we are here talking about the kernel syscall.
* misc other references
porting unix to windows NT, mentions implementing fork on top of CreateProcess
https://www.usenix.org/legacy/publications/library/proceedings/nt-sysadmin97/full_papers/korn/korn.pdf

* other ideas
can do stuff to prepare the process up-front and replicate it to make many
