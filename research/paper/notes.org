* papers
** rsyscall: A cross-process syscall API for Linux
** cross-host operations with rsyscall
** asynchronous IO with rsyscall thread pool (flexsc style)
* what to do after writing
1. send to labs team at TS for review
2. send to fork paper guy for review
3. send to professors as Kai says and ask about research opportunities
* misc other references
porting unix to windows NT, mentions implementing fork on top of CreateProcess
https://www.usenix.org/legacy/publications/library/proceedings/nt-sysadmin97/full_papers/korn/korn.pdf

* other ideas
can do stuff to prepare the process up-front and replicate it to make many

* comments
>1. We need a name for your technique. Let's call it direct_spawn()

Sure, but the name I use in the paper is "direct-style". There's no
real individual function that would be called "direct_spawn". The
closest possible is the syscall to clone to create a new process, but
that has the same interface as clone and is ultimately a small wrapper
around clone. Does "direct-style" have some problem? Maybe
"direct-style clone"?

>2. For OSDI, you need an evaluation section of 3/4 pages. I think we
>   can take a bunch of open-source apps and show that direct_spawn()
>   is better than the alternatives on all sorts of dimensions (errors
>   handling, performance, LoC). IIRC, the JVM forks early to provide a
>   parent that can do low-cost children spawning. Can you modify the
>   JVM to use direct_spawn() and simplify their code?

Yes, it's something that I've considered, but that would require
porting the API to Java, which would take at least a year of work.

Right now my plan is to do three comparisons:

- Show that some code with equivalent behavior in both direct-style
  and fork is much longer in fork-style
- Benchmark fork against clone in large address spaces
- Benchmark Python's subprocess library against Python rsyscall

Do those sound plausible? Should I add some more?

>3. As direct_spawn() is user-space only, Maybe port it to BSD, and
>   show that it also make things better there?

A good suggestion, but it's "user-space only" in a similary way that
CRIU is "user-space only" :) There is a great deal of Linux-specific
functionality being used. Porting is non-trivial. rsyscall is
essentially a libc, which is not something that's easily portable.

>4. You are competing against fork() and posix_spawn() for creating a
>   new process. These are C APIs. So I think you should show your API
>   with C, not Python. Further, in the design section, Python is a
>   distraction. It makes the API unclear and more complex than it
>   needs to be (e.g., child.ptr() is very odd in Python, or
>   inherid_fd() is chainable is strange. It's hard to imagine the C
>   API from that).

I understand, and I wish I could, but it's not possible. The C API
is far more primitive than the the Python API. The usable
implementation is mostly in Python. (bizarrely low-level Python,
but Python nonetheless.) I wish I could have written primarily in
C, but there are numerous disadvantages to writing in C. Good
alternatives would have been Go or Rust, but I chose Python due to
its wide adoption.

I'll avoid using the chaining of inherit_fd, since it might be
confusing. The use of .ptr is hard to avoid without significantly
reworking the API. Other thoughts on the use of Python?

>5. You say that main complaint of fork() is that it's
>   slow. direct_spawn() should absolutely be faster than fork then.

Will discuss in the evaluation

>6. The fork() memory copy-on-write in multi-threaded environment also
>   have the terrible downside that the application memory usage could
>   double while the child has not yet execve(). This can send an
>   application in OOM land, which is terrible.

This doesn't happen if the application doesn't touch its memory,
right?

>7. When you argue that you use a pipe for communicating between the
>   parent and the child of direct_spawn(), it seems that you don't
>   explain why it's better than using shared memory (e.g.,
>   mmap(MAP_SHARED), and communicate there). Also, memory structures
>   that need to be sent around (here, via write(memory_pipe) )could
>   just sit on this piece of shared memory and you have a zero-copy
>   solution. You should describe all the possible ways to do IPC, and
>   argue why the one you picked is better than all the others.

Good point, I'll discuss that as well. (The reason is that that
wouldn't work across different kernels, which is the primary case
for the processes being in separate address spaces)

>8. I don't understand in the design of direct_spawn(), why the child
>   is in a different memory space than the parent. Skip
>   mmap(MAP_SHARED) all together, and just use clone(...,
>   CLONE_VM). Performance will be much better in multi-threaded
>   programs.

Good point, I'll discuss that. (I forgot to note that usually they
are in the same address space; the memory stuff I discussed is to
support when they can't be, such as remote hosts.)

>9. Nit picking regarding CLONE_CHILD_CLEARTID: more precisely, the
>   ctid address is cleared when the child releases its memory space
>   (which is the case when the child exits, or execve, but also when
>   it unshare(CLONE_VM)).

AFAIK, unshare(CLONE_VM) either returns an error or has no
effect. I thought it was better to not go into depth on the
mechanism of CLEARTID; do you disagree?

>10. You argue that you want to support cases of CLONE_FILES, and so
>    it's hard to use pipes. I don't think you need to support
>    CLONE_FILES for process creation. Why would anyone spawn a new
>    process with CLONE_FILES? I think it's fine to not offer such
>    feature. You are competing against fork+execve, and posix_spawn,
>    not clone.

I did briefly explain one application: "for example, the child
process might be in a different network namespace from the parent,
and the shared file descriptor table would allow the child to bind
a socket and the parent to use it." Should I go into more depth?
Maybe it's not clear that execve unshare the file descriptor table,
so you can use this as part of a clone+execve?

CLONE_FILES is something I actually use in an example that I
commented out. (You can still see it in the .tex, it's under
Miredo) I commented it out because it was too long and complex.
(The example doesn't actually explicitly pass CLONE_FILES to calls to clone(),
but that's just an oversight,  - CLONE_FILES was previously the default and 
I didn't update the example since commenting it out)

And, of course, CLONE_FILES is mandatory for creating, for
example, worker pools for asynchronous execution of system calls,
or things like that.  Maybe I should mention that as well?

>11. I think the paper should be about rsyscall (which is what you seem
>    to do), and direct_spawn() should be one of its application. It
>    should also have other compelling applications (at least 3 in the
>    paper), which you seem to have. I'm not sure there's enough
>    content for direct_spawn() alone for OSDI. Maybe for another
>    conference though.

Hmm. I did cut a fair bit of content on Larry's suggestion;
namely, a section that explained the implications of a good
process creation interface, as a justification for extensive
investment into it.

I attached that section to this email. It was between the current
"Background" and "Overview and Examples" sections. Do you think it
makes sense to include?

Is the implementation section too long/uninteresting?

I'm fairly sure that I won't be able to write a paper focusing on all
the applications of rsyscall before the OSDI deadline, so let's just
take that off the table. I understand the appeal though; I considered
it, but rejected it early on because the different applications are
just too distinct, and the resulting paper would be fairly chaotic.

* arguments
  right now people are forced to write C to access this functionality
  I enable it to be used from Python.


  rsyscall is a reimplementation of libc.
  It's inherently not portable.

** ok ok ok
let's think about this calmly and rationally.

i made the decision to not use C over a long time.

right, we were talking about,
how do we multiplex over multiple processes instead of blocking?

and also how do i monitor readiness in a remote process?
epoll_wait, of course.

but presumably while doing that I also want to be able to service local requests.

so I inherently need this async interface at least for blocking syscalls.

and so I might as well have it for everything.

so the interface either becomes,
I take over your event loop and give you callbacks (which is unacceptable for integration into arbitrary applications),
or each system call is split into "call" and "response" parts.

at that point it's too complex to work with.

I could have a blocking interface which is only suitable for setting up child processes, sure.

so some kind of direct_spawn which creates the process,
then some more blocking calls for them.

but that would be too abstracted and too much work.
well, okay right.

so if I did that, which was my original consideration,
then I would need to define a custom protocol for serializing all system calls.

right now, instead, my protocol is simple,
and doesn't serialize system calls.

which means it's very easy to add new system calls - just define the interface for it,
and we're good; no need to add it to a protocol or anything.


so, let's say we avoid that,
by using my current technique,
being explicit about transferring memory and resources,
and using "near" and "far" pointers.

ok, so if we do that then...

a blocking interface could work maybe? let's see....

ok so sure, maybe we could do that.

but it wouldn't be a generically useful interface! i mean, i guess it would be fine, but...

i mean, you might as well use sfork then. if you were going to do that.

yeah, I would just do it with sfork instead.

so the ultimate conclusion of this constrained interface, just for process creation, is sfork.

but sfork is very limited.
but, sigh, I guess it can work.

so the entire point of this paper, then,
is that we can do direct-style process creation from a generic interface for cross-process operation.
if we only wanted to do direct-style process creation (for one process at a time, in a fairly constrained way),
then, we could use sfork.

well, sfork isn't quite perfect;
it doesn't let you do operations in the parent process.
it's kind of just an incremental improvement over fork/vfork.

and, like, doing operations in the parent process might mean doing async operations!
well, but they wouldn't be async in the child.
well, they might be!

** portability
   Yeah I guess a simple port to BSD could be viable. I mean...
   It's pretty dang hard to do all this.

   ok, so they have an rfork.
   and... we could wrap some basic syscalls and make a protocol for them...


** cleaned up
   So the basic operation we want to do is remote syscalls;
   naively, this just takes the form of transforming
   =int read(int fd, void \*buf, size_t count)= to
   =int read(process\* proc, int fd, void *buf, size_t count)=.

   The immediate issue with this is that it's a blocking interface.
   I can't service local requests while making one of these calls;
   these calls roundtrip to another process which might not respond promptly for any number of reasons,
   so calling this naively may block for a long time, which isn't acceptable for a complex application.
   We need an interface that we can integrate into an event loop.

   There are two options:
   We can transform every syscall into a callback,
   for example,
   =int read(process* proc, int fd, void *buf, size_t count, func_type callback)=,
   or we can split syscalls into "call" and "response" parts,
   for example, maybe,
   =void read_call(process* proc, int fd, void \*buf, size_t count)= and
   =int read_response(process* proc)=,
   or we can do something else; in any of these cases, it's no longer easy to use.

   Making things asynchronous in this way is necessary even in a toy example
   if we want to set up multiple processes at once,
   or if we want to perform an operation in the parent and child at the same time
   (such as connecting from the child to a listening socket in the parent).
   Those are capabilities that fork has, that we won't have.

   This is fine for C, and it can be integrated nicely into C daemons,
   but it's not easy to use.
   I could make asynchronous stuff easy to use by adopting, say, a coroutine system for C,
   and then forcing the user to use it,
   but then it's not a library, it's a framework that takes over your whole program.
   I could loosen the requirement that the calls not be blocking,
   but then it's not useful in a complex application.

   The problem is with C.
   Luckily, 1. people don't really like C these days,
   and 2. it's not actually useful to do this in C.
   No-one is writing their container orchestration system in C,
   or any other kind of application that would want to do complex process manipulation.
   Those have no need for C - except inasmuch as they're forced to use C today.
   There's precedent for interacting with Linux in a C-free, language-specific way (Go),
   so that's what I did.

   BTW, if we did loosen the requirement that calls not be blocking,
   then you might as well use my sfork library,
   which is yet another semantics for process creation,
   one which AFAIK no system has ever done before.
   https://github.com/catern/sfork
   But it's something I've considered and rejected,
   because it won't work for real systems,
   which are concurrent.

   Second off, supposing we're fine with a blocking interface,
   the naive interface still has the problem of serialization.
   To call, say, =int write(process\* proc, int fd, void *buf, size_t count)=,
   either buf is a local pointer and we have complete shared memory between us and the remote process,
   or buf is a local pointer and internally that call serializes the memory and copies it over,
   or we don't pass a local pointer.
   Among other reasons,
   sharing memory with a process at a different privilege level is insecure,
   so we can't assume that we'll always have shared memory.
   So we need to define the aforementioned serialization.
   This makes it difficult to 

   We can have a wrapper which serializes the memory, sure.
   It's not performant, but it's fine.

   What about the internal thing, then?
   Well.
   So say we have
   =int write(process\* proc, int fd, void *buf, size_t count)=
   where the buf is a remote one.
   How do we implement that?

   ok it's fine but,
   what about say, dup2?
   we can use high fds like we said we would originally

   yeah, no non-trivial application could work like this

** hmm
*** expressiveness
   ok so we need to be more explicit about...
   the purpose is,
   the more expressive technique.

   we
   I think this line about features that are difficult to exploit is good.

   and we should dig into that in the fork-style and spawn-style sections I guess?

   spawn-style, only a short section,
   since it's just straight up impossible.

   fork-style, we can talk about at more length.

   so we should actually talk about features that are hard to work with.

   yeah, obviously. naturally.

   let's discuss fork.
**** TODO come up with things which are difficult with fork!
     ok ok ok.

     let's do this.

     and theoretically we can do this in the evaluation?
     or the examples?
     or the background?
***** netns
     making a network namespace,
     creating a socket,
     then using it in the parent.
***** others
      nested network namespaces

      pid namespaces need to be nested inherently
      (because you can't unshare them)

      so yeah nesting is a good one
      although you *can* do it with fork.
*** python
   we should also maybe say that it's a consequence of the rsyscall libc for Python.

   like.

   We are able to express direct-style process creation by using the rsyscall library,
   which allows Python programs to perform cross-process system calls.
   Our implementation strategy can be employed for any language by porting the rsyscall library.

   if we lean into the Python-ness, we don't have to make excuses for it.


   Our contribution is an implementation of direct-style process creation for Linux,
   making use of our work on the rsyscall project.
   rsyscall is a cross-process Linux syscall library, replacing libc.
   rsyscall is currently best supported on Python.
** takeaways
   talk about novel applications in the intro
   
   be clear on the requirements of the mechanism

   (I think it's clear enough right now...)

   ok so the main thing right now is that we need to revamp this "Overview and examples" section.

   and possibly merge in the process background to the examples?

   we need to genuinely evaluate fork vs rsyscall.
   and show things that we can and can't do with fork.

   ok so we have these examples...

   maybe we can just bulk them up yeah.

   current organization:

   introduction,
   background,
   overview on rsyscall,
   examples.

   what we want to insert is:
   1. genuine evaluation of fork vs rsyscall
   2. novel techniques you can do with rsyscall
   3. things one might want to do with processes which are difficult otherwise

   I guess this is all done in one way:
   go through each process-use-case,
   and do it with both fork and rsyscall,
   and show the example.

   SIGH.

   ok fine let's do it.
   let's brew up the examples/sections,
   based on the process-applications.

   i obviously need this section, it's obvious;
   it will provide a reference for others,
   and make my paper eternal.
*** DONE abstracting over resources

\begin{lstlisting}[float,language=Python,label={fds},caption={Passing down FDs}]
child = local.thread.clone()
sock = child.socket(AF.INET, SOCK.DGRAM)
# Bind the socket to a sockaddr_in;
# the sockaddr is written to memory accessible by the child with child.ptr
sock.bind(child.ptr(SockaddrIn(0, 0)))
sock.listen(10)
sock.disable_cloexec()
child.execv(executable_path, [executable_path, "--listening-socket", str(int(sock))])
\end{lstlisting}
    
    

    This is further generalized by Linux's generic tr
    
    a child process can inherit file des
    Most fundamentally,
    

    Passing down a file descriptor

    can open a file and pass it down.

In Unix, the mechanism of file descriptor inheritance
allows a process to be provided a resource by its creator,
while abstracting over the precise identity of that resource.[fn:ucspi]
For example, a process can be provided a file descriptor,
which can correspond to any file in a filesystem,
without the process being aware of what specific file it is accessing.
This is further enhanced by Unix's "everything is a file" design;
the passed file descriptor could also be a pipe, a network connection, or some other resource,
without the process knowing.
As another example,
a process can be provided a socket file descriptor on which it can call \texttt{accept} to receive connections,
without being aware of whether those connections come from the internet or from a local Unix socket.[fn:ucspi]
This abstraction mechanism is the basic principle of pipelines and redirection in the Unix shell,
but it is rarely used outside of the shell.
*** DONE also, pull anything else out of the "background on processes" section
    ok extracted - i just wanted the bit about these techniques not being usable in combination with existing services.
*** DONE concurrency
    a pipeline!

    pipelines are good.

Processes run concurrently,
which allows exploiting the parallelism of the underlying hardware.
Since processes don't share memory,
they can provide a less complex parallel programming environment
than shared-memory thread-based approaches.

A popular parallel programming environment is the Unix shell,
which obtains its parallelism by running multiple processes connected via pipes.
The Unix shell has a relatively constrained form of parallel processing,
but it's also possible to create more complex webs of parallel processes,
where, for example, one process might take multiple inputs over multiple pipes,
or produce multiple outputs,
or the processes might be arranged in a loop.

# hmm what is a good example????

# encryption/decryption maybe? some kind of ssl terminator?
# ucspi requires 6/7, so maybe... that's a good idea?
# yeah I could do some kind of SSL terminator thing?
# and then pass the socketpair to... something else..

    In listing \ref{pipe},
    we create a bidirectional connection between two processes.
    hwere the one 
    between their stdout and stderr;
    something which is not possible in a POSIX shell.

    blah blah blah.

    This is, of course, substantially more complex than the Unix shell,
    but it's also substantially more capable;
    we can pass multiple channels of communication or other resources,
    and we can freely combine these webs of processes with other features that we'll discuss below,
    including Linux namespaces.

    # two additions to the spawn-style background:
    # shell and chainloading are spawn-style, 
    # and spawn-style requires a DSL

    but it allows composition of these techniques with other

    The most basic usage of this is in the widespread distribution of software
    as executables which run in dedicated processes.
**** what example?
     let's just do some made up programs again

     try to be evocative with naming;
     two streams between two processes, for example.

     hmmmmmm.

     is made up a good idea?
     yes.

     two streams of data, maybe going different places.

     ummmm

     maybe uhhh

     ok

     maybe we can think of some way in which direct style is actually helpful..?

#+BEGIN_SRC python
def as_argument(child, fd):
  child_fd = child.inherit_fd(fd)
  child_fd.fcntl(F.SETFD, 0)
  return str(int(child_fd))

audio_pipe = local_thread.pipe()
video_pipe = local_thread.pipe()
source = local.thread.clone()
source.execve('/bin/source', ['source',
  '--audio-output', as_argument(source, audio_pipe.write),
  '--video-output', as_argument(source, video_pipe.write)])
video_sink = local.thread.clone()
video_sink.execve('/bin/video_sink', ['video_sink',
  '--video-input', as_argument(video_sink, video_pipe.read)])
audio_sink = local.thread.clone()
audio_sink.execve('/bin/audio_sink', ['audio_sink',
  '--audio-input', as_argument(audio_sink, audio_pipe.read)])
#+END_SRC

In listing \ref{pipe},
we execute a few programs in parallel,
connected by pipes.
We first create two pipes,
then inherit them down to several processes using a helper function, \texttt{as_argument}.
*** DONE customization without support
    Using some kind of user namespace + mount namespace thing?

    some other customization thing?

    TODO this one is a bit tricky

    nested clone? network namespaces? pid namespaces?

    we should show something using nested clone;

    and we should show something using shared fd tables.

    binding a random port in the child is good but you can achieve that by fd passing.
    but not if you're in a network namespace! that's good then.

    that's a useful thing, sure.

    and, another one is, say...
    running a client outside the thing?

    oh we could do this with the sandboxing thing.

    ugh the miredo thing inherits fds though, sigh.

    ok so that's not helpful.

    customization: use a mount namespace, for sure.
    and override  .. ..... something.
    doesn't really need to be real I guess.
    /etc/foo or whatever.

    oh, and we can do it as a function.
    that would be a nice demonstration of abstraction.

    this is fine.

In many systems,
it's possible to modify a process's view of nominally "global" resources.
In Unix-derived systems, this ability is most influentially provided in Plan 9[fn:plan9],
which allows each process to customize its view of the filesystem with private mounts and union directories[fn:plan9ns].
In Linux, these concepts were implemented as per-process namespaces[fn:linuxns].
Fundamentally,
this allows customizing a process's environment and therefore a program's behavior,
without having to write explicit support code for customization.
For example, Plan 9, unlike most other Unix-derived systems,
did not have a \texttt{PATH} environment variable which was searched by code in the process to find executables;
instead, each process was executed with a \texttt{/bin} directory at the root of the filesystem,
which was a union of many other directories,
and simply executed \texttt{/bin/foo} to run the program named \texttt{foo}.
In this way the set of executables provided to a process could be customized,
without any code to parse and handle \texttt{PATH} or any other executable-lookup-specific customization code.







We'll bind mount a single file, sure.

ok one awkwardness is finding a filename.

should i do uts namespaces???
that seems plausible

yeah. that's good.
or no, it's boring.

let's use mkdtemp to make a tempdir?
yeah yeah ok.

no let's hardcode a path
perfect
*** DONE sandboxing

Capability-secure sandboxing.


    Lower privilege level thing, easy.

    hmmmmmm I think I should just combine sandboxing and capsec.

    or, maybe not!

    or maybe yes.

    it's really just a combination of abstracting over resources,
    and sandboxing.
    yeah I should skip it.

    ok so what kind of sandboxing can we do?
    let's be capsecish, and pass explicit resources.
    and... otherwise just lock it down completely?
    unmount all?

    and we can say, caveat, this isn't a full sandbox, but concise for the sake of space.
    yeah and this can be a static binary

    ok so we can fexec a static binary,
    along with one explicitly passed file,
    and unmount everything,
    and boom, we're good.
**** old
The basic isolation powers of processes are used to simplify application development:
it is beneficial to have a private virtual memory space when developing a stand-alone program.
But most systems have additional mechanisms of isolation between processes,
such as different privilege levels and access to global resources,
which can be used to provide a form of sandboxing.
For example, components which may exposed to hostile network requests
can be run in a separate process, at a lower privilege level than the main program;
in this way, even if an attacker gains control over that component,
the attacker will only have access to the lower level of privileges of that component,
rather than the full privileges of the main program.

As a further development of process-based sandboxing,
the privileges of a process can be explicitly enumerated
in a capability-based security model.[fn:capsicum]
By using previously-mentioned resource passing mechanisms,
such as file descriptor inheritance or namespace manipulation,
and by disabling the process's access to global resources such as the shared filesystem,
we can enforce that all resources used by the process are passed at creation time.
**** writing
At process creation time,
we can not only pass resources and customize the process's environment,
we can also deny the process access to resources that it otherwise gets by default.
This is a key part of creating a secure sandbox for potentially malicious code.

Historically, system calls such as \texttt{chroot} were the main mechanism for doing this;
today, Linux's namespaces system is the most important component.

\begin{lstlisting}[float,language=Python,label={unmount},caption={Unmount all and run executable via fexec}]
thread = local_thread.clone(CLONE.NEWNS)
executable_fd = child.open("/bin/static_executable", O.RDONLY)
db_fd = child.open("/var/db/database.db", O.RDWR)
child.umount("/", MNT.DETACH)
db_fd.fcntl(F.SETFD, 0)
child.fexec(executable_fd, ["static_executable", "--database-fd", str(int(db_fd))])
\end{lstlisting}

In listing \ref{unmount}
we create a new child thread,
again in a new mount namespace using \texttt{CLONE_NEWNS}.
We open the executable that we'll later exec in this child thread,
as well as another file as in listing \ref{fds},
since we won't be able to do exec or open files normally after the next step.
Then we use \texttt{umount},
passing \texttt{MNT_DETACH} to perform a recursive unmount of the entire filesystem tree,
removing it all from the view of this process.
As in listing \ref{fds}, we unset the \texttt{CLOEXEC} flag from the database file descriptor,
so that it can be inherited across \texttt{exec};
we don't need to call \texttt{inherit_fd} since, in this case, the database fd was opened directly from the child.
We then exec the file we opened earlier, using \texttt{fexec},
which allows execing a file descriptor,
and also pass the database file descriptor as an argument,
as in listing \ref{fds}.
(The underlying system call for \texttt{fexec} is \texttt{execveat};
this \texttt{fexec} is a helper function.)
Note that this executable must be statically linked,
or it wouldn't work in an empty filesystem namespace,
with no libraries to dynamically link against.

By removing the filesystem tree from the view of this process,
we can run this executable with greater confidence
that it won't be able to tamper with the rest of the system.
A sandbox which is truly robust against malicious or compromised programs requires some additional steps,
but this is a substantial start.
Such a technique can also be used when a full sandbox is not relevant,
to ease reasoning about the behavior of the program being run,
and protect against bugs.

Even if the process needs additional resources,
those can be explicitly passed down through file descriptor passing,
as we do here with the database file descriptor.
This is the essence of capability-based security,
which Linux can increasingly closely approximate.
*** DONE killing child processes?
    can demonstrate using a pid namespace for that?

    yeah that's good.

    although there's the signal restriction;
    does that matter?

    no not really.
    just use sigkill if you want to kill it.

    well, it is annoying that there isns't an interface...
    actually, you've always needed to fall back to sigkill ater sigterm.
    not doing that is a joke.

    zombies, also, are kept around?
    shrug, I'm mostsly fine with that.

    ok maybe this isn't a super good idea.

    our other wacky example can be one depending on a clone.

    aha yes this is good,
    because we can demo nested clone.

    things we're doing here:

    nesting 

    using a pid namespace to clean up resources


    Namespaces can also be used to.
**** writing
    Process-local resource can also be used to control the lifetime of resources used by that process.
    Many Linux resources are not automatically cleaned up on process exit;
    a poorly coded program may allocate global resources
    and not ensure that they are cleaned up,
    leaving behind unuseds litter on the system.

    One resource which is not automatically cleaned up is processes themselves;
    if we run a program which itself spawns subprocesses,
    those subprocesses may unintentionally leak,
    and be left running on the system even after the program itself has stopped.

    We can use a pid namespace, in conjunction with nested cloning, to solve this issue.
    The lifetime of all processes in a pid namespace is tied to the first process created in it,
    the init process.
    When the init process dies,
    all other processes in the pid namespace are destroyed.

#+BEGIN_SRC python
init = local_thread.clone(CLONE.NEWPID)
grandchild = init.clone()
grandchild.execv(foo_path, [foo_path])
#+END_SRC    

    In listing \ref{pidns},
    we create a child process which we call \texttt{init},
    passing \verb|CLONE_NEWPID| to create it in a new pid namespace.
    To create another process in the pid namespace,
    we clone again, this time from \texttt{init};
    note that this is the first time we've cloned from one of our child processes.
    We exec in the grandchild,
    and we can monitor the grandchild process from \texttt{init},
    just as we would monitor \texttt{init} from its parent process.

    \texttt{init} in a pid namespace has some special powers and responsibilities.
    We can handle these responsibilities ourselves, directly from this program,
    or we can continue on by execing an init daemon in \texttt{init} to handle it for us.

    In either case, when \texttt{init} dies,
    the pid namespace will be destroyed,
    and \texttt{grandchild} will be cleaned up.
*** DONE fuse and shared fd table
    make ns thread from root thread,
    open dev fuse,
    make server thread from root thread,
    pass down dev fuse,
    mount dev fuse,
    open file,
    use in root thread.

    okaaaay.

    this should be good.

    I guess I can say, uh.

    well one attribute is Linux's ability to make usersspace filesystems
    and otherwise mock file descriptors.

    another is running a subprocess to run a service.

    yet another is sharing the file descriptor table.
    # !!! TODO !!!
    # we could just hardcode the paths of everything instead of assuming they're in scope;
    # that's probably better.

    ok so this is using a subprocess to perform a task, that's the key thing.

    fo rhtat, sharing the fd table is useful.

    (incidentally, a usersspace filesystem is useful, but not super useful)

    A library can create a child process to perform a useful service.
    This can sometimes be done more easily when the child process shares file descriptors with the parent.

#+BEGIN_SRC python
ns_child = local_thread.clone(CLONE.FILES|CLONE.NEWNS)
server_child = ns_child.clone()
await server_child.execve(fuseserver_path, [fuseserver_path, "/foo"])
fd = ns_child.open("/foo/bar", O.RDONLY)
parent_fd = local_thread.use_fd(fd)
#+END_SRC

    In listing \ref{fuse},
    we first create a child process, \texttt{ns_child}, in a new mount namespace with \texttt{CLONE_NEWNS},
    and for the first time, also pass \texttt{CLONE_FILES},
    which causes the file descriptor table to be shared between the parent process and the child process.
    We create a grandchild off of \texttt{ns_child},
    and use it to exec a FUSE filesystem,
    which will appear only in the mount namespace of \texttt{ns_child} and its descendents.
    Then we can open FUSE files in \texttt{ns_child}
    and use those files in the parent process,
    through the shared file descriptor table.

    # TODO hmm technically we could open devfuse from local_thread, that would work too.
*** misc
    need to find a space for these:
**** shared fd tables
    we can create sockets after process creation and then share them;
    like we talked about earlier,
    we can do:
    create process
    create socket
    use socket.

    we can also use the socket in the parent I guess?

    oh we can pass it down to another process in another namespace, nice, perfect

    oh yeah, actually, for that matter can we create the netns thread,
    then the client thread.

    that would be cool

    yeah that saves a thread.

    and we'll split it between the two

    perfect, and CLONE.FILES does work with newuser.

    so let's rework this

    that's complex though

    let's consider just NEWNET and NEWNS.

    so we can use the fd in the parent

    which is ifne...

    there's a complex use case, which is,
    open the file in one task in a new namespace,
    and then pass it down to another task not in that namespace.

    let's see. hm.
    this isn't necessarily something that fits into our existing use cases.

    maybe I can just skip it?

    let's just skip it for now, I'm avoiding the issue

    nope! got it!

    just mount a fuse filesystem inside,
    and use the mounted file from the outside.
    boom!
**** DONE nested clone
     This is handy, mmm.

     can we think of a way to handle this that doesn't require a bind?

     yeah, let's figure out how to get rid of the .ptr.

     we can't viably use network stuff if we do that. but I guess it's fine.

     ok so we probably should do nested clone before the clone_files thing.

     need to think of a good nested clone.
     a pid namespace, maybe?

     oh, yeah, let's do a pid namespace.
     and have our process as pid 1 in the namespace.
     that works!

     and that can be about, resource cleanup, I guess?
*** thoughts
    maybe we don't need to include each comparison in the paper?

    maybe we don't need to put the comparisons right next to these examples?
    we can do that later, in evaluation?
    although it would be helpful; we'll include one.

    ok, that seems good, seems good.

    um, except,
    the entire point of having the comparisons right adjacent,
    is so we can show off advanced techniques.
    
    OH OH OH FUSE MOUNTS YES OK

    perfect!

    ok so that's how I can use the shared fd tables!
    just mount a fuse filesystem inside,
    and use the mounted file from the outside.

    oh and we need nested clone to do that, too.
    well, to do it sanely.

    oh, hey, we can use it multiple times, actually.
    we can make the ns,
    then clone the server off the root thread,
    using the shared fuse fd.

    ok so that's a good example.
*** design
    ok so I guess we'll stay with this overview and examples design?
    let's look at other papers.

    oh i should have my end be called "conclusions"

    ok I guess I could have "overview",
    then "Applications".

    Overview can have this one create and exec example.

    Yeah, that makes sense.
** evaluation
    We quantitatively answer the following questions:

    - Does direct-style process creation on Linux allow for simpler programs than the alternatives?
      We implement and compare similar functionality using both direct-style and fork-style.

    - Does a clone-based implementation avoid the well-known performance overheads of fork?
      We evaluate clone against fork on several microbenchmarks.

    - Does a high-level Python direct-style process creation interface
      have acceptable performance relative to the Python standard library's spawn-style interface?
      We benchmark direct-style process creation against spawn-style.

*** let's do this
    OK, so the clone versus fork benchmark should be an easy one.

    Also the implementation of things with fork.

**** clone vs fork benchmark
** related work
   need a related work section I guess.

   it's just mandatory, even if it's not closely related.

   we can talk about cross-process syscall techniques,
   and about direct-style process creation.

   criu

   that other rsyscall paper about forwarding syscalls

   transparent migration of linux machines for SSI,
   such as popcorn linux,
   typically forward syscall back

   that google sandboxing thing

   io_uring

   this thing http://madchat.fr/coding/c/c.seku/SyscallProxying.pdf

   "condor's remote system calls"

   process capabilities, is something we're saying a lot
   guess I should distinguish myself from pidfd
*** outline
    couple sections of related work:

**** direct-style process creation
     As we mention in section \ref{background},
     many academic operating systems have direct-style process creation.
     We are not aware of any previous instances of direct-style process creation in a Unix-like environment.

     The closest related work known to us
     is efforts to build Unix compatibility environments, including fork-style process creation,
     on academic operating systems which natively use direct-style process creation.
     Such compatibility environments could theoretically be bypassed to perform direct-style process creation
     using the underlying primitives,
     but regular Unix system calls could not be used to create new Unix processes in a direct-style way
     in such an environment.
**** remote syscall techniques
     There are a large number of Unix-based systems which do something called "remote system calls".
     Most such systems do not allow a single program to manipulate multiple processes.
     Those systems that do allow manipulating multiple processes
     are generally oriented towards debugging and introspection,
     and are unsuitable for a general purpose system.

     Many systems use system call forwarding to implement migration in a computing cluster.
     HTCondor and Popcorn Linux are two examples.
     In both systems, processes can be live-migrated between hosts;
     when this occurs, the system will automatically forward IO-related system calls
     back to the original host.
     
     Some systems intercept system calls and forward them to another process to be implemented
     as part of a virtualization system.
     gvisor\cite{gvisor} and ViewOS\cite{viewos} are two examples.
     In both systems,
     system calls made in one process are intercepted,
     and serviced by another process.
     Also relevant is seccomp system call interception,
     which can be used to intercept system calls and emulate them in another process.

     Some systems forward system calls elsewhere for asynchronous execution.
     FlexSC is one example.
     System calls were forwarded to dedicated system call execution threads.
     The recently added Linux feature io_uring is, in some sense, another example;
     it supports sending a subset of system calls to the kernel,
     which executes them asynchronously,
     rather than blocking the thread.

     Some systems call system calls in other processes for debugging or introspection purposes.
     strace, gdb, and CRIU are examples of this.
     These systems typically uses ptrace,
     which is capable of operating on multiple processes at once.
     Unfortunately, as discussed in section \ref{implementation},
     ptrace is unsuitable as a mechanism for a general purpose system since it is not re-entrant;
     a process which is already being ptraced cannot be ptraced again,
     so using ptrace as a cross-process system call mechanism would disallow use of strace or debuggers.
**** process capabilities
     # We mention the concept of process capabilities while discussing the design of rsyscall in section \ref{overview}.
     Some notion of "process capabilities" has been recently added to Linux in the form of the pidfd API,
     which was directly inspired by Capsicum's process descriptors.
     This notion of process capability is quite limited, however;
     pidfds and process descriptors only allow sending signals to a process or waiting for its death,
     rather than exercising full control over the process.
**** capsec libc alternatives
     The Capsicum system provides a set of system calls
     which can be used to provide a capability-secure sandbox.
     Latter efforts[fn:oblivious], including CloudABI[fn:cloudabi] and WASI[fn:wasi],
     are focused primarily on backwards compatibility with the existing POSIX environment, and ease of porting existing code.
     Programs ported to the capability-secure environment
     are typically launched with an accompanying spawn-style wrapper.
     This means that capability-secure processes are created primarily by an API
     that is distinct from the capability-secure syscall API they have access to,
     which makes it harder for a capability-secure process to spawn further capability-secure processes of its own.
     Direct-style process creation could make it easier to create capability-secure processes 
     from the capability-secure syscall API,
     and reduce the need for a separate spawn-style wrapper.

     Also related is the Principle of Least Authority Shell (PLASH).
     PLASH provides a spawn-style API (in the form of a shell)
     to launch processes in a capability-secure environment.
     PLASH, like all spawn-style APIs, abstracts over the native Linux environment,
     and is therefore limited in what kind of processes it can create.
* questions
  page length? 14 too much?

  citations? do I need them now?
* things to do
  ok, let's strip out the "thread" term.
  it's too confusing,
  and I don't want to unify.
* huh
  I rememberd why I named it direct-style

  it's because fork is (deeply) a continuation primitive - it's, uh, some representation of LEM.
  usually continuation primitives are DNE, but fork is LEM instead, how wacky. well. maybe.
  I mean, in some really super pointlessly deep sense, anyway!

  anyway, that's why I called it direct-style

  well, no, it's just that I wanted something nicer than "imperative",
  and the alternatives are clearly "declarative" or involve advanced control flow.
* comment notes
  page 5 seems to have the comments on the wrong page?
  same with page 29?

  same with page 31?

  a few of the comments seem to be truncated - such as on page 28?
** thoughts
   ok so he says again, put the trick into the introduction.

   but I fear that if I do that, people will immediately dismiss the work.

   I feel like I have to make a kernel extension purely to show that I can,
   even if I don't use it.

   but, that would be a good way, for sure.

   oh, ugh, but file descriptors are how I do the interrupt thing...

   yeah a naive blocking syscall isn't suitable, I need something async;
   it would have to still be async,
   and really, have an identical interface - on both sides!

   but, yes, we should mention it's entirely in userspace in the intro.
** notes about changes
*** DONE baumann
    he's on the PC!
    we need to really mention him then - especially in related work.
*** userspace
    let's say it's userspace

    and let's justify it!
    we chose not to implement it in kernel-space because we don't think it would be beneficial!
    the naive syscall is hard,
    it would need to be async,
    and so it's kind of pointless!

    what might be ideal is a BPF program associated with a process context;
    that would work nicely to lower the overhead.

    maybe justify it...
*** builder pattern
    should we mention that?
    eh no. eh. maybe.
*** applications intro
    he says it's not clear what's novel,
    and what exactly these are;
    applications is also a bad word.
    examples does seem better.
*** use a table in the background section?
    This would be handy, yeah.

|                       | fork                         | spawn                  | direct                          |
|-----------------------+------------------------------+------------------------+---------------------------------|
| Allows any caller     | No, no threads, small memory | Yes                    | Yes                             |
| Programming model     | Complex (returns twice)      | Simple (one and done)  | Simple (imperative)             |
| Maximally powerful    | Yes, can call any syscall    | No, limited interface  | Yes, can call any syscall       |
| Error handling        | Poor, requires IPC           | Poor, not fine-grained | Good, reports individual errors |
| Non-copied attributes | Mutated by code in child     | Set by arguments       | Mutated by code in parent       |

*** add references
**** 
    need a reference for a large spawn interface
    maybe a glibc bug for adding some feature?

    oh we specifically need to cite that indicating errors well requires a big interface back up to the user.
    again, posix_spawn bug maybe?
**** ryscall project
     need to cite it I guess he says

     I guess just the github url
*** DONE we is ambiguous
    I see, this is unclear,
    because we don't know if "we" is calling in the parent process or the child process.

    Yeah that's a good point.

    eh I disregard it
*** DONE remove periods from clone newns stuff
    they seem to think it's a wrapper over clone. hm.
    I should get rid of the periods, I guess.
*** DONE name the syscall response loop
well what do I call it?

the rsyscall server I guess?

the syscall server.
*** evaluation
    when is fork better and when worse?
    I guess that would be a fine thing to say.
    but first we need to know a time when fork is better!

    well, some uses of fork that involve actually concurrent user code, it's better;
    or for other things like memory snapshots.

    but for anything ending in an exec, probably not.
    well you could want to be concurrent when ending in exec - but wait you still can with rsyscall lol.

    ok so let's focus on when fork is better...???
**** low level benchmarks
     ok so the main performance thing is memory.

     so what else do we even compare??? i guess this is fine but trivial

     just need more stuff.
**** high level benchmarks
***** more operations
     we need to do more operations,
     and compare them.

     a real complicated setup.

     comparing all the fork-style and direct-style things would be good...

     well doing them spawn-style would also be fine.


     ok so we'll do a chdir, sure,
     and...
     a cloexec disable, sure, too.

     it would be good to do something that 

     i could do a mount maybe

     except it wouldn't work because of the lack of clone_newns

     let's just compare listing 1, that's easy.

     aha ok we don't need to use preexec_fn if we pass pass_fds.

     or cwd for that matter! nice haha ha hAAHHAHAH.
***** benchmark syscall overhead
      ugh.
      it's going to be like half a milli.

      but we have to, I guess.

      we'll test with getpid

      ugh but it's SOOO slow fug.

      "this suggests that our decision to be all Python is too slow"

      I feel we can own up to that.

      "Native optimizations will be needed for more serious use"?
      no way

      we can use it seriously

      the scaling behavior is immediate and straightforward
      with the number of syscalls made.

      The constant cost of each is high,
      but that can be brought down over time
      with optimizations and
**** other evaluation
     hm m m m m m

     ok so 
**** macro-performance
     maybe we should talk about tss integration's performance?

     we haven't had any perf issues.

     micro-benchmarks of individual things: fine.

     comparison to state of the art: ok.

     so we need to benchmark individual syscalls.

     and expand this comparison to Python subprocess?
**** mention in evaluation
     Our goal is a powerful approach to process creation that is easy to use correctly.
     Performance of our implementation is secondary,
     but the overhead of our implementation must be low
     relative to alternative approaches.
**** benchmark tss integration vs previous things
     vaguely

     oh oh, we can mention that previous techniques took minutes or hours to start up a system,
     whereas ours take seconds.

     detailed benchmarking is difficult,
     because previous systems are not directly comparable -
     the system we have built with direct-style process creation is much more functional.

     maybe I could run mmia integration and see its speed?

     run egregression! perfect

     Taking one representative example, one test built with an old library took
     370 seconds to run,
     mostly spent in system startup time.
     With the new library, the same test took 16 seconds.

     maybe 
**** mention socket activation
http://0pointer.de/blog/projects/socket-activation.html
**** syscall performance
     gotta do it.

     ok let's do it.

     ok so we did it.
     ugh. ok. ok ok ok.

     ok ok.
**** benchmark both ways of writing
     ugh. this means we have to get them running.
     that's something we could do in theory.
     but, tricky.
**** evaluation feedback from my booper
     kill 6.2

     be more explicit about what we're evaluating and WHY we care in 6.3


     i should explicitly state that performance is not a priority
     but it's satisfactory
     and say that right away in 6.3!

     "custom code"? no, code specific to each daemon,
     all code is custom!!! duhhhh just say code
**** mention that
     programmers without experience with Unix
     can program with direct-style process creation,
     passing down file descriptors,
     without getting it wrong.
**** notes
     just answer the quesion immediately!
     is it simpler? yes!
     but say it good

     maybe not even need question! combine into one statement
**** TODO thicker lines on graph
**** TODO make graph lower height
**** bait and switch?
     well, let's wait for another review;
     maybe mentioning it's userspace in the intro will be enough.

     ok he didn't mention bait and switch but he also didn't read it thoroughly hmmm

     i think bulking up the intro will be sufficient.
**** conflicts
     vijay
     larry
     nico
** VIJAY FEEDBACK!!!!
every word of this is precious
*** intro
11:11
The intro should also be beefed up, talk about related work more with
citations, and should give a teaser of the results in the rest of the
paper
11:11 
Try this: imagine the reader can read only the intro. By the end
of the intro, you want them to be convinced to accept the paper
(edited)
**** ok intro ok
     I can definitely beef it up more.
     definitely can read it with the view of,
     the reader can read only the intro!! they need to be!!
     convinced!!!
*** evaluation
I do think the evaluation is a little slim
11:09
I think you want to empirically show different applications using the
new primitive, and how it helps them
11:09
I'm not sure a python sub-process is a strong baseline
11:09
you should compare to something more optimized
11:10
I'd want to see more experiments evaluating different aspects of the system
11:10
and integration with applications that you can talk about in detail
*** show different applications using the new primitive
    and how it helps them
*** compare to something more optimized than python sub-process
    hm. well. it's slow but, that's not the intent here.

    let's not build up my defence mechanism here so fast!

    python subprocess is not a strong baseline.

    ok there are at least two metrics of comparison:

    performance, and
    expressivity.

    am I comparing to the most expressive style?
    no! for sure not,
    there are much better spawn-style libraries out there.
    but, what's the good *baseline*?
    y'know, standard, kinda thing?
    posix_spawn I guess.

    and certainly python isn't the best performance baseline.
    we could compare to posix_spawn directly...?
*** more experiements evaluating different aspects of the system
    ok. what other aspects of the system are there?

    we could start up many processs in parallel,
    or serially,
    we could measure the performance of some of the advanced things I'm doing in examples.

    we could...
    look at the performance of fork communicating errors back over IPC

    we could look at UDS fd passings performance relative to shared fd table.

    we could look at the cost of making a nested child?
    that could be interesting.

    or the cost of bookkeeping, like inherit_fd.

    yeah we can show that performance doesn't decrease with nested children.

    or with more unusual clone flags?

    yeah we use nested children twice, should definitely talk about their performance.

    we use a lot of unusual clone flags...
    but is it clear that those don't affect performance?
    we might as well just say it.

    these are new features, they're kind of in a separate section.
**** different clone flags
     Passing different flags to clone has no significant effect on performance.

     yeah, that seems fine,
     let's actually test it though.

     hm. lol. not true, making these namespaces is significantly costly.
     that's cool, it means our overhead is dwarfed by the overhead of the namespaces we enable.

     anyway, definitely something to mention.

     maybe should uhhh...

     have some other kind of axis?

     we can just say "none of these scale poorly with memory usage".

     yeah we can just have a table of times.

     and with getpid, too.

     is there anything else we might want in the getpid table? hm.

     oh, we can have getpid and clone times both in the same table. yeah, that sounds good.
     maybe one other thing, hmm.

     right so the arrangement is like this:

     - run with stdlib
     - run on local thread
     - run on nested thread
     - run on doubly nested thread
     - run on nested thread with CLONE.NEWNS|CLONE.NEWPID

     and the top is
     - clone/exec
     - clone/exec with CLONE.NEWNS|CLONE.NEWPID
     - getpid

     yeah that sounds good
**** nested children
     we use these a lot, definitely good to bench them

     ok cool we benchmarked process creation nested once and twice,
     and also getpid.
     those are good numbers.

     we should also include creation with different clone flags on the same graph.

     could probably just say double nesting has no effect on the performance,
     rather than including a separate line

     ok so we definitely need a bar chart for this I guess.

     hm i don't see anything caring about colorblindness so I guess i'll just go with colors to distinguish.
*** integration with applications that you can talk about in detail
    OK, ok ok. I can talk about tss/integration in more detail,
    that should be fine.

    Just need to put more words there.

    what can I say? hm.

    well, what would people want to hear?
    just more depth, I guess!

    more specific details,
    and also more benchmarking I suppose.

    there's no immediate baseline.

    right okay,
    so we'll name it as the "integration" library.

    we'll talk about performance I suppose:
    much improved system startup times through techniques that were previously infeasible.

    any other performance things?
    this cascades through to many parts of our system,
    allowing much faster iteration,
    or binary search.

    and pid namespaces, sure, sure.
    what other features are we using?
    let's see where we're passing down fds.

    we should look at the parts where my insanity has most heavily breached through,
    like test_tps or the pillar stuff,
    to see my stuff.

    oh, sims, is another thing.
    hm.

    i guess we are able to, pretty flexibly integrate shared memory things into our event loop,
    by starting up processes and clients for them?

    hmm is that really substantial though?
    we could do that before. hm.

    ok, so another thing is, i guess, passing fds around inside our program?
    that's directly enabled by easy file descriptor inheritance, for sure.

    yeah that seems plausible,
    again, it's something enabled by easy file descriptor inheritance.

    ok so I think that's all I have from just reading the code.

    what else is there?
    there's, the fact that we're running a lot more different unique programs, I guess;
    which is enabled in part by easy process creation... but not a ton.

    let's go ahead and do this.

    what other performance things could we do?
    we could compare across a lot more old and new tests;
    but that's just bulking up that one snippet.

    what other perf metrics...

    well so these are end to end, right..
    system start is the main thing.
    what does systemd-analyze look at?

    it talks about boot time, sure.

    we should mention that we don't run system-wide, maybe?
    we strongly considered system-wide things like systemd,
    but they were ultimately unsuitable do to their orientation towards running system-wide
    or session-wide.

    we could compare our time with socket activation,
    relative to time taken when you do things in strict dependency order?
    that's probably a good idea...
    we can also have the datapoint of the old system too.

    what about detecting system startup?
    that's kind of a socket-activation thing, we defer doing that...
    but we do have various techniques, like connecting in through telnet.
    yeah hm. that's really just socket activation, let's put that in there

    we could mention, maybe... the ability to interactively start things?
    from a repl?
    i'm not sure that's a consequence of direct style, hm hm.
    mmm...

    well, it's a consequence of the fact that we're programming system startup in a real programming language,
    rather than some DSL,
    which is in turn enabled by direct-style. hmm...
    and that it's all in one process. that's hard to do, many things have start scripts!
    yeah we can elaborate this nicely.
    well, we already were doing things all in one place with, like, supervise.
    the API didn't fundamentally change - although, we did get more able to do fd passing, for sure.
    let's look at the diffs since the rsyscall conversion?
    shrug, not much there. hm.
**** file descriptor inheritance
     we should center this,
     and talk about socket activation as a consequence.

     and also, passing fds around as a consequence.

     and also, easy connection to clients for shared memory as a consequence? maybe?

     One of the major benefits has been easy use of file descriptor inheritance.
     Programs can have file descriptors passed down flexibly.
     We've found that programmers are able to use fd passing blah blah blah.
     Our experience has been that with fork-style or spawn-style interfaces,
     all file descriptor logic must be centralized,
     whereas file descriptors in a new process can be built up over time using direct-style,
     simplifying the implementation.

     let's put the socket activation one first.
***** socket activation
      One benefit of easy fd passing has been socket activation,
      blah blah speed up blah blah shared memory blah blah.


     we can talk directly about passing down file descriptors
     to perform socket activation for shared memory transports.

     that would be a good detail.

     oh and also the management interfaces I suppose,
     socket activation of those too.

     good performance thing

    we could compare our time with socket activation,
    relative to time taken when you do things in strict dependency order?
    that's probably a good idea...
    we can also have the datapoint of the old system too.

    what about detecting system startup?
    that's kind of a socket-activation thing, we defer doing that...
    but we do have various techniques, like connecting in through telnet.
    yeah hm. that's really just socket activation, let's put that in there
***** fds as interface
     how do the sims really fit in?
     they are python programs,
     which operate on file descriptors,
     and which can be flexibly implemented "native" or efficiently by an external process,
     with file descriptors serving as the interface.
     on the other side,
     a file descriptor can also be a native Unix domain or internet socket,
     but can also be pointing to a client for some shared memory transport,
     allowing such transports to be integrated efficiently along with everything else.
     such file descriptor passing 
     this is all thanks to a file descriptor based interface,
     enabled by easy file descriptor inheritance.

     This ease of fd passing has, in turn, allowed the file descriptor to be a key interface.
     File descriptors can be used both by internal Python implementations,
     or by external processes implemented with native code.

     could we talk about performance here?
     I guess we could find two things that implement the same thing,
     and show the speed-up.

     i guess i have the... nasdaq sim. that's the only one.

     well, we can try it.

     or the marketdata sims. yeah so those could be interesting to compare performance on,
     except they're completely orthogonal,
     so would be kind of pointless.
**** complicated pipelines
    we also create complicated pipelines for processes which directly speak to each other.
    that's kind of an instance of socket activation.

    we won't explicitly cite fd inheritance,
    but we will talk about this after fd inheritance.

    just, easier ability to make networks of processes.
**** remote stuff
    sure, we do use the remote hosts stuff.
    can mention this as an aside.

    we also use other features of rsyscall which are not covered in this paper,
    such as the ability to operate on remote hosts through capabilities for remote processes,
    and the ability to perform system calls in parallel across a pool of processes.

    one aspect that we're using heavily,
    but which hasn't been mentioned in this paper,
    is remote operation.
**** mention things I do in rsysapps?
     hmm, that's not super proven;
     I'm not sure it's useful.
**** cpusets
    we could talk about cpusets
    but that could also be easily managed by traditional stuff?
    well, not really! you'd need to wrap everything in taskset,
    which is a hassle.
    well, not too much of a hassle. it's just standard spawn-style.

    could be worth it, at least i'll keep it in this list so I don't keep having the idea.

     
