* stuff
so do we really actually just one a single thread,
per connection,
which just blocks on calls,
can only support one call at a time,
easy, simple to implement.

doable in C even.
with a C interface

so okay
suppose we had this.
then the question is thrown into sharp relief:
how do we multiplex over multiple processes/hosts???

duh
we have a file descriptor interface

which is readable
when the thread
is runnable/has returned a value

also, I *could* pipeline them:
I could send multiple at once,
and get them back one by one.
(In the same order? Yes)
But let's not focus on that, that is not that important and also maybe bad.

this is nice and simple

so this is kinda translating a completion interface into a readiness interface


anyway so I have some file descriptor,
which allows me to run any system call over it,
and it's readable when it's done
(vn one at a time ofc?)

okay is it just as simple as
I have a C library

with void read_call(stuff)
and stuff read_response(void)

then I don't have to expose any serialization stuff.
i can just use structs internally

actually more like

error read_call(int procfd, int fd, size_t count);
int read_response(int procfd, char* buf, size_t bufsize);

oh and conveniently I can just completely avoid defining my own headers and enums and structs and such,
since i'll just pass things through blindly

ugh although pointers are difficult, scatter-gather IO is tricky (though actually probably I should just not support that)

look for documented linux syscall interface
think I heard about that for fuzzing

writing this by hand really doesn't seem that bad

consider the simplest scenario which is where we don't care about being nonblocking across host
we just have the interface be,
exactly the syscall interface but with an extra sargument for the procfd
the sysfd

also there are multiple ways to achieve the goal of asynchronicitiy

we could have ujst epoll_wait be asynchronous and monitorabel by readiness for completion
we cuold have a generic way to await on readability of any fd on the remote side
bridging them, that is

what about cancellation?

we might call an epoll_wait,
and then readiness tells us completion,
but what if we want to make another call in the meantime on this system?

we need to be able to do that
so having syscalls be cancellable seems not very generic

also calling epoll_wait is not right, we should call that in "userspace",
and only see readiness in the main interface

okay, if I wasn't asynchronous.
i'd just,

the calls would all look like normal calls,
with an extra argument.
and we'd just have some internal serialization,
very normal.

maybe to monitor readability I need another connection?
that way I don't have cancellability?

but what if I want to monitor readability of a bunch of different fds? i'll allocate and never free connections.

so the ability to make a call then monitor for readability to see when it's done, seems good and easy.
just splitting up epoll wait into two calls achieves it automatically.
but, it's not cancellable!

what would this be in a protocol level?
if i had a stream dedicated for readability notifications...

having all calls block, means I nicely have to explicitly allocate new threads,
through my previously-thought-of mechanism of clone passing an fd. nice.

it's like an interrupt?
I get an interrupt for readability?

oh and what if I want to get readability notifications while in the middle of another syscall?
well, impossible.

resume me when this guy is readable
seems reasonable.
extensible to,
send me a message when this guy is readable,
which is,
be readable when this guy is readable.

readability really is easiest for the kernel to implement.
the kernel just translates interrupts into wakeups of blocked threads.
easy peasy
all we need is some way to indicate what interrupts we're interested in receiving

when we enter an epoll_wait on some set of fds
we say we want interrupts for all of them.

to do the same for my syscall fd,
we need to explicitly request interrupts before waiting.
then un-request interrupts after we're done waiting?

but in practice the way we get an interrupt is by getting completion notification for a syscall.
so isn't it best to have that be the central conceit?

maybe, maybe.

what other ways could we be notified?

(really the notification/interrupt is the resumption of the thread on the other side.)

still wonder what this is at the protocol level

okay but, if we get a completion notification for a syscall,
we need to be able to cancel that syscall.

i mean, maybe we just have two threads.
one for readiness and the other for actual syscalls 

oh, i really can't have an all-blocking interface, because, the network hop is so expensive. it will be weird...

so
i could have a separate connection/thread for readability notification
and just, i open/close those threads as I want

ugh having a separate thread is inconvenient

maybe i can just have a separate mode

okay so I guess readiness wakeup/readiness notification is just,
a bridge between the interrupts of the remote side and the local side.
you'd want the same with futexes or whatever

also you should return the thing to await on
(or maybe not)

so the ideal is then:
an asynchronous syscall interface,
taking an opaque thing and returning an opaque thing to wait on,
(maybe)
augmented by a cancellable readiness notification registration thing.
* notes
C library interface with a bespoke protocol underneath
two issues: defining custom logic for each syscall, and defining a custom serialization/protocol for the stuff.


It would be nice to instead have a protocol interface
no, no, there's no need for that.
I can just poll on the fd and do the operations.

no.
a protocol is better because then the user can handle IO,
*and* handle retrying on partial reads, and all that stuff.

so all I want to get in my library, is a stream of data in,
which eventually leads to a fully parsed response.

i could just use capnproto serialization.

audit and seccomp-bpf aren't suitable because they don't actually look at the pointers.


nice rqusetest b 

* HAHAHA
vfork!
implementing the cancellable readiness thing with select!
write memory, read memory!

RETURNING FROM EXEC/EXIT LMAO YES BEAUTIFUL

wonder if the registers change


now what I need to do is, call syscalls *really* directly so glibc doesn't get in the way with this errno stuff.


exec makes a new stack for you that the process will run on, so you don't have to make your own stack.
but clone for threads, you have to manually creattoeac


to ask on irc:
why SHOULDN'T i vfork all the time?
and, how CAN i make syscalls directly without errno, without assembly


also can I get rid of the read/write by just,
reading and writing from the connection file descriptor?

MSG_WAITALL does the write part, as an argument to recv...
and send just alwys blocks, for the read part!

then i'm down to a single function call in a loop

recv MSG_WAITALL on stdin, cast to syscall struct, make syscall, write response.

does splice write all the stuff? without blocking?

maybe I don't even need to go through local memory?

and I guess I'll just use the exact interface of seccomp trap to userspace?

you could implement filesystems with seccomp trap to userspace.
the BPF filter could even make it efficient, kinda!

conceptually, it would be nice if things were just modeled as,
we have a pipe full of systemcalls from the user,
and we have a BPF program attached to that which filters it,
and we process the ones it sends us...
and sometimes it just kicks them back to the user program to be run there.
but I guess that's close enough to what we have.

 
* syscaller musings
Hmm.
I'm tempted to have the syscaller be a separate structure,
and have separate notions of a pure ProcessContext with associated FileDescriptors
(which don't by themselves let me do operations in that process)
and some means of accessing a ProcessContext, which any individual IO structure will use.

No, that is absurd. We wouldn't be able to actually own and close the fds!
To properly do this we need ownership. Which means a given FD must be tied to a ProcessConnection.
So an FD holds a reference to a... Syscaller? or a LowLevelIOInterface? Hmm...
to close we only actually need the IOInterface.
but to do further operations we need a Syscaller.
so it seems like the basic thing should just have the IOInterface,
while more stuff has the syscaller.
Maybe we should just state that every connection has an associated page of memory.
Therefore it is already fully-functioning.
But isn't there more stateful stuff?
Like pipes, say!
To do a splice, I need a pipe!
And pipelining demands still more sophistication.
Although I can only pipeline through one process connection at a time, since the pipelining will cause problems otherwise..
I guess pipelining is controlled at the ProcessContext level.
Remember what I originally wanted, when I was planning on using serialization: A simple way to do remote syscalls
Now I have to resort to pipelining for good performance, I guess.
Or do I? Let's just stick to something simple:
Each fd is associated with a SyscallConnection or something,
which contains all the resources needed to make any syscall.
Likewise with each piece of memory? But then what of the syscall buffer and the remote sides of the syscall and data pipes?
We'll just require the syscall buffer to exist, and the remote sides of the syscall and data pipes are internal details.
Wait, I think that circular reference is fine.
It will require use of with to preform orderly destruction, but that's OK
OK, so all memory and fds can have a reference to this Connection thing, including the internal ones.

Maybe if my abstract interface is just, perform any syscall?
Should I abstract over memory?
Obviously yes.
Should I abstract over fds, and only provide owned ones?
can't really do that in general.
also, what if I wanted to use memory in the remote place?
I guess I should only abstract over *memory involved in the system call*.
but er no sometimes I might want to pass a specific pointer to read into *remotely*. for the side effect.
well maybe read() can take an optional buffer argument that is either a local buffer or a remote buffer.
guess that could be good.
nah forget it I don't think I'd ever actually want to deal in memory in the near term.
so let's just abstract over memory completely.
and that'll be my interface!
a buncha methods returning ints!
with memory completely handled.
and then i'll build my common fd abstractions on top of that.

What about concurrency?
What about nonblockingness?
Above or below?

well ideally when we call this thing it would only block the current task.
and other tasks would be left free.
but, note that the object is still "globally" blocked.
we can only have one person calling one of these syscalls at a time.
so there's all the same deadlock issues within a single "process"
so maybe it's therefore not so bad that a native Syscaller would hard-block when we call it?
it's a protection against anyone else interfering with its process, y'know...
could we make it truly async? not without some effort.
to make it truly async would require using a native IO library.
ultimately we'll have to make a blocking call in our thread.
I think it is fine that they are marked async. It demonstrates that they may block.
And shows the silliness of an explicit marking for async? though it's a nice effect system...

man. a nice serializable thing which is automatically nonblocking would be exactly what I want,
since it would be a cross-language IO library.
bah! oh well! we have no cross-language type system, so we can't
computer science is not yet advanced to perform such feats.
also it's dubious because we can't really re-expose exactly the syscall interface.

so, anyway, handling concurrency above the interface is the right thing, I think.
even though it leaves this weirdness of tasks marked async, hard-blocking.
but ultimately we need to block, so...

ah, and how do we bridge this into the native trio thing?
the remote syscalls work fine, I can just do a select and it's good.
good-ish...
I'd need a nice queue underneath the syscall interface...
to link up call/response...
since multiple people need to call at once actually...

note: try to perform IO first, and only select if it returns EAGAIN

so anyway, briding into the native trio thing works fine with remote syscalls,
since the object is truly async,
and doesn't block other tasks.

but the native calls! aie!
I guess I'd have some thing where I, uh.
I forward epoll_wait to trio so that it really does not block other tasks?
then I rely on the EAGAINing of everything else?
I want to interrupt the blocker guy if I need to block on something.
I really like the "make a syscall to wake it up" thing, it should Just Work.

actually okay, maybe I do need to just expose a wait_readable call.
because I can't do the select hack in userspace...
and it returns either "readable" or "EAGAIN" though I guess the fd could be closed under me, eh.
no wait it can't because of blocking, eh.

so okay sure I'll expose the wait_readable call, whatever.
since I can't expose the file descriptor number of the remote thingy fd in userspace.
but wait, I need to do that to avoid it when dup2ing...

well, even with local syscall I need to cleanly handle dup2
ok, I will avoid dup2 problems by making the remote syscall and data fds be high numbers.
local dup2s don't have any serious problems, if I overwrite something, oh well.

hey neat, when I vfork the exceptions in the subprocess propagate back to the main process.

oh anyway so when I call wait_readable with remote calls,
I can also make a syscall at the same time (like with native),
and it will interrupt the wait_readable (not like with native)
but I can make it like that for native. just cancel the wait under the hood?

when I make another syscall, it interrupts wait_readable automatically.
then the async layer will not wait_readable again until it's finished performing other IO.

I guess that's a nice invariant.

okay, we'll just have wait_readable only return when the underlying FD really is readable.
And the conceptual reason to call wait_readable: It doesn't take the lock on the object.
Other syscalls can go through while we're calling it.
That will be nice and uniform with native.
And we need to have such a wait_readable call as a primitive to avoid deadlocks.

That is, a deadlock where we call some blocking syscall, and it blocks, but
then we realize we need to call some other syscall to make it
unblock.

Right, the reason we need the primitive isn't because we want to get asyncness in there,
it's because we don't want to take the lock on the object.
which is conceptually the same as not globally blocking

* naming vfork thing
So the real system call wouldn't be clone, it would be... unshare?

Well, vfork is the best way to view it I think. It's just a vfork that doesn't return twice.

Meh, I'll call it sfork.

* 
if syscalls were RPCs

you could set up the new process/do the exec stuff in a remote process

it would be nice to have an exec that creates a thread in the same way as clone

but really clone doesn't need to take a stack argument, does it?
why can't you just conditionally return?

yeah and I mean, why can't I just say, start thread running with these registers

exec and clone are kind of the same thing


anyway if I could RPC to another process space, I could set up its memory with mmap just fine.
then kick it off and let it run.

adding RPC to another process space...
targeted, surgical interventions that massively increase power

anyway if syscalls were RPCs,
and you could just start with an empty address space and map a bunch of things inside it,
then start a thread inside it,
then that would be real cool y'know

unshare
* accessing the environment and args over rsyscall
okay so args and env are at the base of the stack. hm.
so we could access them that way.
that's a bit silly though, let's just get it externally
* handling entering the child process
  I guess it seems fine.
  Maybe I'll wrap the SyscallInterface in something more structured and nice.
* what does the FD hold
  Currently it holds a SyscallInterface.

  And since we mutate the SyscallInterface when we change process...

  There's nothing to update/convert in the child process.

  but wait so, this also means that if I open things in the child process,
  then when the SI reverts back,
  I won't be able to see that they are bad.

  I think I should not be mutating the SyscallInterface, but rather creating a new wrapper thing.

  Yeah, and then I should mutate the ProcessContext to indicate that its SyscallInterface has stopped working.

  Technically the processcontext should support multiple syscallinterfaces.
  argh. so it's desirable that, uh...

  OK, so maybe I should have a single TaskContext,
  which references the Process it runs within,
  and has a single fixed syscallinterface.

  why can't FDs just have both a syscaller and a process?

  well what happens when the syscaller switches what it's referencing.
  or rather what process it operates within.

  I guess one concept is that, um.
  our task moves to another process.
  so er.

  but argh, how do we then prevent other things using that syscaller while in the child process?
  when it will be wrong?

  okay so maybe I have this notion Task,
  which holds the syscall immutably,
  and mutates what context it's in.

  and then each FD holds a reference to both a Task and a FileDescriptorTable.
  and before doing anything,
  checks that the Task and FDTable match.

  That makes sense.
  Task is, then, essentially the same thing as SI,
  just a slightly friendlier presentation.
  well, a nice wrapper anyway.
* epoll/asynchronicity
  Now I need to decide how to handle this.

  Maybe have another wrapper for fds?

  I guess the epoll guy has to own the fd since it has to remove it

  Don't forget to read/write first and only on EAGAIN start blocking.

  Should the epoller be tied to the task?

  I don't see why.

  Well, of course an epoller holds a task, of course,
  because it needs to be able to syscall.

  FDs can be switched between tasks easily though. Or should be able to anyway.

  Maybe an epoller should inherit from fd?
  Can you actually read from an epollfd?

  Oh, I certainly should have an epoll file descriptor in any case.

  And then I have a thing which owns that...

  Maybe release() should return a copy of the FD object instead of an int?
  It's just a way to mutate the object into giving up the fd?

  OK so the question of whether I have an epoll fd that is just known on the other side,
  or whether I have a wait_readable call.
  
  Hm.
  I guess I can just epoll_wait anyway.
  As long as no-one else is using the thread/epollfd...
  Then can't I just make blocking calls?
  If there's no other tasks waiting on things other than in my world?
  Which is probably the case?

  Of course, the epoll_wait needs to be interruptable.
  So that other tasks can cause a call.
  For example, some task has an FD become readable, I read it,
  and then I want to write that data to another task.

  Tricky, tricky.

  Except it's not really the case that I'm the only thing in this world.
  Both native and through rsyscall.
  Others are waiting and might cause me to want to do something in a task.

  Tasks, tasks, tasks

  Single language runtime, multiple tasks,
  freely scheduling on any task,
  tasks tasks tasks

  Tricky, tricky.

  tasks.

  Essentially we're providing a way to work with multiple Linux tasks,
  as objects inside a single-threaded language runtime.

  But when we call a function on one of them, it locks the object.

  We want to be able to interrupt those functions.

  Ideally this would be implemented by,
  when we perform the call to an epoll_wait,
  there's an additional thing that can wake us up:

  More people calling into the object.

  Ho hum he...

  I guess this is an issue of multiplexing?

  We could write a nice API where we run each system call on a different task.
  The ole, allocate thread pool and run syscalls on them, approach.

  But instead we want to multiplex a bunch of system calls on a single task.
  And in some sense, we want to get notified when they return successfully,
  but we still want to be able to send new system calls...

  wait_readable does seem fairly good.

  OKay so I'll stick to wait_readable being abstracted by the interface,
  I'll code using trio-only libraries,
  and, yeah!
* file descriptor .release()
  release() allows some kind of linear movement of types through the system,
  so we ensure they get closed.

  man a linear typed language would be such an improvement for resource tracking.
  blargh
* important design notes
  dup2 takes a file descriptor object and never ever takes a number
  subproc has a method to convert objects from the parent process to the child process
  cloexec is on by default, and unset manually when creating a new process

  wait_readable only returns when the underlying FD really is readable - it doesn't take the lock on the object
* concerns
  okay so...
  what the heck

  if I pass an fd into a subprocess, does it then get the ability to change my own nonblock status of that file description?

  fml

  how to test, hm...

  guess I can just test fcntl

  okay so the internet suggests it really does work that way
  even for stdin/out/err

  fml fml fml

  is there a nice clean Linux-specific solution?

  so I just realized that since struct file is shared between file descriptors,
  O_NONBLOCK can't be really set on file descriptors that might point to shared (non-owned) struct files...

  argh okay so let's think about how we could model this if we have to.

  also terrifying is the prospect that it's not just shared across dup'd fds,
  but also across separate opens of a fifo, as that one guy said. but that seems dubious

  so we need some way to represent whether it has
  extra references that are outside my set of fds

  and also when my set of fds passes outside my control??
  no I guess I don't need that, that will just be a stray files/task thing

  okay so
* file object management
  we'll have FDs point to a FileObject

  And we'll have FOs have a list of FDs?

  And we'll use this to implement something like Rust's cell system?
  A runtime borrow checker?

  If we have an exclusive FD to something, we can perform FO mutations.
  Otherwise we can't.

  And we'll also have a flag on FO, "leaked".

  Or wait...
  I guess when we pass something to a subprocess, a file descriptor will be left behind that is open.

  Oh god
  When we create a subprocess we create a bunch of new references to a FO.
  So we add them when we translate, right?

  But, ones that are marked cloexec are closed once we leave, right?
  So, we shouldn't make references to them.

  I guess we would want to manually... implement... cloexec...
  Track whether each FD is cloexec...

  We'd want to have the FD table maintained for each FilesNamespace
  Christ almighty
  Why can't we just use the underlying data for this, again?

  I wonder if CRIU people do any of this

  So okay let's talk again about what we'd want to do.
  We want to detect when file descriptors are closed by cloexec.
  It's a nice automatic feature but our tracking needs to know about it making it useless.

  Maybe I should just not use cloexec?

  Why do I need to know when fds are closed again?

  Well, to know when the FileObjects are not shared.
  Oh, god...

  In a linear type system,
  I'd hand out a reference to a FileObject,
  and the subprocess would not return.
  Er hm. Even if it returns it's still leakedL forever I guess.

  Ok so I don't need much big stuff, I just need a FileObject with a shared flag, and unsetting cloexec can set the shared flag.
  And it'll be initialized as either shared or not.

  But the issue then is how I actually handle changing flags on the FileObject

  For example, what if, like... I want to leak do some aio on it, then leak it down?

  I guess I move it into the epoller, which sets nonblock, move it out again, which unsets nonblock, and leak it down.

  But what if I want to leak it while using it?
  well obviously that's bad.

  But what if I want to set a file object flag through one FD,
  while another FD is using the O_NONBLOCKness of the FileObject?

  ok, so there's also open file description locks, which also operate on the file description level.
  oh and there's also file offsets but who cares about those lol.

  it's kind of beginning to feel like maybe there should be some kind of syscall to duplicate an open file description.

  Ok so that would be stupid and couldn't work in general,
  because the struct file for sockets, for example, can't be duplicated.

  probably can't, anyway.
  nor for pipes.
  probably.

  okay, so is there *any* reason that I would *ever* want to share FileObjects between FDs?
** benefits
   Hey I don't think I need to pass in ownership of the FD to the Epoller anymore

   Because it's IMPOSSIBLE FUG

   oh wait i still need to do that actually

   uh, hm.
   let's think about it actually.

   so we still clearly need to take ownership, right? so that we can close it...

   but we can't "take ownership" of the FileObject, eh

   well

   okay, so technically we could not pass in ownership of the FD to the epoller

   just register the FileObject, *HEAVILY CONDITIONAL* on the FileObject being exclusively owned by us forever.
   otherwise it'll be leaked hardcore in that epollfd.

   then when we close the last fd, we'll be good.

   I guess in this kind of scenario we'd only really need one epollfd per...
   FileObjectNamespace or whatever...
   aka kernel...
   
   and we can unregister the FileObject using any other FD to the FO, I guess.

   jeese.

   okay...

   so file descriptors actually suck??? :(
   not really, they're still an excellent dependency injection thing
   but they aren't really that cap-secure since they are actually keys to an underlying mutable object.

   so maybe I should take file objects as primary?

   And have a way to manage, file object vs shared file object differences?

   I can even have, like...
   shared file object reference thing
   and real file object

   and have file descriptor numbers exist only under the hood

   aaaaaaaaaaaaaaaaa

   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa

   okay okay

   I guess this may be the right way to go

   let's, seriously, defer this, though
** file object design
   So really file descriptors are references to file objects

   I'm tempted towards a design which looks like:

   UniqueFD<FileObject>
   SharedFD<FileObject>

   With a one-way conversion allowed from unique to shared.

   Then on top of that you can have references to the uniqueFD.

   You convert to SharedFD whenever you want to pass an FD to someone else.
   It doesn't necessarily set CLOEXEC, because you might just be passing it over a socket, say.

   UniqueFD is for resources that we have exclusive ownership of.

   We can't implement FD proxying (it would be too high-overhead anyway),
   so the only option is SharedFD to share something.
   But I think we'd need SharedFD anyway.
   We'd want to represent whether the proxy is exclusive or shared before sending it out, of course.

   O... kay.

   And also, FileObjects are also just references,
   in some cases to things which can be opened a second time and mutably messed with more,
   but that's just application logic, whatever

   And, I guess the FileObject would be the one carrying all the capability flags.
   Actually it's interesting that this kind of makes capsicum support hard,
   if there's ever fd-level attenuation of caps.

   but I think this FileObject-primary approach is good

   And I guess we can make references to FDs and pass them around and such.

   And an FD also has the syscall interface, so it's the whole entirety of the reference.

   OK and also we'll be able to wrap the EpollOwnedFDs directly around the underlying FileObject.

   Should Exclusive and Shared inherit from the same FD class?

   yeah

   both for the interface, and for implementation sharing.
** sharing
   So technically stdin/stdout are exclusive for the lifetime of my process, aren't they?

   So I should be able to set them to a nonblocking mode.

   stderr is still shared though, innit

   I can have a pipeline and, each of the things,
   they all,
   write to it.

   Oh but stdin/stdout aren't exclusive, consider backgrounding.
   That takes away ownership.
   Maybe the shell should save and restore the flags on stdin/stdout when doing that.

   stderr tho

   Urgh, given this blocking problem,
   and given that an async read from the filesystem would require a separate thread,
   wait no it wouldn't, that's only if I want to parallelize it, which I don't really care about.

   Well, I was wondering if maybe I should support easy creation of additional threads/SyscallInterfaces.

   that would make rsyscall a hard dep, if we were using it in the core.

   hmm I guess there's no disadvantage in RWF_NOWAIT being both for pipes and things and files

   since we can't otherwise wait for files

   okay! whatever! I have to do the file object thing, practically!
   there are probably other use cases and stuff!
** wait okay
   Why am I seperating FD and FileObject again?
   I don't need to know when two FDs point to the same FileObject.
   Why don't I just have a single type that represents a file object, which contains the fd used to access it,
   and has a boolean flag about whether it's exclusive or not?

   Oh the issue is that I want the type to tell me when it's exclusive

   Well, I could have the FileObject take a type parameter which tells me whether it's exclusive or not.

   How would I do this in a proper typed system?


   Hmm.

   The file object really is just a marker to make sure you don't mess up.

   Free functions... seem fine for this?

   I just like how ergonomic methods are, since they're self-namespacing.

   hmmmmmm

   having them be methods seems fine really
** open file description
   OK, clearly passing down shared open file descriptions is bad.

   And terminals, terminals are bad too.
   So, yeah, it's fine.
** epoll wrapper approach
   I'll be led to a good design by trying to implement a multiplexer for epoll


   So let's assume that they only want to block once they have polled and failed for every reader.

   right?

   hmm there is a contention that the only right way to do it is,
   to read until eagain

   although I guess I can translate the edge into some status on my side.
   the edge is raised, we save it, we can then read until we get eagain, and then we lower the edge
   and if we want to read again after that then we have to block

   we really need to get the eagain information i guess

   also how does this relate to getting edge/level notification over the network?

   so anyway we return when the level goes high,
   and then we should call again when the level goes low.

   and i guess all we can get over the network is,
   "hey the level is high"
   "do a bunch of stuff"
   and when the level goes low maybe we need to say "ok the level's low, let's block on receiving a high-notification"

   I guess that's a much more stream-oriented approach.
   We could actually emulate that on top of pipes.
   Which makes it much better!

   I see, I see, so edge triggered is much better.
   It's like reading off notifications from a pipe.

   And the notification is, "hey this is readable now".
   And we get the "hey this isn't readable now" notification from EAGAIN.

   And, we could get multiple "hey this is readable now" events,
   as a result of efficient implementation that doesn't require storing data.
   Really we could get such events every time something becomes readable.
   And they'd get coalesced automatically I guess, for efficiency.

   Okay, okay, so this makes a lot of sense.

   Then the multiplexer is simple:
   It's just a proxy for pipes, to direct things to the correct destination.
   They register for events,
   and they get them sent to them.
   So that seems really quite simple.

   And there's no need for EPOLLONESHOT either

   So the real low level interface is this stream of readable events.
   And, we can wait for one to appear,
   but then it's not safe to keep waiting for another one to appear on that same channel,
   we need to go do IO until it's exhausted.

   It's kind of a weird dual-channel thing.
   The readability stream is like a control channel,
   and the data fd is like the data channel.
   Except the data channel also has notification of its own exhaustion, hm

   wait_readable is definitely not the right interface.

   What's a safe interface?

   Maybe some kind of, "local level"?
   We can wait for it to be high,
   but how do we make sure we mark it low?

   I guess it's not safe.

   The raw interface is certainly the edges.
   But how do we make that safe?
** general question
   I have a boolean variable.
   One source will tell me when it goes from low to high.
   Another source will tell me when it goes from high to low, among other things.

   The only actual uses for it involve messing with the latter source.
   There are two sources of things that change it?

   Obvious solution: Wrap all access to the sources

   But, the sources have a bunch of functionality that I don't want to wrap.

   Another obvious solution: Have the source know about the boolean variable and update it.

   Well... that's essentially the same as wrapping.

   I think probably we do want to just wrap.

   Oh, here's the real issue:
   We don't know how many of the sources there are.
   Many different things can possibly cause the variable to change.
   How do we ensure that the user appropriately detects the variable-changing event,
   and appropriately changes the variable?

   Well, the event throws an exception.
   Or, perhaps, can be linearly typed.

   I mean, when we get an EAGAIN...
   That's not really an error. It's a return value saying,
   "hey yo, I don't have anything more to give you right now, come back later, ya dig?"

   But still.

   If there's many sources and they all change the variable, well
   The obvious solution is still to wrap all the sources.
   Though, we don't necessarily have to wrap them to know about the variable.

   We can have the sources return a value that must be consumed by the variable.
   Though that requires a bunch of types.

   I think the best model for Python is to just have the EpollWrapper wrap every method.
   And update the readability status thing when it gets EAGAIN

   It works! Yay
** type-directed model
   We could have some operation on an fd,
   consume the fd,
   and either return the fd and some data,
   or an EAGAIN wrapping the fd, of some specific type.

   No we'd have to work with fd-operation pairs.
   Well, in any case, that's the basic model.
   Let's call the fd-operation pair just an "operation".

   So we have an operation.
   Operation -> Either (Operation,Data) (EAGAIN::Read Operation)

   We consume it and either get data and the operation back, or no data back and the operation wrapped in an EAGAIN.

   To unwrap the EAGAIN, we need to get a read posedge on the source.

   But, the read posedge has to come after the operation has been done.
   Tricky.

** no unique shared fd types
   It's too much overhead.
** TODO create epoll wrapper
   This has two advantages:
   can pass arbitrary data into epoll
   can pass weird epoll flags

   no wait the only weird epoll flags go in the event mask and I can already pass all those from python
** things which operate on file objects
*** Open file description locks
    urgh how do these work
*** File offsets
    but I can explicitly specify the offset.
*** O_NONBLOCK!!!! and other file description flags
    ugh EPOLLET really very much requires O_NONBLOCk to be set, otherwise you can't tell when to stop reading
    (though for streams (as the manpage says) you can detect partial reads)
** next up: supervise
   I could maybe do clever things to support multiple children in one supervise.
   But let's do the simple one child case first to see how it works first
** only on one thread thing
   I find myself frequently wanting to do a thing in only one thread calling into an object.

   Once the thing is done, all the callers can return.
   But, just, only one of them needs to actually do it.

   I find myself frequently wanting to do:
   "here is an async method. multiple tasks can call this async method at once. "

I find myself frequently wanting to perform some function in only one task calling a method,
and have the other tasks just wait until that function is done.

   It's kind of like picking a task to schedule some processing on,
   and then if other tasks call the method too, they just have to wait until the processing is done.
** resource issues
   many resource leakage issues are resolved by the fact that resources are local to a task
   and the task can be destructed all at once.
** fd leak on script interpreters with execveat(fd)
so thereotically we should just never cloexec it
always let it pass through
in which case,
the interpreter will get an argument of the form /dev/fd/N
well, the interpreter should really know to just use that as an fd!
and I could just send patches to bash and python to fix them...
if they get an argument like /dev/fd/N, they should just use that fd instead of reopening
especially because the permissions might be tricky on, say, memfds or whatever
though, both of them can support running scripts from stdin.
i guess the real issue is that the filename is provided as a path not stdin?
well, if it was provided on stdin then stdin would be used up, argh.
yeah so providing as an fd argument is best
I'll code around that for now,
and use it to test...
well no
I'll write a minimal C program that execs into something via open and execveat
also this works for sending the script once??? over a pipe??? not over stdin??

oh this doesn't work because normal executables will have to close the fd too
but then how does the dynamic linker work??

argh, the program is already loaded into memory by the time the dynamic linker runs
hmm so how can we handle this for ELF binaries?
for elf the fd is loaded into memory and isn't needed.

argh
should we just dispatch based on whether it's a #!?

blargh I guess we can't execveat for now
** process stuff
   pids are mostly terrible but only mostly

   as long as you only kill your children, you can work with them in a race-free manner

   I need to implement process management stuff (wait, sigchld, kill) in Python, not just rely on supervise.
   This way is less racy (can be certain I'm killing the thing I expect, while supervise can pid loop and I can kill the wrong child)
   And it reduces my dependence on weird stuff (supervise)
   and that weird stuff, I'm having trouble figuring out how to represent anyway.

   should I set subreaper? maaaybe? meh I'll figure that out later

   So, OK.

   I'll have some multiplexing on top of a sigchildfd,
   which then does a wait,
   and dispatches events,
   and sees what it sees.

   And kill on top of that.

   Essentially it's the same multiplexing as for epoll.

   so we'll have a Process object?
   which represents an existing standalone process?

   maybe it should be a task object instead?
   so we can accurately represent pseudo-threads and pseudo-processes and all that stuff?

   ChildTask, perhaps?
   Representing a standalone task that is a child of ours?

   Only direct children can be safely killed, since grandchildren can be collected.

   So ChildTask is only for direct children.

   What if we double fork?

   Well...

   Well, what if we double fork and then take ownership of the intermediate task?
   We want to support that.

   I guess we have some kind of ChildGroup thing
   Or like, some kind of TaskId thing which points ChildTask points to as its parent?

   And a task's TaskId can't really change..
   but our Task structure thing can.
   Well, actually we should really model it as a thread, not a task.
   Since the TaskId can change and thereby change what Linux task we're operating within.

   Thread makes more sense really...
   It's sfork that allows us to change our taskid.

   Fork too, kinda. But let us not speak of fork, that forbidden syscall.

   So sfork lets us change our taskid.
   But really it pushes our taskid on to a stack.
   We should make sure to explicitly model the fact that it changes our taskid.

   It would be unrealistic to pretend that it doesn't change our taskid.
   Since it's visibly different - children have a different parent.

   What happens to children when you change process namespace?

   We'll have a Thread, which contains a Task (which changes) and a SyscallInterface (which does not)

   We can consume a Task to produce a ChildTask, namely a thing thing.
   That's exec.

   A Task, I guess, has another blocked Task inside it.
   That's the Task that will be resumed when it sforks.

   I think it's best to have it look like that.
   That's realistic, though not necessarily exactly what the interface will look like when it gets into the kernel.

   So eh...

   A ChildTask contains a parent: Task

   And I guess also a Thread?
   And we can only kill it when thread.task == parent?

   soooooo
   okaaaay

   it's not really a collection of registers and an address space.

   the address space, you know, is actually an attribute of the task.
   and the registers don't even matter

   it's actually really truly just a wrapper around a syscallinterface
   and the task it corresponds to, can change.

   how would we even represent this in the kernel, sigh

   I guess we can't?
   what does it really mean to have continuity here?

   it's an illusion
   we copy our thread and we think we're the same

   but we're actually a copy

   are there any other means of having a syscallinterface other than "syscall" and "fd"?

   well we could do it over shared memory.

   and, doing it over shared memory...

   well, we'd have a task which we sfork inside, to create a new task,
   which is still listening to the same syscallinterface.

   so... we just want to track what task is currently active on the other side.

   could multiple tasks be on the other side?

   like, we send a syscall and,
   we can send another and,
   how do we determine which task gets which syscall?

   do we want the syscall interface to somehow give us some kind of token?

   which we can use to make a syscall on a specific task? ha ha.
   that's exactly the same as the current syscall interface.

   and anyway it doesn't work for a specific task,
   but rather a specific thread whatever thingy.

   cuz when we sfork inside a task
   it pushes the task on a stack and makes a new one.

   so this is kind of a "mobile thread".
   it's a thread that can move between tasks.
   and move between processes.

   who knows, maybe it could even move between hosts?
   well it can certainly move between containers

   hmm it sure would be nicer if sfork didn't create a new task

   exec after sfork I guess, as always, does three things:
   creates a new task with a new tid in the current namespaces,
   makes that tid have a new address space,
   returns us to the parent,
   blurgh

   wait a second, exec probably also creates a new file descriptor space, to cloexec in?
   blah

   well, okay, so.
   all this would be complicated.

   what if we just accept that sfork does create new tasks,
   it does create a stack of tasks,
   and exec is just the normal thing.

   then all that I'm doing is,
   having a single thread of execution move between tasks.
   the new task starts up with new registers,
   and the old task gets the new task's registers when it execs.

   so I guess this is pretty unnatural
   generally a task and a thread are identical

   so how would I do it otherwise?

   I guess

   I would just start up a task with a new SyscallInterface

   And in that way avoid the problems of "a thread that moves between tasks".

   Actually, for that matter,
   I would even be able to double-fork and start supervise?
   No no I wouldn't

   Wait, yes I would!
   Through the subreaper thing!
   Since it would inherit great-grand children.

   Huh.

   I could even do a CLONE_PARENT to have supervise directly become the parent of the great-grand-children.
   (haha, that would be so crazy)

   So okay, blah blah blah,
   I guess the best thing to do is to use clone to create new SyscallInterfaces.

   So, then, how do I...

   wait, argh.

   To use clone to do this, I would need to start using rsyscall immediately.
   Even for local, shared memory stuff.

   Although I guess I can directly write to the shared address space.

   Maybe I will indeed just do that, have a shared memory rsyscall.

   So okay, I will also need to set up the child stack right.
   
   I guess that I could probably have a C routine to do that, for now, with the one-process model.

   Oh, I guess I can have a C routine which returns a buffer or something?

   I can get the bytes that need to go on the stack,
   and then pick how I put them there.

   Yeah, yeah, sfork is weird, making tasks on the fly is better.

   Let's be sure we can do this right with remote rsyscall though.

   I would need to prepare the stack.
   But I guess I can prepare the stack just fine.
   It's just a little (a lot) weird.

   Though I wonder if I can leave child_stack NULL?
   Probably not, I do have to change the fds that I use.

   And no matter what, it would loop and then they would both read from the same fd.
   hmmmmmmmmmmmm

   OK I think building the stack is fine

   Doing this with rsyscall is just clearly better.

   So that's my next project.
   In-process rsyscall so that threads work.

   Yeah, just making more tasks is clearly better.

   Oh, also this means I no longer segfault if I don't use sfork :)

   Oh, let's just have a C function that starts an rsyscall thread in the current space.

   Er wait no that's not right, we need to control the clone args.

   hmm I guess we could use CHILD_CLEARTID and futexes as a way to get notified on child process exit :)

   oh wait no, exec will wipe that out

   okay I see, clone_child_cleartid and settid have to be there because
   uhhh, probably posix reasons
   oh, cleartid is how notification of child process exit happens.
   and I guess wait can't be used because, er, tricky posix reasons?
   whatevs

   oh also let's have the task have its own fd space, so we can get easy termination notification still
   er, probably.
   maybe.

   haha this use of CLONE_PARENT will be great
   well, maybe

   okay so it does seem pretty useful to have in-process rsyscall things I guess
   which I communicate with using file descriptors so they can be polled on

   oh, so!
   ptid, ctid, and newtls can all be ignored for our use case.

   we'll get notification of exit through fd hangup, I guess.
   possibly through __WCLONE if we must

   I don't think we need to or want to specify CLONE_THREAD
   although maybe we do

   all we need to specify is the flags, and the child_stack, and possibly the child signal

   we'll build the child_stack ourselves, hum hum...

   will we mmap it?
   instead of mallocing it?
   probably? that's the most common with a totally remote thing.

   we'll still do direct memory writing to it I guess

   oh, to kill a task, we'll, er
   close the infd,
   wait on the outfd,
   unmap the memory.

   okay so the role for C is just to build the stack

   what will the stack even look like?
   I guess we have some assembly routine which loads registers from the stack,
   then calls...

   oh wait, oh no

   clone continues from the point of the call??

   how do we fix this then...

   so I guess in a low level implementation of syscall,
   we'll call the system call,
   then ret.

   the ret will allow us to jump to an arbitrary location by manipulating the stack.
   okay, seems reasonable.

   so we'll write that component of rsyscall in assembly

   and skip the whole errno drag too

   hm.
   newtls, that's tricky.

   I'll avoid it. I'll make my syscalls directly instead, with no deps on glibc, so I'm totally freestanding.

   So, let's do all the syscall wrapping in Python.
   We'll have a class that provides a syscall interface and takes a function to do raw syscalls.
   Which uses cffi to do memory access.
   And we can wrap that class in either a syscall to thread or not.

   That's so type-unsafe it's not even funny lol.

   I guess I can write standalone Python functions that take do_syscall and are type safe.

   It's like assembly from Python.

   OK so we'll start off with just replacing io.py with cffi to async do_syscall.

   Then we can replace the async do_syscall with a thread-remote one.

   Replacing clone will have to come last.
   Since we'll need to be able to do all rsyscalls remotely first,
   and refactor the interface.
   Also can't do execveat too, because sfork clone needs it to work.

   But can do everything else
   Then add the new interface and test it
   (with a new syscall kinda thing maybe)

   Then replace the old interface

   OK so I converted things.
   Now let's add a clone2 which will do the stuff!

   hm
   should I just pass bytes for the stack

   probably not?

   okay so I need mmap now

   hmm apparently I can't do such fancy relocations in assembly.
   so I can't directly call rsyscall_server

   so probably I should instead call some kinda gadget that will call something of my choice.
   so, syscalls.
   stash all registers,
   pop all registers.
   hum hum.
   if we did that then I guess we'd be good.
   but like, most of the time our registers don't change
   so it's pointless

   we would only be doing that to support the fact that our stack can change
   
   okay, so we can have a generic gadget for turning stack args into register args,
   so we can call an arbitrary C function.

   this will be useful: it will allow us to call (in a separate thread) any C function in our address space.

   we can possibly use that to call dynamic linker functions to load libraries.

   it would be nice if status was a full int, but oh well.

   can we figure out a way to get a return value at low cost?

   maybe after we call, we move eax to someplace?
   maybe the base of the stack? :)

   that would be cute.

   so I guess clone is our "call arbitrary function remotely" entry point.
   just has to be combined with a gadget

   of course, we are relying on the "immediate ret after syscall" behavior.
   can we do better?
** child task monitoring thing
   I'll have some multiplexing on top of a sigchildfd,
   which then does a wait,
   and dispatches events,
   and sees what it sees.

   And kill on top of that.

   Essentially it's the same multiplexing as for epoll.

   This will be a Task object
   Which also implements ChildTask?

   Or maybe I just get back a ChildTask object and a Task object when launching a thread.

   Automatic cleanup is the tricky part.
   We don't get fd-based cleanup since we share an fd space.
   Could we run supervise in the middle to clean up our threads?

   That does seem possibly viable.

   Oh wait no!
   supervise won't help either because it won't get notified by the fd closing.
   Hm!

   I could resort to PDEATHSIG to notify supervise.
   That would work perhaps.

   The nice thing about thread groups is that they offer a guarantee.

   They are not nestable though

   There are too many task cleanup things in Linux, and all of them suck!

   I could also PDEATHSIG to kill the child threads.

   classic problem, classic problem

   I guess supervise + PDEATHSIG is fine.

   But I still don't like the fact that I don't have race-free-ness!

   I'm not killing my direct children...

   To have a cleanup task, my children must be theirs...

   Hmm, could I do an arrangement like...

   me -> supervise (with subreaper) -> childspawnerwaiter -> [all children]

   Then when I die, supervise is notified, and kills childspawner and all other children.

   Hmmmmmmm curious, curious.

   Alternatively, I guess I could wrap around me instead.
   Then I could spawn and wait on children directly.

   supervise -> me -> [all children]

   Like... treat me as something which can be sigkill'd?

   Essentially externalizing the cleanup?

   The fact that my children share my fd space makes this all tricky.

   I could just PDEATHSIG them too I suppose.
   
   Well... hmm. Then if someone sigkills supervise, things break.

   Oh wait, does SIGHAND just negate this whole problem?
   Does it mean that a signal to one, causes all to die?

   We should check that.

   OK yeah

   If I set PDEATHSIG for everything
   And have threads/tasks I control be direct children,
   and have uncontrolled things run under supervise to clean them up,
   everything will be good.

   This does suggest I can't exec in a direct child to something uncontrolled.
   Unless I can figure out a way to..

   Oh!
   I'll just fork off a child of my direct child
   and exec supervise in my direct child.

   So this seems... really good and clean!

   Direct children (such as supervise and rsyscall) can be trusted to terminate when I SIGTERM

   Indirect children (under supervise) get a harser approach, being SIGKILL'd by supervise.

   so it will look like

   me -> rsyscall, rsyscall, rsyscall, [supervise -> [nginx -> worker, worker, worker]]

   And PDEATHSIG will ensure cleanup.

   We don't really have to set PDEATHSIG on supervise since...
   well actually we do have to, because we could have:
   me -> rsyscall -> supervise -> rsyscall

   And fd sharing between 1,2 and 4.
   (presumably only temporarily, while execing in 4)

   Then we'd need supervise to be killed when 2 dies,
   because the fds wouldn't be closed until 4 dies.

   What would the kernel support look like then?

   \_()_/

   It's not clear yet whether supervise will be doing child monitoring?

   I guess we can do:
**** me
***** rsyscall
***** rsyscall
***** supervise
****** rsyscall
       Here we will monitor child processes and stuff.

       It seems like it would be better for the thread to exit by raising a signal.
       That way it can be handled.

       And we could have a signal handler that does filicide and terminates.

       That would allow removing supervise from the picture
******* nginx
******** worker
******** worker
******** worker
*** continuation
    but wait, how does this interact with the remote use case?

    I guess it's pretty hard to load a library remotely

    Although actually I guess it could be pretty simple.

    So let's assume we can do a remote library load.

    Then we'd have our daemonized rsyscall process,
    which is controlled by its fds.

    When we shut down, we terminate the connection and its fds close.

    It reacts by raising a signal.

    That calls filicide if the signal handler is registered, but either way it terminates rsyscall.

    That results in PDEATHSIG terminating all immediate children.

    That all seems sensible.

    Oh wait, I can even load the library remotely through using clone-to-run-function hacks!
    What a cool hack that is.
    I just need to run it through ld.so or something, so that the dynamic linker appears in my address space.

    For now let's just put supervise in the middle.

    In this model, supervise does basically nothing at all:
    It just provides a process that calls filicide on exit.

    It's like a hack around the fact that I can't load libraries to get a function to register as a signal handler.

    So yeah, this all seems good.

    supervise then is really a simple utility.
    It does basically nothing.
    And is useful generically, and from the shell.
    well no not really, since it doesn't exec for you :)

    we're just using rsyscall or other magics to handle the child status reporting below it, if we care about that.

    seems great!
*** ways to clean up tasks
    thread groups

    process groups

    controlling ttys

    pid namespaces

    control groups

    PDEATHSIG

    I guess the ultimate way is to have a separate cleanup task.
    Which I don't SIGKILL.

    And I guess I'll use PDEATHSIG to notify it, shrug!

    We can also clear pdeathsig if we want to detach.

    So PDEATHSIG is a decent thing. I guess we'll keep it.

    filicide, then, is what we need. It needs to be in the kernel so it's robust to SIGKILL

    It's a thing which sigkills all transitive child processes when your process exits.

    I guess essentially it just SIGKILLs all children until there are no more children

    subreaper is required I guess.

    essentially we want a parent-level attribute that specifies the signal to send to all our children when we die.
    that way our children can't turn it off.

    and we want to keep sending it as we get more children reparented to us, I guess.

*** how did LinuxThreads do it??
    how did they manage to have SIGKILL to one thread, kill all threads?

    probably by pdeathsig or somefin
** implementing the child status monitoring thingy
   so I'll allocate a sigchld signalfd

   then attach it to an epoller

   then wrap it in a multiplexer

   to dispatch waits to the right associated ChildTask

   note well that subreaper means I'll get wakeups for unknown children.

   rsyscall[reap]

   so handling sigchld and manipulating child processes isn't so bad after all
   IF you're the only one in the process doing it

   actually no, just: if you don't have to stick to posix semantics

   hmm.
   aren't privilege level changes in Linux threading libraries inherently racy since one thread has to change before the others?

   Oh okay I guess first I'll do a signal multiplexer thing.
   I'll explicitly opt in to signals that I want to receive instead of terminating me?

   Then we take the SIGCHLD queue and

   We wrap it in Waiter

   Which has a "make" method which returns ChildTasks with a given tid.

   Or actually I guess a clone method?

   No, Waiter returns just ChildTasks.

   We have another thing that takes no arguments and returns a (Task, ChildTask) tuple

   Since it needs the pointer to rsyscall stuff to launch.

   We also want to have some kind of SupervisedTask which is required to fork off
   things which can
   exec random binaries

   And what about our trampoline which lets us run arbitrary C functions?
   Shouldn't we expose that?

   And that just has a ChildTask dep...

   Or rather it's a wrapper around a ChildTask.

   We don't want to share SIGHAND; not sharing SIGHAND means we can have specific tasks do filicide,
   without everything doing filicide.

   VM and FILES are both namespaces for dynamic resources,
   which we can safely share because the kernel multiplexes/allocates in a non-interfering way in those namespaces.
   (as long as we don't MAP_FIXED or dup2 to an unallocated address/number)
** rsyscall connection

   OK!

   So now I need to turn an RsyscallConnection into a syscall interface.

** runningtask

    Close this to murder the task.

    But wait what if the task execs?
    Then it is also fine to free these resources.

    But how will we know?
    HOW will we KNOW?

    So we can never exec in our base task, right, because then there will be no-one to
    filicide for us. Though, if we make a supervised task, maybe?

    So we have to figure out how to consume a task's resources after an exec.

    Also how does this relate to remote/out of process rsyscalls, ya dig?
    With those, one task may depend on another to be accessed! So we can never really exec them.
    So we do kind of have a differentiation between leaf task and lead task.
    Or rather, tasks which are depended on by other tasks,
    and tasks which aren't depended on by others.

    Right, because, only leaf tasks can exec - if a non-leaf task execs,
    we can no longer track its children!
** passing an epoll'd socket down to a subprocess
   Oh no!
   We need to be able to turn O_NONBLOCK off,
   but even if we're in a separate CLONE_FILES,
   that will still affect the same object.

   So we need to, in some sense...

   Oh, wait!
   Since we're passing it down to a subprocess,
   we can't ourselves continue to O_NONBLOCK on it.
   It needs to be consumed by the subprocess.

   So we'll just have some kind of release_from_epoll(),
   which returns the underlying,
   and unsets O_NONBLOCK,
   so it can be used by the subprocess.

   All fine.

   Connect and stuff can happen asynchronously without a problem.
** IntFlag!!!
   Woah!!! Use this thing for all the flags!!! So excite!!
** name
   it will be a family of related libraries that are similar in structure

   so, rsyscall is I guess an okay name
** binding sockets
   so I can do:
   chdir; bind(./name); chdir

   or I can do

   open(O_DIRECTORY); bind(/dev/fd/n/name)

   the former gives me a full 16 or so extra characters.

   but obviously I have to do the latter
** fifos
   interesting, fifos should always be opened for reading with O_NONBLOCK

   that way they won't block forever when opening for read
   which means deadlocks aren't possible

   maybe all opens should be done with O_NONBLOCK

   nah it has no effect for files
** path object, O_PATH fds
   There are a few common operations between Paths and O_PATH fds.

   namely: fchdir, fstat, fstatfs

   and we can probably use it with AT_EMPTY_PATH

   do we want to support having a Path which is backed only by an O_PATH?
   maybe, let's think about it future
** sockets
   So to know the address type for a socket,
   I need to know what kind of socket it is.

   There are a bunch of calls using the address type:
   bind
   connect
   sendto
   recvfrom
   getsockname
   getpeername
   accept

   Also, sockets are only readable once they're connected.
   so...

   Also, after a shutdown, they're no longer readable.
   Hm.

   Let's not try and track the readability status.

   But let's indeed track the domain in the type.
   Since it can't change

   And what about the type?
   er, that is, the sockettype.

   Maybe I should mix that also into the class type?

   Okay, so all three parameters can change the address type, actually.

   Maybe I should focus specifically on the address type for these SocketFiles.

   Maybe it can be, like,
   SocketFile[InternetAddress]
   SocketFile[UnixAddress]

   And so on.

   Hmm, we also need to think about how to handle fd passing

   I guess when we recvmsg,
   we get back a t.Tuple[t.Optional[T_addr], bytes, t.List[ControlMessage], MsgFlags]

   Then we can parse the ControlMessages to see what kinda do-hicky we got.

   But, this can all be represented low-level.
   t.Tuple[t.Optional[bytes], bytes, t.List[t.Tuple[int, int, bytes]], int]

   god the socket API is so crap
   i think, maybe

   okay, so the question is whether we have the address type above or below the FileDescriptor.

   I think below, it's nicer.

   But processing of recvmsg results is above the FileDescriptor.
   What class does that take place in?

   Well, I suppose each ControlMessage will be parsed.

   And will already contain whatever fds we pull off the socket
   But we need to specify a File type.

   I guess we can change the File it refers to, after the fact.
   Just have it refer to a plain base File at first.

   OK! Remember the central goal:
   Expose Linux features as they are!
   Do not become opinionated!
** unix sockets
   interesting, connect on a unix socket immediately returns without blocking,
   as long as it's to a listening socket

   it doesn't block until the server accepts
** child task scheme, final version for sure
   So I can clone and exec at any time without restrictions.

   And when I clone I'm relying on someone up the supervision tree,
   to have enabled filicide.

   And there's an easy helper to take a task and enable filicide on it.
   (either by signal handler, or execing, or starting up a thread waiting in sigwait then calling filicide)

   And when I clone I get a ChildTask back,
   which is usually wrapped in a larger [Something]Task,
   which holds all the resources given to that ChildTask,
   so that they can be freed when the child is killed.

   And for rsyscall tasks,
   that object is RsyscallTask,
   and it has a reference to the Task,
   and it has an exec wrapper method,
   which execs in the Task and frees the resources it used and gives me the ChildTask.

   But I don't have to go through the RsyscallTask to exec,
   I can just exec directly if I so choose,
   including from the root task on any system,
   even though that may break things.
** exec
*** DONE child tasks stay around after exec
   Hmm.
   Also, when I exec, how do I make my child threads go away?
   If I exec up in the tree, my children won't get signal'd, because I am not exiting.

   Actually I think that is fine.

   I have a neat visual of the Starcraft terran HQ, lifting off, setting up a big hierarchy of threads,
   then anchoring again and transforming into a high-efficiency C program/factory/assault fort.

   For it to be efficient, though, we'd need to free lingering memory and close fds referenced by no task.
   or something. but that's for later.
*** TODO detect when exec has completed
   Though, hmm. how do I detect when an exec has completed?
   And won't now return with an error?
   Traditional way would be to use files being closed.

   This is one place where sfork was nice.
   When I exec'd, it returned to the parent on success.

   How would I ideally do this?

   Well I could use the sfork trick I guess
   Hm! This is a puzzle.

   So after exec, the fd table is unshared.
   So maybe we could start off by unsharing the fd table?
   Then we close the rsyscall-side fds,
   and then we exec.
   Then we'll get either a hangup or a result.

   Maybe I can just close the fromfd,
   so that if it tries to respond,
   it gets a sigpipe,
   and exits?

   Yes, let's just convert an error into an exit.

   Wait, wait, wait, again I can't do that, if I want to exec into rsyscall
   Because, if I want to share the fd in the threaded rsyscall with the process rsyscall,
   I have to keep them open.

   Well if I'm doing that then I'm not freeing all the resources owned by the task.

   I'm pretty much only freeing the stack space.
   So then the question is: How can I know when the stack space is unused?

   Hmm, I guess this is an issue that is solved by running in a separate address space.
   Then the stack space is definitely freed up when the task exits.

   But if I want to use clone...
   And thereby have a thread in my address space...
   I have to manually free that.

   So what's the trick?
   How do I know when to free the thing?
   Well, after an exit or an exec.

   OK, and also keep in mind that the task might not be our child.
   It's gotta be task-only.

   Though... in that case we don't have anything to free.

   Oh dang, also we can't do it by exiting on error,
   because then we don't know when to free stuff on success.

   We need some kind of event after the exec but before exit(0)

   If we constructed a process from scratch,
   then kicked it off,
   then it did something to no longer need the resources we gave it,
   shouldn't it free those resources itself?

   So we're in a situation where it's just borrowing resources
   And we need to know that it no longer needs them.

   Well how the heck do we do that

   I guess there's an easy way to be sure: if the address space changes
   For fds we can just see when there's no more object references

   Ehhhh hmmmm
   I think separate fd tables and address spaces are good because they keep things nicely reference counted
   Everything in a space is needed by that space,
   when a space goes away all those things have their refcounts decremented,
   and maybe go away.

   Yes, yes, that is all true!
   But what about when it's not a separate address space!
   Oh. Maybe then we can't call exec?
   But, argh, then how do we even get new tasks in the first place?

   So okay, clearly a separate process (address space, fd table) is very nice and it keeps things nicely refcounted.

   But we need to also support the case where we aren't in a separate fd or vm namepsace.
   In that case we have to know when things leave ourselves.

   So how do we figure out when the reference to the stack is dropped?
   Indeed even other more rich functionality could theoretically switch off its stack.

   Well, those things would have rich signaling functions to say when they drop refs.
   (or just be explicitly controlled by the runtime)

   But if we're talking just system calls, how do we do it?

   Well how would we manage this prop'ly?

   The stack would be linear and passed to the thread
   When it goes to exit or exec,
   we'd free the stack purely in registers.

   Okay, so that is all fine and good.
   What stops us from doing that with exec?

   Well with exec,
   the thread stops at the point of exec,
   it doesn't get the chance to continue.

   What is this in terms of continuations?

   OK so it looks like I can use CLONE_CHILD_CLEARTID

   I want to get notified on mm_release,
   which notifies currently on cleartid and vfork.

   cleartid is gross though because it does a futex wakeup instead of an FD wakeup.

   a CLOEXEC pipe is nicer than CLEARTID maybe

   Oh, but we have to unshare before exec if we do that.

   urgh

   hmm so that seems like it might be okay
   we'll force the thread to exit if its exec fails?

   how does that interact with main tasks and such?

   well, if a main task execs,

   well, I can't figure out a task to monitor it from in that case!
   a parent task who can share a pipe with it, argh

   the same problem applies for any exec-monitoring thing

   the problem of responding to the notification and freeing the resources after the exec

   that requirese some kind of monitoring

   which I guess only the wrapper can do.

   some kind of on-the-fly vfork?

   like, instead of setting vfork,
   I call block_until_child_exec,
   urgh which is essentially the same as vfork
   
   You could emulate vfork with clear_child_tid, I think.
   Do a clone,
   in the parent do a futex wait on an address,
   in a child do clear_child_tid,
   then proceed to exec.

   so we'd want something that we can bake into our event loop

   I guess if there was a waitid event on exec,
   then...

   we could also emulate vfork that way?
   neat

   so OK,
   two methods suggest themselves to me.
   1. use CHILD_CLEARTID
   2. use ptrace

   ptrace would obviously be disgusting but, is it really that bad?
   it's certainly better than futexing trash...
   and I only have like,
   a tiny bit of ptrace.

   But, of course, that is a portability problem because ptrace can be turned off (through the YAMA LSM)
   But, eh, whatever.

   ok so futexes seem utterly crap, I don't see a way to wait on multiple things

   could I like, mmap an fd or something, and specify that

   ughhhghhh

   so i'll just do traceme,
   set PTRACE_O_TRACEEXEC,
   forward any non ptrace signals,
   and untrace once exec is done successfully

   look on the bright side, supporting ptrace is cool!

   oh hey and uhhh
   could I speed this up with that seccomp yield to userspace  things?
   nah, there's no way to exec in the original process with that
   it tells me before the syscall instead of the syscall

   ptrace is not an acceptable solution
   because ptrace isn't recursive

   CHILD_CLEARTID or vfork it is.
   vfork is probably just plain unsuitable.

   so we need to figure out futex integration.

   I guess that essentially means one thread per futex.
   Doing.. what?
   ugh, maybe just bridging a futex to an eventfd?
   that would be cool and generic I guess...

   OK let's do it.
   Not that specifically, but a thread dedicated to waiting on the futex.
** fd namespace   
   It should contain a list of tasks in that namespace.

   And when an FD's task goes away,
   it picks a new one from the FDNamespace list.
** file descriptor table inheritance issues
   fork is really problematic
** shared vs unshared file descriptor tables
   The benefit of unshared is that we can detect things via file descriptor hangup, which is nice.
   Also it's just a lot more nice isolation, I guess.

   What's the benefit of shared?
   Well...
   We can very easily pass file descriptors between tasks?
   But we could do that with unix sockets anyway, just slightly harder.
   We kind of don't have to deal with inheritance being tricky,
   since any fd in a space is valid in new tasks.

   But we kind of want to explicitly move fds between tasks anyway,
   so...

   On the other hand, do we really want to rely on unshared fd tables?
   Because, isn't that somewhat expensive?
   Maybe not that expensive...
   It's mainly shared memory that we want to preserve.

   So we'll have the shared memory threads,
   but each with their own fd table.

   That seems fairly good I guess.

   I doubt we'll have issues around moving between tasks being too expensive.
   And if we do, we can resurrect the trickiness of shared fd tables.

   The issue with shared memory is the same as with shared fds, though.
   When one dies, its resources aren't freed.

   So I guess we could just go all the way to separate processes.
   But spawning them requires spawning threads, anyway, so...

   well, not if we use sfork... blargh

   okay, separate fd tables it is
   that is clean, I think.
   and also, that's kind of natural for "process is a virtual machine" notion.
   actually no it's not, why would each CPU have its own fd table,
   ridiculous

   maybe because of NUMA???? NUMA fds???? NUFA????

   maybe we should exec so we trigger cloexec so only explicitly passed fds go to our internaltasks

   if we start out shared, then move to unshared,
   how do we handle this inheritance question?
   we have to deal with that either way.
   how do we handle inheritance?
   it would be nice to have a CLOUNSHARE maybe
*** the file descriptor inheritance question
    I still need to think some about how to properly handle the semantics of fd table inheritance.
    that's why I currently have all my child tasks using CLONE_FILES so they share fd tables
    because that way there's no inheritance going on

    fd table inheritance means that any libraries not controlled by me will have their fds duplicated and kept open,
    which might break those libraries

    so this is why I can't have things in their own fd table, but in the same VM space as me.
    the only way to trigger clearing of the fd table is through exec.

    if there was a clounshare...

    wait wait

    a thread that I unshare in,
    if it's running rsyscall,
    can't use any fds I don't know about.
    since I won't send those commands

    but if it's running arbitrary code

    oh wait no

    it's the other libraries, the ones left behind
    the threads that live and breathe with fds unused
    if I fork a thread off, and then unshare, and I have the fds, and the original guys close the fd,
    then that's a weird situation, one which cloexec is supposed to prevent.

    I'd really want to have CLOEXEC also affect unshare.
    Then I don't have any of these issues, right?

    well... I can implement this myself with scanning the fd table.
** argh
   okay so I have two options

   option 1: implement CLOUNSHARE so I can deal with unsharing the fd table,
   and then have each task in an address space have a different fd table
   (this doesn't provide a way to exec into a new process and keep using the same rsyscall fds, right?)

   option 2: use futexes and child wait and all that stuff.
   and pdeathsig.
   this is gross but I can have everything in the same fd table so it's fine
** argh argh option 1
   OK, so let's be realistic, right?
   Isn't this required for starting subprocesses?

   We unshare(CLONE_FILES), assign some fds to some other fds (fds which we explicitly know the number of),
   then exec, and it's all good.

   There's no way to do the stdin/stdout assignment without unsharing CLONE_FILES.
   At most we could first exec a separate rsyscall process,
   and then start running from there.

   Although.

   Doing either of those irreversible things, really seems like maybe it something I shouldn't be doing.

   Feels like maybe I should have unshare(CLONE_FILES) from the start.

   Then I start my child, not passing CLONE_FILES,
   and it inherits all my fds.

   (I can have some other mode for running a thread pool for blocking syscalls over shared fds)

   Man, doesn't this suggest that we probably shouldn't even have CLONE_VM either?
   What if I keep alive some memory that a library was usin'?

   No no, that's exactly opposite. We're all in the same address space, but that's fine.
   Each one manages his own stuff.

   And we never need to unshare(CLONE_VM) (and it's impossible anyway, lol)
   Because there's no refcounting hangup stuff that we want to trigger.
   And things are never placed at known offsets.
   And all that stuff.

   Yeah, there's no way to inherit a CLONE_VM.
   You're either in the same address space or a completely new one.
   Er...
   Well, that's not true, you can inherit CLONE_VM as you move to a new thread - fork for example.

   Well, we won't be inheriting CLONE_VM, anyway.
   We'll be doing it all in a single place, right?
   A single address space

   And we manage memory explicitly within there.
   Why do this?

   After all, if some thread in that address space dies,
   its memory resources will not be collected.
   Only its fd resources.

   Why don't we just forcibly always move to a new process, and exec the thing?

   Because it's far cheaper to write to memory in a single process.

   Well, we could provide a shared memory section.

   And isn't this also an argument in favor of always being in the same fd table?
   After all,
   it's far cheaper to pass an fd to another task when you're in the same fd table;
   you don't have to send it over a Unix socket.

   But the thing differentiating the two is the new-process-start.
   That is, what happens when we exec.

   When we exec, the address space is destroyed and recreated, not inherited - fine.
   But the fd table is inherited.

   This means we can avoid dealing with address space inheritance.
   We could just have everything in its own fresh address space,
   all fine and good.

   But we can't avoid dealing with fd table inheritance?
   Is that right?

   It would be nice if we could get a fresh address space and put things in it.
   Inheriting addresses would be weird.
   And a gross abuse of virtual memory.

   Since ideally everyone would be in the same address space...
   And exec would merely set up a new program area...

   Which, in fact, we could do in userspace...

   We could implement processes in userspace -
   but only if we had each with their own fd table and sighand and stuff?
   (and assuming that malloc et al was smart enough to not collide other things)

   I mean, if they syscall directly,
   they really do each need their own fd table and things.

   But that is fine.

   Running them in a single address space, somewhat neat, though somewhat useless.

   But! The issue is!
   CLOEXEC does not trigger!

   When we start a new thread running a program we've loaded,
   its fd table is unshared,
   but not CLOEXEC'd.

   Furthermore, if we first spin up a thread to do preparation of the fd table,
   it'll be crazy


   OK!
   classes of fd:

   explicitly passed down: not cloexec or clounshare
   private to a thread: CLOEXEC, CLOUNSHARE
   dynamically passed down: not cloexec or clounshare

   But, what about when we fork?
   Do we really want all the library-private things to go away due to clounshare?

   so when we fork off a *thread*, or when we exec a new *program*, we can't use any of the libraries we had before
   even though the thread could call those functions, because they're in its address space

   but if we fork our process, the libraries are in our address space,
   and we're in the middle of a call stack,
   and we probably are going to want to use those libraries.

   okay, triggering cloexec

   urgh I want to be able to iterate over the cloexec fds from userspace :(

   okay, okay, we'll hack it, a lot of fds to iterate over, but whatever

   so this kind of leaves it as,

   I can pass in some fds into a new thread,
   and some will be dynscoped in.

   If I want to have multiple people operate on the same fd,
   I can just pass it into all of them.

   But the fd *table* isn't shared-mutable,
   the mutations each make aren't visible to others.

   Yes, it seems good

   okay...

   I wonder if I can support both methods?

   Both "perfect shared memory eternal glory",
   and "sublime justice private fd table".

   So I will clearly want to do CLONE_DO_CLOEXEC, right?
   Always?

   Okay, so what I have is,
   two different ways to handle detection of the memory space no longer being used.

   OK, so.
   The private fd space way is cleaner, isn't it?

   That's incremental.

   Now, a true thread would support sharing the table, wouldn't it?

   Well, no.
   It's cleaner for each thread to have its own table.
   FDs are exclusively owned by one thread, after all!

   And that allows for neat process stuff.

   But, sharing the fd table...

   Hey wait a second
   The futex task also exits when it's left the fd table.

   But, it's not clear what the task has taken with it.
   So does it really allow us to close those things?

   We can clearly close the fds in our address space
   The question is whether we can close the communication fds.
   And, again, yes, we clearly can, unless we are reusing it.

   Really, each thread having its own private fd table seems better.
   But it is limiting.

   Language-level access control over who owns an fd is more flexible/better/more glorious.

   We'd need to, um...
   On thread exit, track down all the fds it owns, and close them?
   Along with memory resources too?

   Well uhhhhhhhhhhhhh

   A thread doesn't really own an fd nor does it own memory.
   We have all that managed centrally.

   I mean, why would I use my own thread system, rly
   Well, to integrate it with a nice beautiful epoll event loop.

   OK, so let's suppose I start a thread that shares my fd table.

   I need to be notified when it drops the stack.

   Then I go forward and unshare the fd table.

   I need to CLONE_DO_CLOEXEC (after unsharing) so that private fds, and the fds of other threads, are not duplicated.

   Though.
   Yes, that makes sense.
   It's not possible to unset CLOEXEC while multithreaded-in-fd-table,
   since it may be inherited.

   Everything (except for dynscoped things) is CLOEXEC.

   Then I unshare and inherit everything,
   then I perform some manipulations and unset CLOEXEC on some things,
   and then I call exec.

   So I don't need to CLONE_DO_CLOEXEC, unless I'm not going to exec.
   Unless... I regard this as racy.
   Having the duplicated fd... is there a race there?
   Might I need to do this immediately, atomically?

   Maybe what I need, then, is some form of atomic,
   unshare, unset cloexec on things, do_cloexec

   I mean, alternatively...

   I just don't inherit anything?
   Sounds gross, that's not acceptable.

   So then, maybe something like:
   int unshare_fd_table(int *inherited_fds, size_t count);

   Which unshares the fd table, closing everything in it that is marked cloexec and which is not in inherited_fds.

   Well, what if I implement that by a socketpair?
   Oh, ha ha, I can't do that, because I can't inherit the socketpair! Gross.

   Okay, so I don't think there's a race here.
   Just imagine that I had unshare_fd_table, it's instant, sure?

   But it would be equally instant for me to do the unshare and unset cloexec and call do_cloexec.

   Er.

   That is, one possible interleaving is,
   if there's some thread A that wants to close fds which another thread is waiting for,
   A doesn't run for a while.

   But wait, there's certain things that aren't possible.
   Like.

   Thread A could close fds 4,5,6,
   then signal using a futex that they closed them.
   Then other coordinating process would believe with absolute certainty that it should get hups on them.
   Immediately! Right?
   (if they're local pipes, sure)

   Well, no, some other thread could also have just forked, as things do.
   Since it could have always happened before by fork, I think we don't need to care.

   But suppose we did care!
   It would sure be tough to deal with!

   How would we ever go from a shared fd table,
   to a private one,
   inheriting some things from it,
   while closing the things we don't want,
   without racing against others?
   ...locks?

   a lock on... close?

   no, yuck, screw that

   so if a library somewhere deep below,
   performs a fork,
   what should they do to ensure they aren't keeping other fds erroneously open?

   well, how do we pass fds to the fork?

   I guess this is a good reason to have something other than "dynamic scope".
   or, I guess, globals across multiple tasks

   you really want something absolutely task-local

   well.

   is keeping something memory mapped an example of keeping something erroneously open?
   the address space may be a problem in itself if that's the case...
** okay
   so
   I guess I'll do the split thing.

   I'll completely get exec and exit notification through the fd hangup.

   I'll additionally poke the thread, after that's done, to free the stack,
   and release the task.

   oh god okay

   so I'll make the thread, unsharing the fd table by not passing CLONE_FILES

   then I'll unset cloexec things,

   and then I will call do_cloexec with a thread thingy

   yeah and this is the only way to do it, I can't do cloexec at thread creation time
** unix task thing
   Environ
** translation API
   So we can translate fds in two ways:

   Through inheritance and through fd passing.

   Essentially we should have a function which takes a file descriptor in one task,
   and returns a file descriptor in another task/fd table.

   That works easily for fd passing

   But the range of validity is more limited for inheritance.
   Because new fds can be created in the old task.

   Also, you can only inherit once. Because after you inherit, the new FD could be closed or whatever.

   It sure does seem like a one-shot approach is better, hmm.

   But it's not truly flexible

   We want to inherit fds while simultaneously making arbitrary calls in each side of the task boundary.

   I mean, what would be the C API?
   We have some library object,
   and we want to use it in the new fork/in the new thread with a different fd table,
   so we have to do some things.

   Well, wouldn't it be better to just call into it and say,
   hey migrate yourself to the new task ok

   And it can make arbitrary calls to do so?

   And what exactly are the capabilities it uses?
   The FD translation, of course, but that's one-shot...

   OK, so again, what would be the C API if we provided FD translation?
   Well...
   We do this post-clone, and...
   It just picks some fds to preserve?
   And otherwise does nothing?

   It does this direct style?

   Doing this direct style seems a whole heckuva lot better.

   It would be nice if we could do this direct style while not having to be staged about this.
   i.e. if we kept the ability to translate forever

   Should we just use Unix socket based translation for everything?
   That is a heckuva lot more expensive...

   Well, no, we can just focus on that style of translation for long-term things.

   i.e., we keep the context manager approach,
   and the only do it once approach,
   and...

   OK ok okay, so just saying, "here are the objects I want to translate..."

   It's not really good because I can just write my code as, "take this translation thing in and then run arbitrary function" object.

   So a contextmanager where I can really run arbitrary code is better.
   And in that scope I do the things
   And I leave that scope
   And the curtain falls - but the task remains usable.

   And I have a check that I don't translate things made after the fork

   Maybe I can do all this in the kernel?

   Well...

   I mean, when I fork and unshare memory,
   then I get a nice snapshot of fds.

   When I unshared the fd table and don't unshare memory,
   the memory can change due to other threads,
   and mention other fds.

   So how do I work with that?

   Maybe if I could, like, build an fd table ahead of time and then use it?
   I guess I would want to be able to,
   operate in one fd table and send fds to another.

   But that's exactly what unix sockets do, so...

   The issue I guess is that to do a Unix socket you need to have a reference to both sides
   But how do you handle that when you have already cloned and cloexec'd.

   What about just passing a list explicitly to initialize new fd tables?
   And have everything cloexec be closed?

   Unshare can take this list too

   I guess the idea is, when you don't pass CLONE_FILES to clone, you can pass CLONE_DO_CLOEXEC and pass a list.
   And when you do pass CLONE_FILES to unshare, you can pass CLONE_DO_CLOEXEC and pass a list

   when I exec, I wipe out all the library-private memory (by making a fresh address space) and the library-private file descriptors (through cloexec)
   (it occurs to me that I can't really do exec in userspace, because I can't find out the library-private (executable-private) memory space)

   But what if I want to pass things down through an exec?

   First I need a thread

   That thread needs resources to do the exec

   I want to be able to actually run code in that thread, not just pre-prepare everything.

   But it does seem like pre-preparing everything is better.
   No! I reject that!
   Pre-preparing is in many ways not possible, such as setuid and setns.

   So if we really run substantial code in the thread,
   then it needs resources.

   Those resources could come through shared memory...
   and shared fd tables...


   Well, okay, so, if I want to run arbitrary code in a thread.
   That's not really something I can do.
   It needs to be in a separate address space not just for safety, but also for cleanup.

   So that's fine.
   So maybe I do that arbitrary logic in a separate process?

   Then what I need is a way to establish that separate process.

   And, so, well, I need a way to pass fds down to it.

   In practice, today, that means setting them as not cloexec.

   Which, actually, isn't that fine if I have a single-threaded space?

   So couldn't I, then, just mangle the fds up and then call a quick clone + exec?

   How do I get the error code of the exec?

   Just put it on the stack, whatever!
   It's easy enough.
   And we can use the futex thing to know about its success.
   
   Could vfork factor into this?
   Naw...

   OK, so this essentially takes the approach of, let's never share our FD table or our address spaces,
   except immediately before spawning a thread to exec into something else.

   That thing which execs, can be - if we so choose - a direct binary
   Or it can be a rsyscall thingy which we can then direct to exec.

   OK, but how does this help at all?

   This is just an approach of, requiring the fds to be specified up front.

   Actually the fact that you can't dynamically translate when doing it in a separate process really suggests,
   specifying the fds up front is best.

   OK, so we'll do that.
   Specify the fds, or whatever, up front.
   Now, we need to be careful about it.

   Should we have the cloexec baked into the thread-starter, then?

   And we just unset cloexec appropriately in our thread,
   and they'll inherit those things in their thread?

   We have to be single threaded, of course,
   but I guess we've always needed to be single threaded (or locked) to pass additional fds down through exec.

   If we did it after the clone/unshare...
   Then we could be multi-threaded...

   If we unshared the fd table so we could get exclusive ownership on fds,
   then set some not cloexec,
   then either did an exec or manually did a do_cloexec (rather than clone yet another thread)

   Well then we could be spawning things without a problom.
   Maybe the trick is to have the thing which runs in the thread and does an exec or a do_cloexec,
   not be generic.

   Instead have it be a totally not-generic thing for just unsetting cloexec.
   If I want something generic then I have to exec through into, rsyscall.
   Then exec from rsyscall into whatever.

   OK, but why not be generic, though?

   Because then we have the issue of any fd we want bein' eligible for passin' along.

   What if I have some kinda
   mock thing

   I say let's translate these fds,
   then with that I can build the rsyscall task and other things,
   assuming that they don't actually access those fds...
   though I could dup them...

   or pass them down and close them up above....

   should this be a thread-level API???
   if I pass file descriptors down???

   OK so last quick question, where do we do do_cloexec?

   We do it after creating the rsyscall thread,
   but before returning it to the user?

   Sure

   and we call a function which disables do_translate
   finish_translation or something
   I guess that's the best we can do, we can't give them access to the new task and gate it behind finishing the translation at the same time.
   How do we enforce that they call finish_translation?
   Well, we couldn't enforce that for arbitrary code.
   But for our code, we want that to happen... reasonably soon.
   We can't gate the RsyscallTask behind it because that's taking away real powers, and doesn't help anything.
   What if we could gate the unshare behind it?

   Like, alright, if we want to always be safe,
   then we need there to never be a time when fds are duplicated

   And we also want to make sure users don't try to translate fds that are made after duplication.

   All this really suggests doing it up front,
   before unsharing.

   But if we do it up front,
   that has its own limitations.

   Bah! Screw it! User handles it! Fug!

   Argh ARGH

   So I'm just about prepared to embrace "every rsyscall task has a unix socketpair to its parent"

   But let's see.

   let's list the problems
** list the problems
   We need there to never be a time when fds are duplicated
   And also make sure users don't try to translate fds that are made after duplication
   And also avoid closing fds that have been opened in the new task and marked cloexec
   And deal with the fact that an fd could be closed after translation but before do cloexec

   OK ok so we should do it up front
   We'll take a list of FileDescriptors, and damn the typing issues.
** paths
   Paths probably should not have Tasks embedded into them, it is too confusing.

   "also, we need to figure out how to prevent non-leaf tasks from closing"
** threads
   OK so we can avoid threads hmmmmmmmmmm

   nah

   wait yeah

   otherwise we have to have all this crap just to make threads, BORING

   wait nah

   because otherwise we block others from progressing.

   hmmm

   with mkdtemp, at least, I need the..
   need the thing...

   since I don't wanna block others from running while...
   the rm runs...

   yeah.

   so let's get some kinda standard task,
   and let rip

   StandardTask or something which has all the resources inside it

   Or like, ProcessResources?
   HostResources?

   TaskResources? That would have the FD things and child processes
   ProcessResources would have the functions and libraries
   FilesystemResources would have the binaries and stuff

   I guess we can put this inside the Task?
   And unshare them appropriately when necessary?

   Hmm.
   What if we had a dictionary of resources inside
   FDNamespace, VMNamespace, MountNamespace?

   Since inside those namespaces you can access anything...
   No that would be awful

   StandardResources containing the aforementioned.
   And a Task.

   Maybe StandardTask then since it contains a Task.
   Or EnrichedTask.
   StandardTask, and I'll change the name later if necessary
** passing path parameters
   So I have two alternatives.

   Alternative 1: I use a contextmanager to set the fd to not cloexec, then set it back if there's an exception.
   Alternative 2: I build up a list of fds that I need to pass down as I serialize various objects.

   Alternative 1 is kind of like building an implicit list of 2

   Maybe I should just not even re-set cloexec

   Just do the mutation with no way back

   After all that's how most things work...

   setsid for example

   ON the other hand, serializing a path over a unix socket, for example, demands that I get a list of fds out.

   And one can't really serialize a setsid.

   Building a list seems better then.

   Hmm, serializing a path over a unix socket is tricky though

   Because we can't encode the same fd number in it.

   Hmm, and furthermore we can't rely on root dir or cwd.

   Really when serializing a path over a unix socket,

   we need to always open it as a dirfd and a path segment,
   and pass those along explicitly.

   Which is very different from how we can inherit paths!

   Having a different path for inheritance seems better than.

   So it's a bit weird though,
   what if we make a path into an argument and then chroot?

   Well don't do that then!
** what to do
   Once it reaches a certain level of maturity, I should use this inside TS to get more validation/testing.

   i.e. immediately upon finishing the basic stuff

   My goal: configuring/running nginx.
* criu
  I feel like I have a bunch in common with criu

  They are manipulating and preparing tasks as objects,
  and I'm doing the same thing
* unrelated thought
  dependency injection is autoresolving dependencies by type???
  that's... that's... that's implicit parameters :(((((
  looking up things by type :((((
  but I hate dependency injection!!!

  urgh, I've just had a terrible thought
  a typeclass can be modeled as an additional argument that is automatically looked in a table indexed by type, right?

  well it's just occurred to me that there's another practice that is like that,
  where an implementation of some functionality is looked up in a table indexed by type,
  common in a certain primitive language.

  and it's a practice that has reviled and disgusted me,
  so it's terrible to think that they could be the same

  how can you achieve coherence without global uniqueness of instances?
  just error if there's ambiguity or whatever
* multiplexing without depending on event-loop specifics
is single-prompt yield not powerful enough to do multiplexing without control over the prompt?

how about multi-prompt yield?

well, if I had a multiprompt yield.
then I could have a prompt for my specific multiplexer
and I'd want the user to be able to just call into me without themselves having to be within some handler

so they do it
but
oh even with a wrapping a function around the prompt to get the continuation, I only get the cont under the prompt, derp

a message passing perspective

yield to specific prompt (that's the cap they call on)
then it's good
so multi-prompt yield with first class prompts would be fine, assuming we could keep the handler on the stack.

maybe I could implement multi-prompt yield in python

it wouldn't be slow:
we'd have a separate stack, essentially, for each yield thing. which is fast.

and it would be in direct style.
when someone yields back up to us,
we'd see that as a response to a send into them.

and we'd be able to continue on and mess with them.

but, I feel like that must be too powerful for the simple use case of wanting to multiplex access to a single resource
(aka: taking requests in, submitting them to the resource, and sending the responses back out, possible in a different order)
I feel like there must be a simpler way to think about multiplexing access to a single resource than that..

well that sounds like message passing
which is equally powerful

so really that's just exactly what it is...
is it *exactly* what it is?

in a sense, yes.

is it REALLY what it is?

what other ways to multiplex are there

well exokernels have embedded code
i could poll at user level
i could have it wake up everything
maybe there's some relevance of passing down an fd...

if I have it wake up everything then I could handle it in user code.
but how would I wake up multiple things???
they'd all wait for one thing
and when it happens they return

ho hum hee

if wait_readable is my only primitive, can I do it?

I guess this is related to the whole question of,
what is the syscaller interface? how wide is it? etc

hmm, I just realized that I think this is essentially the same question as another question that I have previously deferred until later
(that other question being, "do I really need to expose wait_readable in my interface-to-native-system-calls? maybe there's something more naturally ~Linuxy~ I could expose")

hmm, actually it just occurred to me that this question is essentially the same as an earlier question I was pondering and deferred until later, so I guess I'll just defer this one too, and be trio-specific for now
(the earlier question was whether to expose wait_readable in the system call interface which abstracts over making system calls in the current thread vs. in another thread, or to expose some other primitive async mechanism, or do something else, or whatever)
** should we call PDEATHSIG in the trampoline, or with rsyscall?
   In favor of trampoline is that it will apply for all threads

   Against trampoline is that it will not reaaally work for everything

   If the parent dies, but there are still some threads alive in the address space,
   when we die, our stack won't be cleaned up.

   Well wait, that's an issue with rsyscall as well.

   No, no, we solve that by just making sure PDEATHSIG is set for all of our immediate children, right?
   Is that the plan?
   Yes, I think so.
   We don't want anything in our address space to survive our death, right...?

   Well, we could. But that would require careful resource management.
   So I think PDEATHSIG does work.

   Then we're back to the question of trampoline or rsyscall.

   What if we wanted to trampoline into something that should survive our death and be in our address space?
   Again, this would require careful resource management.

   Do we even want anything to be able to survive our death? I argue no!
   Or, more specifically, by default that shouldn't happen.

   Also, in the future we'll have an inheritable immutable PDEATHSIG.

   Although, we still want to daemonize things, don't we?
   How will we deal with that?

   I guess we'll have a PersistentChildTask,
   which is really a supervise (possibly watching multiple children)

   We'll have to unset PDEATHSIG for the supervise,
   as well as for the children.

   But, it will be in a separate address space anyway.

   Hmm hmm hmm.

   So yeah, how will we design PersistentChildTask?
   That is something tricky.
** PersistentChildTask
   Oh boy, this is a tricky one.

   Yep.
   Tricky.

   Okay time to start thinking about it.

   Could we just have a persistent rsyscall task which is not our child?

   That's perfect, yes, let's do that.

   That matches the form of remote tasks too, we can just reconnect to them.

   It's SLIGHTLY awkward that if we kill it, it murders everything on that host.

   But I guess we could have multiple of them on a host.

   It could be some kind of rsyscall server, where when you connect to this unix socket,
   it spins up a thread for you?
   But it persists forever while the unix socket lasts?

   hmm

   but we do want a specific thread to actually persist,
   don't we?

   We kind of need that, to preserve the child-parent relationship.

   We could become aware of SUBREAPER...

   Though that doesn't help, does it, we'd just reparent to some parent person,
   and want to preserve them,
   because if they go away, then we're ruined.

   We could use supervise to make the child-parent relationship irrelevant.

   Hmm.
   If an rsyscall task gets wedged by me sending a bad syscall,
   aren't we screwed?

   We can't recover.

   Wait wait wait, of course we can recover.
   EINTR is a thing... wait, will we not get EINTRs?

   Can we send it a signal that we've specially designated for interrupting bad syscalls?
   Yes, and we can get EINTRs.

   That seems distasteful.

   But it works.

   Okay, but, so, I'll either do persistent rsyscall tasks, or use supervise and a Unix-socket-serving thread-server.

   But in either case, don't I need to be able to start things that don't die?

   Also if I use supervise...
   Can it be a thread inside the thread-server, which I communicate with using an fd?

   And I'd probably do the same with the persistent rsyscall tasks, right?
   They'd just be threads in the server and I'd access them through a thread-server.

   Hmm.

   A thread-server, which implements the thread creation logic I've done in Python, in C.
   That seems fairly interesting as a concept.
   PROCSESES AND THREADS R FILES, DEAL W/ IT HATERS

   Okay but wait, it would be specialized to only work with rsyscall server threads.
   That's undesirable.

   Could we..
   could we just...
   send it a function address and some arguments...
   and get back a whole lotta nothing...

   Hmm yeah, like, how do we send it to the fds or memory or all that stuff that we might want to send it.
   I guess those are questions for our runtime to solve, not our kernel-interaction-thingy.

   Only dealing with rsyscall servers seems neat.

   Though! How do I get the data?

   Also, again, I guess I'm just considering something like,
   I have some threads, I get the fds to talk to them with...
   I send those fds *into the server*,
   so it persists.
   Is that how I'd make a persistent thread?
   But how would I do that?
   How would I send the fds in?
   Since I only have one side, and they have only the other side,
   how would I get the fds from one side to another.

   Also, again, the data fd.

   Oh wait, I guess the threads I create wouldn't be persistent.
   In fact I could even connect to the thread server from inside the server.
   Then it'd be okay.

   So then, data fd. How do I data fd.
   Hmm...
   So I can allocate memory without memory, using mmap.

   Then I can write into that memory one byte at a time with syscalls, though that's mega-gross.

   Or I can do the multiplexing trick, but that requires care.

   So I guess, uh

   Oh wait, screw connect, let's just use a protocol.
   We'll say, "hey gimme some fds BAE",
   and it'll send them over.

   Although that's tricky when done remotely.

   We could have an initial process that we get,
   which we then use to connect to the persistent process server.

   And we get that thread,
   and we send the fds back to our initial process over unix sockets.
   Seems good.

   Wait, wait, no, no.

   Then we have to go through two address spaces. Not three, which is nice, but still two.

   So we'd really like a direct connection to the persistent process server.

   Wait, can't we just do that?
   We pick ourselves up, send our fds over, and throw ourselves over, by our bootstraps.

   So we get our initial process. It comes with a free data fd, compliments of the house.
   It has access to the unix socket.
   We open that to get a thread.
   We send the fds for our initial process over the Unix socket.
   Including the datafd.
   We dup those fds over the one for our thread inside the thread-server.
   Now we have direct access to the thread-server.
   And we only have one data fd.

   So that seems fine and good, and at least that's possible.
   We'll probably think of something better later.
** so how do we actually run one of these
   We probably have an initial initial process, right?
   Which just runs vanilla rsyscall.
   And we bootstrap our way into a persistent thread-server from there.

   And how do we do that?
   Well...
   I guess we can exec our way in, assuming our initial process isn't forcibly terminated.

   But is that sustainable?
   What if we want to create another one?

   We won't have another task that is unparented/unfilicided.

   So, what do we do?

   Oh wait, I think it's fine, isn't it?
   Don't we just double-fork?
   Standard bidness?
   That's gross though, I don't want to double-fork.

   Hm, so.
   This is sounding like I'm in favor of doing it in rsyscall, as a first-class thing.

   And then I can choose to not do it, and instead exec something that will persist.

   Of course, the real issue is nesting persistence...
   What if one of the applications I start, wants to persist?

   But I don't want to persist?

   I guess I have to give them the ability to persist.
   I have to pass it down.

   How do I pass down the ability to persist?

   Well. I can allow a task to live forever by default,
   and remove that ability with a persistent pdeathsig thing.

   Meh, it's some kind of silent persistence, but I guess it's fine.

   Maybe I should have the inheritable PDEATHSIG thing partnered with a clonefd?
   And the inheritable PDEATHSIG doesn't kill things made with clonefd.
   That way you can make pseudo-persistent processes which live past a tree's death.

   How would that work?

   I guess maybe I can like,
   get an fd for some process,
   which unsets pdeathsig thingy for it??

   Designing this API is hard :(

   OK, so yeah, doing it in rsyscall in code is better.

   Alternatively, we could just do it by default.
   *Should* it be by default?

   How do we control this?
   How do we control persisting tasks?

   Well, the ideal way is by linear resources.
   When we go away, the resources we own (our children) go away as well.
   
   But, we can opt in to survive the death of our parent.
   
   But, if we want to guarantee our children die because they might maliciously do that.

   Hm, hm.

   So what if it was literally a capability that's passed down?

   Oh, what if it was some kind of process group fd?
   And when they're closed, everything inside is killed...
   But you can attach your process to them, and survive!

   Like normal process groups, they aren't recursive.
   You can only be in one.

   And so, if someone passes down a "survival" process group,
   you can enter it.

   Actually maybe it should be recursive,
   then you can make more fine grained divisions.
   You can spawn a pgroupfd off of another one,
   and it will be the child of that one.
   And, if the parent is closed, it will be forcibly closed?

   Well, maybe that's silly

   Okay, why would we want to be recursive?
   Why can't we just add ourselves to the survival group?

   We could, we could, we certainly could.

   Can we also solve the "library wants to run subprocess" problem this way?
   Well, we can have the library set no signal at clone time.
   Then what?

   Could we also also solve the "detect reparenting" problem?
   Say, because you stay in the process group forever?
   And maybe you can get notifications when anything in it dies, no matter the depth?
   No, that's a mistake

   But maybe there's a "top level" of the process group,
   and when your parent at the top level dies,
   you go to the top level?

   And you can get notifications for things at the top level dying
   Nah that's complex

   Oh wait this doesn't work
   Self-reference, as usual, foils it.

   So that would imply we do need recursion.

   That's interesting.
   If things have owners, self-reference is a problem.
   But if owernship is a stack, it's no problem.
   That's like linear regions etc

   Anyway.

   I think the default would be death in any of these schemes.
   So I think it makes sense to default PDEATHSIG on.
   Turning it off is a rare operation anyway.

   Yeah and let's do it in rsyscall main, eh?
   umm

   oh wait, heh, futex waker isn't gonna wake up and die when rsyscall dies...
   no wait yes it will...
   let's put it in rsyscall main for now.

   naw, in that case, let's do it at startup.
   no wait, what if we die before that?
   we have to set pdeathsig as soon as possible to avoid this race of death...

   no wait, it's fine, there's no race, we can conditionally do it

   argh gotta fix up rsyscall trampoline or something

   blah, let's just put it in rsyscall and whatever
* cool feature: context manager container
  I can start a container with a contextmanager and spin it off by returning.

  hey, and supervise is a nice pid1 to run inside the container

  neat neat neat neaaat
* standardtask with my own additions
  I could have my own additions-thingy inherit from Task...

  And have it be stashed inside Paths and things...

  That's not particularly type-safe eh
* things to improve
** TODO fix the race with PDEATHSIG so that a thread dies even if we die before it sets that
   I guess we can... know the pid of our parent (have it passed down),
   set pdeathsig,
   getppid,
   if ppid isn't what we expect, suicide.
** TODO use AsyncExitStack heavily to manage resources!
   When we pass in a resource to something,
   we need to immediately put it on our AsyncExitStack.

   Then when we finish successfully, we can cancel some of the things off of our AsyncExitStack.

* portability things
** assuming there are 64 signals so sigset_t is an 8-byte integer
   I think you can probably compile the kernel with more signals?
   But maybe not because it looks like it's hardcoded in a header somewhere.
** rsyscall assembly code
** building a stack manually to pass to rsyscall trampoline
** architecture-specific syscalls
   clone at least is arch-specific in its argument order
** using /proc for bind and connect
   Because there's no bindat and connectat.

   This is an awkward blemish
* desirable kernel features
** notification of exec (more specifically: mm_release) through wait
   This way we can detect when a child task/thread has called exec,
   and free any resources they had.
   Otherwise there's no way to know for sure that they're done execing.

   This is like CLONE_CHILD_CLEARTID,
   but that only has a futex interface,
   which is not suitable for an event loop.

   Currently I'm just starting one thread to monitor each futex,
   which duplicates the number of threads I run.

   ptrace doesn't work because ptrace sucks and break gdb/strace.

   Passing down one end of a cloexec pipe doesn't work because if we're a CLONE_FILES thread calling exec,
   the end will still be open in other threads.

   Using futexes from a normal event loop sucks.

   vfork is kind of similar but sucks.

   I think this would allow an asynchronous posix_spawn.

   Currently posix_spawn *must* take over the thread to use vfork.
   But if you want to spawn another thread to do it...

   ugh then I guess you can just do that, spawn another thread and then posix_spawn.

   Oh but that messes up your child relationship! So yeah, you can't do that!
** a CLONE_DO_CLOEXEC flag to pass to clone, to close CLOEXEC fds
   This would be good and useful.

   Though we can do it in userspace, it's ugly and requires creating an extra thread for full correctness.

   And it would be simple.
   Just need to call do_close_on_exec.

   oh god, clone really is out of flags
** sigkill-resilient filicide
   Maybe, like, the combination of supervise and PDEATHSIG?

   Except, we'd want like,
   CHILDDEATHSIG...
   a signal to send to your children when you die.

   If we want to combine this with a better proc interface,
   we could maybe have some kind of childgroupfd,
   which when closed, kills all the children in that group.

   We can't realistically enforce things the same way capsicum does,
   which is to ban usage of non-forkfd process creation.
   Also I don't think that's robust to loops anyway.

   Oh, the same is true for the childgroupfd then.
   Well, no, they can create childgroupfd within them, but,
   they're still contained in the childgroupfd.

   Oh! Let's just have an inheritable PDEATHSIG which can't be unset!
   That's easier conceptually.
   Though, reparenting will still happen before the actual signalling, which is awkward.

   Maybe I should have the inheritable PDEATHSIG thing partnered with a clonefd?
   And the inheritable PDEATHSIG doesn't kill things made with clonefd.
   That way you can make pseudo-persistent processes which live past a tree's death.

   There are several things we want:
   - it should work to kill legacy processes that don't use this API
   - there should be a means for processes to still daemonize if we want to allow that

   A childgroupfd does actually meet these.
   If we have a childgroupfd,
   all the children in it can be killed when we close it,
   but not if they are inside their own childgroupfd.

   Like process groups, but as a file descriptor?
   Hmm, that seems like a nice idea.

   So you can escape the process group easily, is the thing?
   Maybe we have a flag, "escapable" or not.

   Of course, escaping will just require that something has a reference to its own process group fd.
   oh, capsicum doesn't support passing process fds
** remove old confusing comment from dequeue_signal
   in signal.c

   It just wasn't removed in b8fceee17a310f189188599a8fa5e9beaff57eb0 when it should have been
** bindat, connectat
   Would be nice to be able to pass a dirfd for these. And also lift the length limit.
** some way to deal with fd inheritance?
   CLOUNSHARE?
** the ability to take existing memory and wrap it up in a memfd
   This would be useful to allow a uniform interface for operations.

   Instead of providing both an interface for operating on memory,
   and an interface for operating on file descriptors,
   we could just have a single interface that operates on file descriptors.

   Anyone who wants to operate on memory has to wrap it up in a memfd first.
** clone_child_settid doesn't seem to work without clone_vm
   i.e. it doesn't work when the child is in a different address space
** exec needs to take a list of fds
   fds that it should pass down through exec.

   Even if they are cloexec.

   Then cloexec can be inherited.

   This allows us to use execveat on scripts and things.

   We just put the fd in the pass-down list.

   Of equal relevance, this also allows shared-fd-space tasks to exec and pass down fds.
   (otherwise they can't remove cloexec because it's racy)

   Hey, and exec has a remaining argument left!
   We could add it!
** make connect(Address), accept(FileDescriptor) symmetric
   i.e. so it's connect(FileDescriptor) and accept(FileDescriptor)
** unify connect/accept with passing file descriptors
   This would be a nice design...

   Even better would be to unify it also with openat.

   Then there'd be only one ways to get a file descriptor:
   Use openat on another file descriptor.

   (well, and also by passing it in)
** being able to pass a NULL name to memfd_create
   Requiring memory to allocate memory makes it impossible to bootstrap memory allocate using memfd_create.
** add a MAP_DONT_ROUND flag for mmap to not round the length and offset to page size
   Currently we will round them to the page size,
   and the page size depends on the arguments and what kind of file descriptor we're mapping.

   But because mmap rounds the length up,
   munmap on the resulting mapping won't work with the mapping we pass in.

   We'd rather fail to mmap,
   than get a mapping that unexpectedly can't be munmap'd.

   If we get a file descriptor passed to us from someone else,
   that file descriptor might have a larger page size than we expect,
   so if we mmap that file descriptor it will unexpectedly silently round up,
   and then munmap won't work.

   Plus we probably also want an fcntl to get the page size.

   Ah, and also this flag should probably require that we pass MAP_HUGETLB and the right size bits
   if we're mapping a hugetlb file descriptor.
   That way we won't accidentally pass the correct length?
   I guess if we accidentally pass the correct length then it's fine.

   Still probably should require the right size bits.
** CLONE_PARENT doesn't need to be disallowed for container inits
   It's already possible to make siblings of a container init,
   and any other process can call CLONE_PARENT whenever it wants.

   So, let's not bother disallowing it.
   Since it's useful for me, since I can have my parent monitor my children centrally, instead of me...
   And it allows for more correctness possibly:
   since I can maybe avoid process child waiting raciness by just having,
   when I waitid,
   everything is from orphans.
** put additional flags in accept4(flags) to accept only specific sock types
   If I can just put additional flags in accept4,
   that allows me to conveniently do additional typechecking of my accept.

   similar to nsenter.
** at-functions should operate on the fd when a null pointer is passed for the pathname
   That way I don't have to allocate memory for them :(
* possible kernel bugs
** MMAP_GROWSDOWN doesn't seem to work
   It doesn't grow.
** CLONE_CHILD_CLEARTID doesn't seem to work if you don't pass CLONE_VM
   Even if the address is in shared memory (mapped with MAP_SHARED, same inode and everything in both processes),
   it doesn't seem to do a futex wakeup.

   Didn't yet check whether the address is cleared or not - probably not.

   Oh, this is probably due to the number of mm owners being 1.
   It skips the wakeup in that case. Infuriatingly...
** (possibly creates userspace bugs) munmap works differently on hugetlb files
   This means that if I control the file that some userspace program mmaps,
   I can ensure that their later munmap will fail.

   That seems at least good enough for a denial of service,
   and possibly could even cause security problems.
** epoll with EPOLLET doesn't report up-edges if the down-edge has already happened
   I bet epoll doesn't actually send me every edge.

   I guess I can test this.
   
   blaaah

   argh this makes sense, and it applies even more clearly for level triggered
   if the epollfd is readable, and then it's not readable, i'll be screwed.

   nesting epolls doesn't really work


   aaargh

   so the scenario is as follows:

   add readable pipe fd to epollfd with EPOLLIN|EPOLLET
   select() or poll() or nested-epoll on the epollfd, see that the epollfd is readable
   read from pipe fd until EAGAIN
   read from epollfd, get no events, boo!

   ok whatever i guess this can also happen with signalfd

   couldn't fixing it cause bugs?

   after all...

   if in response to an epoll readability notification,
   you perform a blocking read on a pipe FD A,
   as is currently safe...

   so, the fix would make it so that,
   if you add stream socket FD A to an epollfd (with EPOLLIN|EPOLLET),
   then the FD becomes readable,
   and you happen to perform a nonblocking read to flush anything remaining in the pipe buffer,
   then the next time you wait on the epollfd, with a fix, you would get an EPOLLIN event.

   without the fix, getting the EPOLLIN event means it's safe to perform a *blocking* read,
   so you might go ahead and do that,
   and then deadlock.

   the people in #kernelnewbies said to report a bug so I guess I will

> argh
> epollfd has an annoying behavior where if an FD registered on it becomes readable, the epollfd will indicates as readable when checked with select/poll/another epoll instance...
> but if the FD is read to EAGAIN before the epollfd is epoll_waited on, you won't actually get an event for it, and the epoll_wait will block
<sarnold> that sounds squarely in the "do not do that" category to me
> well - I would expect that if an fd A is marked as readable, that performing seemingly totally separate operations before "reading" fd A, would not cause fd A to longer be readable
> but thinking about it, I guess you could probably get a similar behavior from signalfd if you unblocked the signals it's monitoring before reading it
> but I still think it's pretty weird for EPOLLET to effectively drop events - my mental model for epoll in edge-triggered mode was that I'm getting a stream of *all* the edges
> not just the edges that happen to be "visible" at the time I epoll_wait
<ukleinek> catern: sounds like a bug to me
> (I seem to also recall reading, possibly in some manpage or maybe TLPI, that when readability is indicated on an FD, it's safe to perform at least one blocking read, though maybe that's only for pipe/stream sockets... bah)
> pipes*
> ukleinek: well... I worry that fixing it could cause bugs, in fact
<ukleinek> catern: escalate it to the right maintainer?
> hmm, who would be the right maintainer for epoll? it isn't listed in MAINTAINERS
> I guess I'd just send it to lkml, linux-api, and CC some people who've recently touched epoll?
<derRichard> catern: btw: did you check https://idea.popcount.org/2017-02-20-epoll-is-fundamentally-broken-12/?
<derRichard> maybe it covers your case
<derRichard> epoll is very hard to use right
<derRichard> it is full of pitfalls
> yes, I saw it, and I disagree with that article :)
> IMO it's perfectly straightforward to use if you use EPOLLET and you don't try to rely on auto-closing
> you just have to have a proper ownership model, so that fds registered on epoll can't be closed without going through epoll
<derRichard> yeah
<derRichard> and if you know that, you better report a bug
** CLONE_SIGHAND + CLONE_NEWPID don't work right together (see pidns.c)
   signal(SIGCHLD, SIG_IGN) seems to be set in the parent when I kill the pid namespace init (my child).

   So I don't get the SIGCHLD signal.

   More than that, even if I have the SIGCHLD signal blocked in the parent, it doesn't get set as pending to me.

   Strange, strange.
* design decisions
** TODO should we grab the Task out of other objects, or pass it explicitly?
   Consider RsyscallTask.make as an example.
   It takes a ChildTaskMonitor and an Epoller as arguments,
   both of which contain a Task.
   Should it also explicitly take a Task,
   or just use one of theirs?

   Also we can grab the Epoller out of the ChildTaskMonitor, for that matter.
** sharing SIGHAND/handling SIGWINCH/setting up thread locals
   Python sets up a SIGWINCH handler.

   In an rsyscall thread, the thread locals aren't set up.
   So errno isn't set up properly, so any code based on the libc isn't going to work.

   The signal handler will therefore segfault.

   I could try to set up the thread locals properly, but that would require something libc-specific, probably.
   Or using glibc's clone interface but... 
   Yeah, it's not possible since I'd need to set up TLS on my own by paving memory etc
   Not calling functions.

   Well, I guess I could maybe have a helper for it?

   In any case, it's complicated. Better to have these threads be thoroughly non-POSIX.
   Which means they can't share signal handlers with the POSIX thread.
   So we'll probably clear out the handlers on thread startup?

   Yes.

   OK, we just wipe out the SIGWINCH handler, because that's easy and solves the immediate segfaults.

   We don't bother to properly deal with signal handlers other than that.
   Don't send the rsyscall thread signals/don't established handlers in the main thread.
** module is organized in a way that mirrors C headers
   This is the only organization that has any chance of being familiar.
** paths don't embed dirfds
   Unlike cloudabi or other straightforward translations of capability-security into a POSIX environment,
   our native path type doesn't embed a dirfd inside itself.

    two things:

    no "far" or "handle" equivalent for paths
    (everything could have a far/handle equivalent, but we don't do it for most structs.
    because most of the time the ownership is too tricky.
    we only do it when it's convenient, namely for fds, pointers, memory mappings;
    things with clear ownership semantics)

    no dirfds embedded in paths,
    they're just names

    which really is a consequence of them not have far/handle equivalents
    because embedding a dirfd is making something far, really.
** don't attempt to abstract over memory
   At multiple points, I decided to not abstract over memory as much as I had been doing in the past.

   Abstracting over memory complicates the implementation,
   and is less flexible for users.

   We ultimately have gone with being as powerful as possible without abstracting over memory;
   and it turns out, abstracting over memory isn't even really necessary to have a good user experience.

   I hate the socket API though, because it's the one wart here.
** we use str pervasively instead of bytes
   The str type can encode arbitrary bytestrings using surrogateescape,
   Python has embraced strings-for-filenames with the PathLike interface,
   and str is much more convenient to use.
* knowledge
** tkill vs kill
The difference between kill and tkill (besides the special negative parameters to kill, which tkill
doesn't support) is that kill always delivers the signal into the shared SIGHAND table, whereas tkill
delivers the signal into the task-private (thread-private) table.  Note that pthread_kill, raise, and
other such things work like tkill (though they actually use tgkill).
* game
  non player processes

  file descriptors are inventory

  namespaces are rooms

  avoid metaphor: namespaces aren't rooms, they're just what exists in this world
  essentially we'd build a rich shell for exploring Linux things
  (the python interpreter)
  but also have a text advanture like simple translation layer
  which translates simple commands into obvious 

  sockets are telephones?
  you can pick one up and it makes a copy I guess

  I can use this to demonstrate the essential weakness of the concept of,
  just putting an internet socket out there where anyone can dial it if they have the right number,
  by having a call verb that allows you to call people

  and have some kind of task where you want to provide a service,
  and want to avoid some prank callers from calling you.
** hmm well
   if sockets are telephones,
   we'd want to representing blocking when you try to connect.

   but to do that we need p'rhaps to have the socket pre-existing,
   so that we can use some magic to make it nonblocking.

   a fearsome heat sears your hand and you snatch it away!
   is what happens when you EAGAIN.

   (i guess you heat things up by making them nonblocking?)
* what is this? explanation to normies
it has the intent of providing an interface to Linux tasks as first-class internal entities,
that can be manipulated by running syscalls inside of them
** amoeba
it's kind of going back to python's roots and being a distributed scripting language :)
** make low level functionality generally available
   there are things that are easy at a low level, but become hard with layers upon layers of abstraction.

   let's make an abstraction that is much lower level and much thinner than usual,
   to undo those hardnesses that are created by abstractions.

   maybe abstraction is a bad word for this.
** it's a distributed thread library
** it's a library for building distributed systems
** it's a way around the reduced expressiveness of existing IPC/RPC APIs
my current thought is that we resolve the problem of reduced expressiveness,
by having everything in one single program,
which is able to act across remote hosts directly if it needs to.
(somewhat inefficiently)

and if we want to increase efficiency for some task,
we'll explicitly describe some limited protocol,
for performing that specific task.
so we don't have to be generic.
** salespitch, Unix style
   do you feel a deep joy at hearing about ucspi, pipelines, socket activation and inetd?

   do you feel frustrated that so many new ways to run processes across multiple machines are cropping up
   and inventing their own new ways to compose processes "inspired by Unix",
   instead of just actually using the literal Unix model that works so well?

   well worry no more,
   rsyscall is here.

   (hmm s6 people hate the words "socket activation" so maybe skip that)
** integrated thread library and container deployment system
** a language which uses other processes for most of its functionality
   instead of pointlessly writing a webserver in this language,
   we just start and run nginx.
** direct style deployment
   Instead of "declaratively" building up a program/model of our deployment/distributed system,
   which we then submit to a runtime to be deployed,
   instead we just directly deploy each component as we describe it.
** everything should be a library
   like Jenkins

   or Kubernetes

   why does a Jenkins pipeline run inside Jenkins? is gross!

   have standalone program use Jenkins features as library! easy!
   support many languages! not single groovy sandboxing insanity!
** native API for starting remote processes
   there are many different APIs for starting remote processes

   specialized to various things
   like kubernetes
   or like some API to spawn a game process like Google's thing

   but now we can use the native API instead of something layered on top

   this API is already there, now you can use it!
** minimizing mobile code
   All these systems give you some means to give them code,
   which they then run on your behalf, out of your control.

   the spirit of rsyscall is to minimize that as much as possible.
   you have a single program that controls everything.

   instead of uploading some program to some deployment system to run on your behalf in the appropriate environment,
   you just directly run your code.
** totally separating "running a language" from "operating system API"
   Usually the APIs for stuff like memory and files are tied closely to how the language actually runs.

   But we don't care how the language runs at all.

   The language implementation, and the OS API, are totally separate.

   The language implementation probably uses memory under the hood,
   but we expose a totally language-based API to memory,
   that doesn't involve the details of, "oh these classes are actually pointers to memory".
   Our memory isn't convertible to language objects except through explicitly reading it into the language implementation,
   and deserializing it.

   For us, memory isn't immediately accessible:
   It must be explicitly read into the language core,
   and explicitly written out from the language core.
** first-class processes
   Treating operating system processes as a first-class object to be manipulated.
   The methods of this object? System calls in that process.
** escape the process monad
   normally we run our programs inside a single instance of the process monad,
   but monads have a major limitation:
   you can't manipulate multiple monads of the same type at once.

   to do that, we need to escape the process monad.
   that's what rsyscall does.

   our programs are no longer forced to run as DSLs in the process monad.
   instead they can manipulate the process monad and run their own DSLs.
* explicitly modeling memory
how would I do this?

why do I want to do this?

so I need a syscall interface which is indifferent to memory,
I guess,
for most functionality.

or, no.
i'll only implement some few things as indifferent to memory?

I can implement

like, child monitoring, I guess

so ultimately what I want to do is read a file

and the most efficient way to do that is to run cat on the opened file,
connected to a TCP connection to the hub

but, when the opened file is on localhost

then, that is not necessary.

so we only need to bring it to localhost if we are actually gonna look at the bytes

read is not the way to get the bytes in a file.

we need some higher-level thing.

we want to bring it to the local task.

which makes local memory rather special

or rather, memory which is under language control

okay well I could deal completely with opaque memory

and then have a special case inside the library to copy/convert local opaque memory to language control

then I can have Path library that,
supports read_text,
and,
is aware of the distinction between localhost and remote host?

yeah...
one specific task has the attribute that,
its address space is shared with my address space.

(or multiple tasks like that really)

or really actually,
a task is in an address space,
so it has the AddressSpace object in a field
and there's a global local_address_space,
which we can compare against the task.address_space with "is",
to see if we can directly access memory associated with that task...

so that's good for the in-local-task or in-address-space-task.

but what about same-kernel, different address space?

socketpair

need to handle it anyway with data socket
ehhh

what do we put in standardtask?

standardtask can, i guess, be an abstraction?

no, it will be real

but then how will we achieve memory-using syscalls?

ah, we'll have standardtask hold an abstraction,
MemoryManipulator

which might be shared between others in the same address space, maybe.
or might point to a dedicated thread for the purpose of manipulating memory.

but in any case it will provide the memory-abstracted syscall interface we want, right?

maybe, or maybe not

we need a really true syscall interface

that presents all the details of memory

then on top of it we can build SyscallInterface
which is memory-abstract.

I guess StandardTask will also hold a MemoryAllocator,
which doesn't need to be abstract.

that's what users and classes can use,
to get memory in the remote task,
so they can make system calls that need memory and stuff.
it's a nice simple dynamic allocator.

then I guess the Path thing itself,
for reading a file to bytes...

cases on whether the Task is local or not, I guess

if it isn't local,
it uses the cat-to-datafd trick.

if it is local,
it just reads it directly.

this is an operation on a file descriptor, I guess
and it works in batches
and, is probably async?

now wait a second, how does the Path get the datafd
how does it manage to get a connection

well,
hm

a StandardTask, I guess, will provide a means to create a new pipe between,
the task inside the StandardTask,
and,
a closer task???

probably through a new TCP connection, at least inside TS where we have a trusted network.

so how do we read a fd to bytes?

so we have to have a reference to the local task, of course.
and a reference to the remote task.
and then, the ability to create a pipe between local and remote???

which possibly requires a route of tasks between local and remote

so then to read an fd to bytes

we create a pipe between the local task
and the remote task
and cat the fd to that pipe and read it in the local task

so in fact, in general,
this is a way to transfer data from,
one task to another.

so to read an fd to memory in another task,
we can do this thing.

we provide the memory

or we provide the fd to write out to
(if we provide an fd, we directly cat into it, giving the remote pipe in as cat's stdin and the fd out as stdout)
(like splice between two hosts)

and so the wrapper around this is just then,
to read something to bytes we first allocate memory,
then do a read from a remote fd to bytes in a different place)

and hm
if we're not going between tasks at all,
we can just read directly.

so this doesn't require special casing local memory nearly at all,
just in the very first step when we allocate memory,

this is nicely abstract

could we reduce it even more?
just have memory? or just have an fd, even better.

use a memfd if you want to read it in memory...

it would be nice if we could take some existing memory and wrap it up in a memfd.

that would be cool
we should not bake that in, though

so okay, this is the foundation, right?
we can take memory or file descriptors from any task,
and splice/copy it on to memory from any other task.

this is how our memory manipulator will work

we actually might not even have read/write operations on memory

instead all reading/writing has to happen directly locally.
cuz, duh, we can't actually read/write from remote stuff.

different thing: we can always run the read/write system calls, on task-local memory and fds.

so when we're reading a file, what will we do?

I guess we'll go through the generic interface

we'll say,
here's some memory in the local task so we can get some bytes
here's an fd in some whatever task
then we pass those two to rsplice
and it does the efficient thing to copy from fd to memory.

what about, also, when we can just directly use local memory?
that is to say:
when we're making syscalls in the local space,
we can build the args with bytes,
then directly pass those bytes to the syscalls.

so really that's another thing I guess?
appearify: mem -> task -> mem
which takes some memory, and a task
and returns some memory in the task

naturally,
this is a method on the memory manipulator or memory allocator thing
since it might need to allocate?
but also might not?

and we have our bytes,
and we turn it to a memory object,
and pass it in,
and possibly it is used.

it's nice that bytes are immutable :)

hmm that's not good though, hackers, that's not good

what if i need rw memory out
like what if it's an in-out parameter

and what about ownership?
do i lose ownership of the bytes once I pass them in?
that can't be right

oh i think i should just build any arguments in memory allocated
by the allocator

hmm but then i have read/write methods on remote memory, not good

so two alternatives:
- build directly in memory allocated by MemoryAllocator.
  This requires read/write methods on the remote memory.
- build in local bytes, allocate memory and copy to remote task if necessary
  This is inefficient in the local case where we might be able to build directly in memory.
  Hm.

so I think we should have

some kind of allocate and fill thing

where the memory allocater takes a bytes
and returns memory containing that bytes

and ownership of the bytes is lost,
since you see, bytes are immutable

and it has a with-context manager

and it takes a writable=True parameter,
so that if the memory is read-only

then the bytes can be used

and if the memory is writable,
then a copy is fine.

also a bytebuffer would be maybe okay

but maybe not,
who knows whatever

essentially split out all the logic into individual classes

have only the primitive interface centrally
seems good!

hmm, direct access to the channel would be better though

but sharing a TCP connection is hard - how can we ensure that previous users have cleared the buffer?
and it's too expensive to create a new connection on each usage, probably
a TCP handshake on each syscall is probably too expensive

although
maybe we can just
allocate a TCP connection for each object?

we are probably making a TCP connection for each file we read...
and a cat process...

so we could easily afford one for each object that wants to do syscalls
but I don't think so, no, that's a bad idea
we can, instead, just share the connection! abstracted slightly
yeah, it's fine to just abstract it.

in any case, explicitly modelling memory is definitely a good idea:
at the moment, if we wanted to proxy between two file descriptors in the same task,
and we didn't want to use splice for whatever reason,
all the data would have to hop through the hub.
gross!
we should be able to read into remote memory and write that remote memory back out,
without pulling the data into the hub.
hub is a bad word, I should say "runtime" maybe, or "language"

ok so we have a simple SyscallInterface that just has,
the method,
"syscall"
* rsplice
  I guess we can grant the ability to,
  for any "bytes",
  get a Pointer and Task and stuff for that bytes,
  in the local address space.

  and we'll have a gateway class thing

  which is the gateway between here and there

  and supports splicing things

  and it's an abstract interface

  and the awkwardness of this interface will discourage working with data

  what to call it?

  gateway?
  rsplicer?

  HomePortal

  rdma...?

  Gateway.

  And it'll be in standardtask?

  Or in task?

  but it certainly requires async processing stuff...

  how can we put it in Task?

  we can't.

  oh but we must?

  no, we only need to,
  if we want to have nice syscall wrappers.

  memory gateway, hm.

  memory is just an actor that we send messages to, y'know

  baking it into task is really necessary tho right

  well no, it's technically possible to get along with just syscall

  we could bootstrap our way up.

  hmmmmmmm MMMMm

  if I don't put it into Task,
  I'll need StandardTask for pretty much everything

  So yeah, stick it in task.

  maybe merge it with syscallinterface
  maybe not

  ok so now I need to think about how to handle this split thing
  where two things want to write to memory,
  but the write isn't needed until the latter is done?
* pipelining/mobile code
** muse 1
mobile code hmm

i mean, for any specific function,
we can always stick a function into the library to handle it.
** muse 2
  i'm tempted to bring this all to its ultimate conclusion
  by just sending over snippets of assembly to execute

  they can have the syscall args embedded into them
  and do "syscall"...

  then we'll return back, what?
  the register state?
  eax?
  nothing? force it to do its own return?

  meh we'll stick to just syscalls for now,
  it's nicely constrained to force heavy use of Linux stuff
** muse 3
this explicit modeling of all these things
like memory and such
makes mobile code pretty viable y'know

unusually viable

that's a later step if we choose to do it

we can easily reimplement syscallinterface and gateway on top
** tail calling
   we could even have the mobile-code-receiver be entirely tail-called

   as in, it has no main loop,
   we're just require to send over the right mobile code,
   which has a tail section which calls wait_for_more_code

   we can even be stackless
** explicitly managing resources
explicitly managing resources on the remote side is so much better

instead of sending a message which causes some mysterious amount of resources to be used,
instead just have some code which you control which runs on the remote side,
and which you communicate with using some internal protocol,
and if peple want to talk with you they go through that code, I guesso
* logging
maybe we should have a per-task logger?

or better yet: a per syscall-interface logger?

then we call getlogger on it to get the right logger.

well what is it I really want?
I want to be able to know which task a syscall is coming from.

I don't actually care about customizing the level based on each one

I guess a logger per syscallinterface seems fine.
* small unix socket paths
  What would I do eternally?

  So I want to be able to connect to an arbitrary Unix socket path.

  hm.

  Why don't I just open the directory in the connector,
  and then connect to the path?

  That's not fully generic, but it comes close.

  Yeah okay so, the path is limited in size...
  but we can do it by directory...

  but then the final component still is a maximum of 108 bytes...

  which we can work around by... making a symlink, I guess?

  oh, hm, can we do this with an O_PATH socket?

  that would be neat... :)

  WOAH IT WORKS!

  ZOMG

  this is transformative

  so you can connect to a socket

  through an O_PATH descriptor

  this is great

  oh and it makes sense, since the /proc/self/fd entry is a symlink to the real socket.

  okay so I guess what we'll do is,
  we'll pass down the O_PATH descriptor?
  hmm, maybe not
  I think we should probably just pass down the path and have the user open it.

  on the other hand, hmmm..

  what happens if I O_PATH open something under a dirfd

  it works! omg!

  so this handles the connecting side.

  but now what about the listening side? does this let us listen on sockets with components more than 108 characters?
  
  in other words what happens when I listen on a symlink?

  i guess we can find out but

  i am not sure this whole thing actually works in different filesystem namespaces!!

  in fact I think it's likely that it doesn't...

  oho of course we can't bind through a symlink

  we can't O_PATH open something that doesn't exist yet!

  we can use mknod to create a socket, I guess.
  then the socket exists, but can we bind to it?

  wait wait, can we use a symlink to do the binding?

  and how does this even look when we call getsockname?
  will we get a huge (>108 bytes) address back and have to find size by strlen???

  what??? I guess we get the dirfd-relative path when we call sockname or peername?
  even on the remote side??

  well no on the OH hm

  what???
  we do???

  this is confusing...

  okay so let's try binding on a symlink

  ohhh it fails because the symlink already exists!

  hmmm

  what if I bind the socket then rename it?

  yes that seems wise.

  okay!
  so for connecting:
  open O_PATH the socket then connect to /proc/self/fd/pathfdnum

  for binding/listening:
  open (O_PATH or O_DIRECTORY) the directory you want to bind in,
  then bind on /proc/self/fd/pathfdnum/whatever,
  then rename whatever to the real name.

  Well, here's one issue:
  What if whatever is taken as a name?

  I guess we are just generating a temp name.
  Hmm, tricky.

  Also we only do this if the name is long I guess?

  Otherwise we bind directly.

  so yeah I guess getpeername is just whatever the original name was.
  lol

  so, at least I've discovered this!
* stuff

  write a shell with rsyscall

  some other thing what was it
  some explanation of what rsyscall is

  meh
  it was nothing original

  it was I guess the notion of a language based system?
  
* another advantage
  Don't hardcode position of fds in fd space!

  That's bad!

  It prevents you from using other things such as a debugger!

* unshared memory
  ok so I guess the first step is to do this with a thread

  rather than execing into it

  hmm or should we actually exec into a separate process?

  after all what's the point in forking really...

  it's dumb!

  copying your address space? pointless!

  primarily because all the other threads, and the runtime, don't get cleaned up...

  so yeah let's go with execing

  alternatively, that's a bit of a headache, so let's not

  okay yeah so we have to pass down the stuff

  ah! I need to figure out how to avoid blocking the rest of the program

  I guess having a single linear Task,
  which I pass in and pass out whenever I'm gonna block,
  is the way to do it ideally.

  And what abstraction would we build around this?

  I guess we'd have some thing that we can ask to borrow the task,
  so we do the blocking internally?

  It's better for that blocking to happen on the actual task.
  i.e. we should be blocking on a response coming back;
  the requests should all be pipelined

  hmm.

  if we have two epollers both with the activity_fd subscribed, we'll get into a spinning situation.

  the first one will wait, then the second will wait, which will wake up the first one,
  and the first one will send a new wait request through,
  which will wake up the second,
  and the first will send a new wait request through,
  and we'll just be spinning! ugly.

  okay so how do we handle avoiding the spinning with epollers?
  wait, hm.
  what if it's edge-triggered?
  suppose it's edge-triggered.

  oh wait, but.

  we can't have a loop in epoll.
  we can't have an epollfd which is our "do we have stuff to do",
  which points to another epoll which is their "do we have stuff to do",
  which points back at us.

  could we do that nicely if, hum.
  if we, um.
  if we were edge-triggered?

  hmm, loops are possible, but loops are always unnatural I think.

  so we want something that references "the rest of the world".

  and we want that something to also reference us? nonsense!

  we are the only thing that is!

  so when we're us, we are, um, hum herm.

  I should read more composable concurrency stuff.

  ughhh that's a drag

  ok so if i don't read more concurrency stuff.

  how would I just do this in Unix?

  I have a good ole fashioned C library.

  It performs some function and exposes an fd.

  I include that fd in my event loop.

  This way I pump its work loop whenever it needs something.
  So it can maintain its service.

  What if there's some function that completes asynchronously?

  well I guess I can just,
  pass down a,

  callback thing.

  yes and mhm

  I pass down a callback

  and eventually it gets called

  and okay I guess it um

  it just y'know

  oh I pass down a callback,

  as well as a rest of the program thing to do immediately,
  and return up and continue running,

  and eventually I pump its work loop with an epoll

  and, yeah.

  but the question is.

  what if I'm a-wanting to do some things in the function?

  and...

  okay so...


  I want to do an operation that is atomic, aka a syscall.

  It's a blocking function.

  How do I do that?

  I just don't, I guess.

  I have some epollfd and I just add to that and assume someone will call it.

  So yeah okay so that's the thing.

  Someone can't pass in to me, and have me block for them.

  Okay I was thinking about this before yeah

  Either we take in an activity fd,
  and we block on our own events plus that,
  or we take in a blocking fd to add our events to,
  and we just return.

  Um, that's weird.

  So either we take an object on which we can register events and supply callbacks,
  or we take in the event representing the rest of the program and do our own blocking.

  Oh so no I guess a CML way would be,
  we bundle together our own events into a single event and return it.
  Yeah, yeah.

  So either we return a single event representing us,
  or we take in an event representing the rest of the program.

  bah let's just have it work by accident.
** proglangdesign question
  Hey #proglangdesign

  So, consider a program that does only minor computation,
  and spends all its time blocked, waiting for any of a bunch of things to happen.

  There are lots of languages and idioms where (put maximally vaguely) if there's such a program A, and a component B of that program,
  and A calls into B, and B wants to perform some blocking operation,
  then B (at some level of abstraction) will send to the rest of the program A some event thing that A can run/monitor.

  For example, B might return a future, or might register a pair of (event description, callback) and return nothing,
  or might be a coroutine that yields up some event description to some central event loop handler thingy,
  or more esoteric things.

  Essentially, B reifies the blocking operation and the subsequent operations it wants to do,
  and gives that to A to run at A's leisure.

  But what about the inverse operation?
  If B wants to perform some blocking operation and doesn't want to block the rest of the program,
  why doesn't *A* reify all the blocking operations that A wants to do,
  and send it in to B?

  Essentially, why can't we reify the blocking operation and subsequent operations that A wants to do,
  and give that to B to run at B's leisure?
** monads
   kinda like monads I guess

   or I guess like objects sending messages? no...

   I mean, ultimately I want to avoid having to do the conventional thing,
   (but which I've recently convinced myself is weird)
   where I'm in some deep library function and I make a call up to do blocking.
   I think that deeply requires weird stateful stuff.

   Instead I'd rather be able to block directly, and wherever I am, always have the rest of the program be calling down into me to do blocking.
* memory again
  OK so I used the memorygateway agressively.

  Now let's evaluate whether it's necessary :)

  So if I just had a "pipe" which allowed me to move memory from one place to another.
  That would be fine and good wouldn't it?

  talking about memory as memory allows me to, um
  well I can transparently use RDMA stuff y'know

  if I specify,
  hey here's the two sides.
  here's the memcpy.

  well yeah the pipe is really essentially this gateway thing.

  should it be unidirectional?

  yes. wait no.

  HMM I think we want read and write and all those things to not be memory-abstracted.

  That way we can easily do PeerMemoryGateway.

  I guess read can just be a free helper function.

  Yes and it can take a memory allocator thing and allocate some memory
  and write it into there.

  I guess. But then what about easily, y'know, reading from a file descriptor and getting those bytes into the runtime?

  Inspecting those bytes is tricky yeah.

  But I need the ability to read and write arbitrary bytes to even do the basics of AsyncFD stuff.

  Y'know I feel like I should probably extract some of those fundamental components into helper libraries/threads/things.

  Though maybe not, I should prove the concept of doing it all through syscalls.

  Keep both implementations and test equivalence?

  Anyway, so I allocate memory and copy into it with a helper.
  Seems fine and good.

  Then we need some help to get it into the runtime.

  I guess that's really more of a, hm.

  We have some kind of connection from the host, to the runtime.
  And, yeah.

  Well, I guess we maybe could have a big Runtime object,
  which has connections to all the hosts?

  Hmm no I guess we need the connection to the individual host.

  And we'll do a read() method on an individual file descriptor.

  Yes that makes sense.

  That is to say, that kind of reading is in the io level,
  not in the base level?

  How will Path work then?

  Path will rely on this connection, I guess, since we maintain the data in the runtime.

  Or will it?

  I guess we could go all the way, and have Path be only dirfds.

  But that's just as slow as maintaining the path pointer remotely.

  So if we maintain this data in the runtime.

  And send it over on demand.

  Then, that's all good.


  HMMM wait a second so, hm.
  Ah yeah so our Path class is correct, right, it correctly operates on Paths.
  And we could indeed replace it with a dirfd-only thing.

  I guess operations would still require memory access to actually be performed.

  So we are not really improving anything if we want to allow easily accessing arbitrary filenames.

  So Path, because it needs to work on arbitrary filenames which are stored in the runtime, needs to absolutely rely on the memory gateway.

  Well.

  We could have the dirfd thing,
  and have it operate on pointers.

  We could, I guess, have dirfds, as well as cwd and root things.
  And have them indeed operate on pointers.

  I suppose I could slide this API in under the current API.

  Should this be the same as base or separate from base?

  I think separate. We can combine later if necessary.

  Basically, a memory-access-free task and abstraction layer.

  ya okay I guess
* object capability design question
  It is a central precept of capabilities to not separate naming from authority.

  But, what about a situation where I can precisely name an object,
  but there are multiple possible (equivalent in power) authorities that I could use to access it?

  Hmm okay so they're suggesting just only exposing io.Pointer and supporting an equality between io.Pointers to see that they are the same.

  We could, I guess, only carry around the SyscallInterface along with the io.Pointer.
  But wait, we do need to be able to do the dynamic check on the task to check its authority.

  Well, hmm, I guess we can couple the io.Pointer with a MemoryGateway actually, not a task.
  Well, a MemoryGateway wraps a Task.

  An FD is the thing that we'll couple with a Task.
  And the Task I guess has all the namespaces on it?
* epoll notes
    # first, let's have them both be externally driven, I suppose
    # no wait, what are we going to do testwise?
    # so we have them each registered as epoll on the other one.
    # we'll call wait on the pipe_rfd on both of them.
    # then we'll activate some... stuff...
    # how do we call wait on the pipe_rfd on both of them, if wait blocks??
    # we can't, I guess.
    # before blocking, I guess we need to ensure that no other tasks are ready to run.
    # that's tricky, so let's go with external blocking for now.
    # okay so this is hard, very hard, we will probably not be able to run things until everything is blocked, so...
    # so...
    # how exactly do we ensure that everything that can run, has run?
    # well it's a matter of doing all pending work that we control
    # and only then blocking
    # blocking surrenders control back elsewhere...
    # but if we run a function for someone else...
    # and they call back into us...
    # well actually that only will happen if we have a level-triggered approach.
    # also... don't we need to do something level triggered then?
    # I guess we'll, um...
    # when we do the call into some other guy, they'll do just the, um...
    # just the nonblocking approach.
    # they won't block.
    # we'll block.
    # but no we won't block either.
    # well then when will we block?
    # okay so but yeah.
    # when we do an external blocking for some other guy,
    # that is, internal blocking from our perspective,
    # external blocking for them,
    # then,
    # they need to not block.
    # so yeah...
    # but then alternatively, we can say, let's externalize our blocking to this other guy,
    # and they'll call us when we're good.
    # I mean...
    # proposal is to do blocking internally, when exactly???
    # like, I guess we really do have two modes?
    # one external-blocking always, one internal-blocking always?
    # an internal-blocking guy can monitor for an external-blocking guy...
    # but here's the issue, how can we handle doing internal-blocking in any of our stuff??
    # after all, when we do an internal-block,
    # well, we can't be sure that other events aren't going to appear
    # well, except when we can in fact be sure?????
    # like when we are the only thing in the world,
    # we call our thing,
    # and any new action has to activate us?
    # blaaaaaaaah!

* segmentation
  lol this is a lot memory segmentation stuff
  
  basically I have far pointers in terms of memory segmentation

  well, there's an additional thing beyond memory segmentation:
  I have to perform the access through a specific resource.

  also the segments are not just numbers, you have to use an object to set your segment to something.

  model segment registers with dynamic scope
  I think that would be a really cool mapping

  so then how do you ensure your accesses are correct?

  not sure

  but you could also manage the current, um
  the current filesystem namespace with it, yeah

  yeah, you'd just have, um, setting that dynamic scope as,
  an expensive operation, I guess,
  but nevertheless an operation,
  and you basically can assign the current namespace to a thing.

  so, yeah.

  but now I just need to understand how dynamic scope can be associated with an object.

  I took notes about this before.

  Well, wait, reverting back to the previous state is expensive too.
  It requires you to save a thing.

  So... maybe we'd just want to be mutable.
  Urgh. So okay, let's just be mutable.

  We have the Task, and it has some mutable attributes.
  And that's it!

  OK so FarFD is fine.

  The fact that threads are first class objects is weird.
  But cool.

  Let's do the Task thing now.

  meh there's still the weirdness of...

  given a far pointer, I still need an additional thing to access it. (a thread)

  I wonder if there's a "physical" (hardware) metaphor for that too?

  afaik, far pointer systems never had to allocate and identify the,
  like, bus lane that they were accessing memory through :)
  
  that would be cool though.
  would that correspond to what I'm talking about?

  maybe:
  if the segment physically exists,
  and we set up a bus lane so that it's actually pointing at that segment and pointing at me,

  then that's kinda the same?

  we still want the far pointers on our side.

  hmmm.

  oh, the fs and gs!

  effectively you just,
  you sent up all the segments right,
  and then you use them!

  hmm what if we don't have far pointers, we just have pointers used with a specific task?

  that would be less realistic, true

  but wait, we could support join on them to get equality.

  hmmmmmmmmmmm

  that's strictly less functional though...
  because it doesn't invalidate pointers used before.

  paths used before are not invalidated,
  nor are other things.

  but maybe that's good?
  no that is not good, the paths are not in general valid after a change.

  this is kind of the issue with moving a thread between address spaces,
  because the instruction pointer might need to be invalidated.

  and, yeah... the fact that the we have some specific channels we can do instructions through,
  and they are each mutable with respect to which segment they use...

  it's a lot like us...

  which is useful, since I could, say, set up FS and GS at the start of some block to point to specific segments,
  and then use far pointers within that block by just passing the correct segment override prefix.
  but because this is stateful, how do I make sure that I don't mix this up,
  and use the wrong segment override prefix with the wrong far pointer?
  or what if I have some user-provided far pointer and I want to use it,
  and the user *should* be providing a far pointer whose segment matchesr

  it seems like the best I can do is a dynamic check at the time that I use so-and-so far pointer,

  how do I make sure that I use the segment override prefix with the right addresses?

  so were there ever any idioms for managing those?
  
  also! what about idioms for managing the current state of the extra segment registers?
  of course SS and CS are constant most of the time, but {E,F,G}S could, if I understand right,
  be arbitrarily pointed at new things

  which is useful, since if, say, the user provides a bunch of far pointers in the same to me,
  and I want to do a bunch of operations on them,
  I can just set up FS correctly at the start and just use the segment override prefix.
  but how do I make sure that all those far pointers are in the same segment?
  seems like I need to do a dynamic check.

  but, hm.
  if they provided me a bunch of near pointers,
  and a segment.

  that would be type-safe I guess...

  well, what would that correspond to?

  i guess, they provide me with a bunch of raw addresses,
  and a syscallinterface.

  that would be type-safe.

  so that's actually more like,
  they provide me a bunch of near pointers and a segment override prefix.

  so yeah, what if the user provides a far pointer to me,
  and helpfully they claim they've already set up one of the segment registers to match that segment,
  which is good because they're slow to change.
  so how do I validate that?

  well they should provide near pointers to me I guess.

  what if we bundle a far pointer and a segment register override prefix?

  so imagine that we have more segments than we have registers

  and, we want to pass around far pointers and the segment register that's been set up with the matching segment id.

  so that we don't have to allocate segments.
  

  so we clearly need to have an interface that is,

  f(segment register, near pointer)

  then probably we have something that's:

  f(Bundle(segment register, far pointer))

  which does the type check...

  so if it's much more expensive to change a segment register than it is to do a dynamic check...
  maybe this works?
  maybe this makes sense?

  we do a join across all the segment registers to make sure they're the same...
  well, across all bundles to make sure they've got the same segment register, I guess...
  well, no, we extract the segment register and check the thing,
  check that they are all the same.
  and we also reduce the bundle to a near pointer.

  no so, that's not maximally exploiting the efficiency.
  the segment register doesn't need to be the same across all of them;
  it just needs to... um...
  it just needs to point to the same segment?

  
  no, this is silliness...

  hmm..

  okay so we have this pointer type which is a packed pointer,
  which is the segment register plus a far pointer.

  we pass around the packed pointer to places.

  what are the methods on the packed pointer?

  I guesse they are,
  "gimme some far pointers and I'll do these things"?

  or packed pointers?

  or near pointers?

  so f(segment register, far pointer) doesn't seem that bad *if* it does the type check, right

  f(segment register, far pointer, far pointer, far pointer)

  and have it do the check that segments are correct.

  so then we have a bundle like,
  Bundle(segment register, far pointer),
  which has methods which take... what?

  Bundles or far pointers?

  Well, if it's far pointers, it's cheap, innit.
  We just delegate on down to the function.

  If it's bundles... isn't it also cheap?
  We also just delegate down?

  I guess we just don't even bother checking the segment registers of the others.

  segment registers are required to actually access things.
  near pointer combined with segment register is what's necessary

  okay so i've got objects which are near pointers and objects which are segment registers,
  and then i've got a function (near pointer, segment register) -> stuff,
  which is how we ultimately perform an access to a near pointer, we run an instruction
  with a segment register override prefix to specify which segment register to use,
  and pass a near pointer in a general purpose register as an argument to the instruction.

  and then i've got an object containing a near pointer and a segment id, aka a far pointer object.
  and then i've got a function (far pointer, segment register) -> stuff,
  which is where I specify a segment register to use for accessing this far pointer,
  and we do a dynamic check to check that the segment register is set to the correct segment id.
  this is nice and fast, if changing the segment register is slower than checking that it's correct.

  so what would I call an object containing a far pointer and a segment register?

  the existence of this object asserts that we can correctly access this pointer,
  and throws an exception if we try to use the far pointer and segment register together and it breaks.

  near pointer

  far pointer

  a segment is a place

  a pointer is a location inside the place

  a segment id is a place identifier

  a segment register then is a route to the place?
  or like, a channel you can follow to get to the place?

  how does that relate to a notion of source routes as far pointers...

  if a source route is a far pointer

  then what's the segment register?
  the network interface????

  naw...
  maybe a segment register is just an even more far pointer

  so hmm

  each segment register could theoretically have different segment ids?

  no i guess segment ids are global

  from our perspective it's truly uniquely defined

  i guess it's hierarchical

  but a representation as a graph is better.

  i guess putting a segment register on it is just like one step further on the graph.

  but we need to check that that element in the graph is correct?
  and, hey, wait, we don't walk through the namespace really!

  although, do we actually?
  we walk from the task to the far pointer,
  but that fails if they differ.

  I guess it really is just a more far far pointer.

  distant pointer?

  what's the word for more far,
  but also more accessible?

  far implies you have to go through some additional absolute thingy to get to it.
  or, I guess, you have to go from some larger context.

  a wider view on the world

  astral pointer?

  so we really do need this thing

  but then I guess it's wrapped in a thing which also carries a memory gateway?

  and translates down to the active fd?

  how do we do that as a free function?

  I guess we just have the memory gateway
  and allocator
  and far pointer

  I guess we could have them be methods on active FD after all

  No that's a bad idea

  We'll have the allocators, um...
  return near pointers??

  the only thing we really need is to have the io.Task inherit from or compose from the far.Task.
  which is easy! we can even import the near and far stuff in the base stuff

* getting it to work
** blocking on local thread
   reading/writing is blocking us indefinitely when it's on the local thread.
   hm.
   I guess putting it on a different thread would be fine.
   but, argh, running the syscall connection on a different thread is weird
   can't we be async about this?
   let's lay it out

   we want local reads to go through an Epoller,
   which requires reading pointers to get the result.

   remote reads can't go through an epoller because we're implementing the memory-reading functionality here.

   I guess one side can be async and the other not async?

   that would make sense.

   yeah and running on a separate thread isn't really helpful anyway,
   it'll still block others.

   OK! one async, one direct.
   That makes sense, right?
   There's a "local" part of the gateway,
   whose memory we can already access;
   then there's a "remote" part,
   whose memory we cannot access.

   we need to support a pointer for async fd things.

   I guess we should take a lock when getting the near pointer.
** blocking forever on wait, I guess
   okay so why is this happening?

   aha the futex is hanging forever

   i bet because the address is not properly shared?

   maybe

   hmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm maybe we could/should invoke it after creating the thing?

   that's a little weird

   oh my globbing glob glob

   ugh okay i'll look at the memory maps

   bah! it seems to be the same file mapped!!
   the inode is the same!!

   okay what if i just set set_tid_address after launching?

   what the heck aaaaaaa

   okay so when we're unsharing memory,
   we only need to do this if we start another thread in the same address space,
   because then the stack will stay around

   what does that even mean, how do we even represent that

   bah argh blairhlagh

   okay so, yeah, so launching a futex thread for this is useless when it's in a different address space, right?

   like, we wouldn't do that for a standalone exec

   would we?

   no, it would be useless.

   the address space would close and...

   so yeah let's just make a task that doesn't have this futex thing.

   also it might make sense to have a near pointer and far pointer for memory mappings.

   that way we can refer to ones in other address spaces

   hmmmm we need to support threads setting up the stack right

   wait actually it's quite different because we immediately free the stack
** start up rsyscall executable
   so it's working in thread

   now for execing a separate process

   so I need to write a helper

   I think the helper for remote systems will be different from the local helper.

   probably maybe statically linked so I can get into it immediately?

   or should I use Nix for it?

   statically linked is best I think. it's a small binary anyway.

   and the interface with two fds specified is fine

   so hm hm hm
** helper C function for exec
   should I make a helper function that just does exec?
   that's all the system call does?

   it would need to take arguments at a static location if it wasn't to use a stack

   or just use a stack I guess.

   yeah that's best

   derp

   we can specify the stack and the address and all that stuff

   we can use a pre-existing stack memory so we don't have to allocate

   then it would be good
** okay so we do need the futex
   and it does seem to work at home so let's move on

   (possibly a kernel bug? luckily my home kernel is more recent I think so it should be fixed)

   so we just need to figure out a nice api for detecing the finishing of an exec or exit

   it needs to incoporate both waiting for a message on the fd,
   and also waiting for a child event.

   And when I get either I cancel the other

   And when I cancel a system call, I close the interface;
   that's the cleanest way to work,
   since otherwise I have weird races.

   OK yeah seems really clean

   and, to exec, we need to have one of these monitor things

   or I could just even close the the interface rather than cancel.

   oh nooooo aaaa okay I see

   so if I want to just pass the sysif down... without closing the fds...

   then I need to treat this somewhat specially.

   ok so, really, closing the interface is not going to help.

   more specifically, I still need to cancel on thingy.

   Then the close interface shouldn't actually close stuff?

   Hmm tricky

   So we definitely, probably, maybe,
   need to make a new syscall interface after exec.

   even if it's the same...

   bah maybe I should do all the stuff twice

   no hmm hmm I think I can do it externally I guess

   if I can just harvest the fds from the syscall interface.

   hmm

   so what do I need

   the infd and outfd need to stay around

   I probably need to make a new gateway???
   but if it's already a different address space it'll stay with that gateway?

   but what if that address space is already shared with others?
   well.

   we could be on a shared gateway

   yes hmm

   meh, let's just use additional file descriptors for now.

   so wait, um

   so we're talking about that because,
   achieving a clean execve requires cancelling the syscall
   and cancelling the syscall requires or suggests closing the interface so it can't be reused
   we don't *have* to do that though

   what will we pass down?
   nothing perhaps, we can do the cancel outside.

   it doesn't make *that* much sense to have the Thread be the thing
   after all, we can make an MMReleaseMonitor after the fact,
   in a separate process.

   hmm so getting notified of when the exec is successful is fairly important for,
   a scenario where,
   you spawn a process using a thread?
   and are passing resources through your address space and file descriptors and stuff?
   I guess maybe the need for that notification is why people don't do that.
** blocking memcpy
   OK so it looks like trio is not even monitoring our epoll fd.

   Somehow it's blocked.

   Oops it was my fault, I cancelled the thing
** TODO make sure cancellation is all good
** ok
   so things are working.

   what's this division between things with an exec monitoring thing,
   and things without?

   things either have an "active tid address" object or an "inactive tid address" object,
   and they move from one to the other.

   otherwise we'd not be able to track erroneous creation of one invalidating the other.

   how do we compare to make sure it's valid?

   I guess that's a thing inside the Task?

   a tid address thingy?

   how would I handle this with my ideal API, something coming through wait?

   well I'd need to turn it on again after each fork I guess?
   or maybe it'd be inherited?

   in any case I'd store the current state of it.
   no i'd just force it on..
   no wait, it would break my grandchildren so I'd need to turn it on explicitly.

   so I have to activate it for each new thread
   I activate it after the fact, after spawning.

   Then I can only exec in threads with it on?

   Would it be inherited over exec?
   No I guess not.

   so I have two things:
   a monitoring for exec complete thing,
   and a flag-of-whether-that's-enabled thing
** TODO yikes i really hacked this together
** just do ssh too
   okay so I probably need to do this by socket forwarding?

   that is distasteful because it places requirements on the remote side

   hmm using an ssh library directly would be good

   no let's do it with openssh command line
** openssh unix socket forwarding bootstrap
   1. Run bootstrap command on remote host using ssh.
   2. Bootstrap command prints out environment,
      and binds a Unix socket at some path and prints the path and file descriptor number.
   3. Run a second ssh command forwarding a local unix socket to the remote unix socket path.
   4. Connect twice to the local Unix socket.
   5. Bootstrap calls accept twice on its bound Unix socket,
      and prints out the fd numbers,
      and execs rsyscall binary with those fd numbers in arguments.
   6. rsyscall binary prints out relevant C library function addresses,
      then calls rsyscall main loop function

   We don't specify ControlMaster; if the user wants connection sharing, they should configure it!

   Need to start with the rsyscall binary thing
   and starting threads in there.

   so the binary needs a stdout passed down
   to which it will write various addresses
** rsyscall binary thing
   OK so I did that

   But to test, I want to run a thread inside it:
   which requires multi-hop memory copying, I think.

   Or some kind of scheme or awareness or something.
** multi-hop memory handling
   Let's figure this out.

   This is also in general the problem of data at more that one remove;
   a datafd that isn't direct. hm.

   I think we should try to support direct stuff,
   and just not support indirect stuff for the moment.

   I guess that's 0 or 1 hops are allowed, in a sense, hm.
** efficient socketpipe
   We want to be opening direct connections for all our things.

   Actually doesn't this include, like, pipes to subprocesses too?

   That should be good, very clean and nice.

   We'll explicitly deal with the socketpair and socket passing.

   We'll have some kind of class which contains the connection-maker (abstract),
   and an optional Unix socketpair (concrete).

   It will have a helper method to make the connection and pass it over the socketpair if necessary.

   OK so I guess what is really going on is,
   I have a local fd space?
   And the helper gives me something where one end is in that space?

   Weird.
   No that's not right

   I have a task,
   and the helper gives me something where one end is in that task?

   One end is in the local address space, that's clear.

   But an fd is not in any address space!

   It's a matter of task:
   The fd is in an fd space such that there's at least one task in the local address space.

   I guess that's a bit weird so I guess we'll return an active fd.

   We return an fd, plus a task to use it, where the task is in the local address space.

   This means we can use local pointers.

   And the other fd we return, is just a far FD, in some fd space, who cares where.

   And we pass that around to get it locally???

   No, I think we return a far FD in some concrete known fd space.
   And then we can pass it around to get it to the fd space we want it to be in.

   so... LocalSocketpair?
   EfficientSocketpair?

   Bridge?
   BridgeMaker?

   Wormhole?

   Tuber returning a Tube?

   BridgeMaker
   Bridge
   DirectBridgeMaker

   Connection

   ConnectionMaker

   Connection

   We need to do this because,
   we don't know up front how many children we want to make:
   we want to do that direct style.

   Which means we need to make new connections to children,
   which, because we want those connections to be efficient,
   means we need to make those new connections directly.
   And in the worst case that means passing fds around.

   But I don't think it's really that crazy.
* socket passing
  So CMSG structures are aligned to sizeof(size_t).

  Interestingly I guess this is true even for struct cmsghdr and len independently?

  I guess yeah so things are aligned like that.

  So I guess I'll support CMSG_LEN stuff

  Oh I can just use CMSG_LEN from the standard library, that's convenient.

  So I support I can use this array thingy.

  I guess I just build the struct???

  I'll use it with cffi I guess

  Yeah I'll define the structure in cffi.
  that'll check compatibility nicely

  same with msghdr
* garbage collection
  I think I will just neglect freeing things from now on.

  I could achieve a GC'd by writing to a queue in __del__ and monitoring that queue from some closing thread.

  Oh but I guess I need to flush that queue when closing the task or changing the task's namespace.

  Oh and I guess I'll have to use active fds and pointers to do ownership.
  They're the ones that truly own things.

  wait but if I change the task, then I can no longer close it.
  very tricky

  I should support both explicit context managers and __del__ based freeing.

  And, I suppose, explicit closing. but maybe also this subsumes "release"?
  hmm if i support context managers, I need release.
  well, only if I support them in a way that is actually usable :) :) :) :)
* great post
http://www.evanjones.ca/software/threading-linus-msg.html

This shows some of the core idea.
* user namespaces
very important caveat: they don't nest!
* secure usage of rsyscall when you setuid to untrusted users
  prctl(PR_SET_DUMPABLE, SUID_DUMP_DISABLE);
* DONE look into using msg_dontwait for pipes
  Maybe we don't need to use O_NONBLOCK at all.

  Nope, doesn't work.
* plan
need to make a nice API for passing an fd, I guess.
that's the first step?
well, I guess the first step is tasks started by level zero.
then a nice api for fd passing, then tasks started by level one.
then tasks started by level two and on forward.
then, tasks in a different subprocess
meh let's do the fd passing API first

so, let's just stick to using rsyscall_spawn_exec - i.e., operating in a separate process - for everything.

shifting back to a single process should be an easy optimization after that!
** passing
   Three tasks are involved in every child start:

   The access task, through which we access the syscall and data fds.
   The parent task, which will actually clone off the child.
   And the connecting task,
   which has the ability to both create file descriptors to communicate with the access task,
   and transfer file descriptors to the parent task.

   If the connecting task is the same as the access task,
   then we just use socketpair.

   If the connecting task is the same as the parent task,
   then we don't have to transfer file descriptors.

   The capability to create the gateway and to transfer file descriptors,
   are both optional arguments,
   which must be supplied if needed.

   The transferring file descriptor capability is simply a socketpair,
   one end of which is in the connecting task,
   and one end which is in the parent task.

   The bridge-creation capability is... hmm.

   Well, let's keep it reified for now.
   It's a (Unix socket address, listening Unix socket) pair,
   one end of which is in the connecting task,
   and one end which is in the access task.
   
   (I guess it can be abstracted to just a generic address)

   Let's begin with all three the same.
** ok neat
   so we got subprocesses working.

   now we need to get a different access task working.

   the way we do that is, hm.

   do we even need this connecting task thing?

   oh, yes we do, because otherwise we're forced into using a filesystem unx socket thingy, yuck

   So there will be an optional access_connection.

   Which is, umm...

   A listening socket on the connecting task side,
   and an address on the access task side.

   And we make the task and we're good.

   We abstract over address family,
   but hardcode sock_stream.
** ok now ssh
   we got the unix socket now ssh

   so what if we have a separate bootstrap for the unix socket,
   such that we can actually print the path?

   erm hrm part of the issue here is wanting to support multiple terminal applications.
   multiplexing is a bit tricky

   we really want each process under the control of rsyscall,
   not spawned by ssh,
   that's silly.

   so we really want to just pass the terminal fds in to rsyscall,
   and then spawn something using them.
** new plan
   OK so what we'll do instead is:

   1. Run socket_binder on remote host using ssh.
   2. socket_binder prints out the path of two Unix sockets to stdout.
   3. Run a second ssh command forwarding a local Unix socket to the first unix socket path,
      and also running the rsyscall bootstrap.
   4. The rsyscall bootstrap connects to the second unix socket path,
      and receives the listening socket for the first socket via fd-passing.
   5. The first ssh command exits.
   6. Connect four times to the local (forwarded) Unix socket.
   7. Bootstrap calls accept four times on its bound Unix socket,
      using the four fds as (bootstrap_describe, describe, syscall, data) pipes.
   8. Bootstrap prints out the fd numbers,
      and the environment,
      and anything else relevant to the bootstrap_describe pipe,
      then closes it.
   9. Bootstrap execs rsyscall binary with (describe, syscall, data) as arguments.
   10. rsyscall binary prints out relevant C library function addresses to the describe pipe,
       then closes it.
   11. Rsyscall binary calls the rsyscall main loop function.
** socket binder
   Ok we'll start with socket_binder I guess
** exec
   OK so to properly exec something,
   we always need to have the futex around.

   To have the futex around,
   we need, hm

   Well after calling exec, I think it disengages.
   So we need to reset it.

   Yes it disengages.

   So after exec, we definitely need to re-engage it.

   The weird thing is that I don't see any reason it should, umm,
   not fire on the clear_child_tid in a,
   non-CLONE_VM-created process.

   Weird.

   Seems like a possible bug.

   So anyway, we still need to re-engage the futex after an exec.
   So let's do that.

   We need to, hmm, share some memory.
   Possibly we pass down a memfd?

   That seems sensible.

   Man I sure have to do a lot of working around limits in the Linux API! Oh well.

   So I pass down a memfd,
   I map it into memory,
   I set_futex_address,
   I spawn a futex monitoring task elsewhere - likely in the connecting_task maybe.

   Hmm it could be a separate memfd for each task.

   But! We do need to remember the memfd!
   Because if we exec, we need to save it!
   Um, well, we are unlikely to exec into the syscall server again...

   So maybe instead we just, hmm...
   Make a memfd and inherit it right then and there,
   and mmap it and set_futex_address,
   and spawn a futex waiting task in addition to the main task.

   That's a bit inefficient to make a memfd each time, but it's fine.
** memory mapping object
   ok, should we have a memory mapping object?

   I guess we can take a memory mapping object and split it.
   By only mapping part of it.

   I guess I should indeed have a memory mapping object.

   HmmmmmmmmmmmMMMM

   Is it kinda like segments?

   Paging?

   I guess I represent paging explicitly?

   Yes that clearly should be done then.

   But how do I differentiate from pointers then?

   A pointer is something inside a specific range

   MemoryMapping contains a ptr???
*** huge pages issue
    so the length is different and rounded up, hmm.

    which means, we should only be sending in correct lengths.

    because we don't want to invoke that rounding, it's weird.

    so that means we need to know the page size.

    the page size is not *inherently* 4096.

    and offset and length must both be multiples of the page size.

    soooo maybe I should, hm.

    have an argument saying what we think the page size is?

    there are multiple sizes of pages...

    so we essentially say,

    "hey, given whatever the page size of this object is,
    map some number of pages for it."

    yeah, awkwardly we can get hugepages via MAP_HUGETLB,
    or we can get hugepages via mapping hugetlbfs

    either of them can determine the page size, and we have no idea!

    ideally I guess we'd, hmm.

    so an fd corresponds to some pages of memory

    so maybe munmap should take that fd?

    how else do we know the page size?

    alternatively the system call could take the page size,
    and assert that it's correct.

    argh or even better, the system call could not do any rounding.
    that would be the ideal.

    yeah so if I have an optional argument specifying the page size,
    and it just does a validation that the length and offset match the page size,
    then I can return a mapping and things are fine.

    ideally the underlying system call would do that validation but it doesn't.
    oh well!

    and we'll default the page size to 4096 because that's what everything uses.

    Oh and we need the page size in the mapping object.
* futex goofiness
  what the heck, I can't get futexes to work across processes! argh!

  or more precisely, I can't get clone_clear_childtid to clear the tid

  I guess

  ffs I can't get futex to work properly in separate address spaces

  FUCK my life

  atomic_read(&mm->mm_users) > 1)

  GOD DAMN IT

  OK so we're fucked

  How are we supposed to figure out whether the exec succeeds??

  this piece of shit...
  fuck...

  fuck!!!

  okay, what about vfork?

  I could use a robust futex????

  OK so, hm.

  So I could use pthreads?

  I lock a mutex at the start

  OH!
  The robust list actually is per-process!

  Hmm!

  If I give up completely on running arbitrary code...
  Then I can do it.

  aaaaa okay okay fine
** robust list
  so the only way to do this is by setting the robust list after clone or exec or whatever.

  Let's, I guess, do this in the futex page?

  ugh it's not working
** kernel hacking
   so I need to get a kernel hacking setup running again, ffs.

   I guess what I want is a Nix expression that builds a disk image and script,
   which I can pass a kernel image to.

   Ooh, I can do super Emacs integration awesomeness.

   OK so I can start qemu with -qmp pointing to a pipe of my control.

   Then I use that to learn whether it's nographic or not, figuring out the terminal.

   that may be excessive

   ok so basically,

   meh let's just simplify it down to just starting something that gives us the monitor,

   and we do the appropriate attaching on top of that.
   very simple helper function.


   or the more expansive vision would
   prompt you for a configuration.nix which you could edit on the fly,
   then build and start it in a VM

   I guess I'd want an emacs command that rebuilds, and hard reboots the VM.

   Then another command that runs a test in the VM?

   bah BAH

   okay so we need to mount it across, hmm

   and we'll start off by kernel debuggin' when we enter this process or something.

   that gives us a test we can run inside the kernel.

   or we can just printk in mm_release

   so yeah we need to run the thing inside the VM now.
** yay it works!
   everything is good once more!
* need to remember
** DONE recv/send
  gotta try using recv/send instead

  now for recv/send stuff!

  oh it doesn't work oh well

  at least that's resolved!
** using main epoll
  also the using the epollfd in different threads

  what we need then on an asyncfd is,

  the fd,
  an epoll in the same fd space as that fd so we can add/remove/modify the fd,
  an epoll in any space so we can wait on the fd.
** linking
  also another thought is,
  I'm kinda doing a distributed dynamic linking thing here.

  you know, if I went full-in on Nix,
  I could nicely not have to specify the path to the bootstrap.
  Instead I specify from the source host,
  exactly what I want on the remote host,
  and Nix satisfies it.

  of course this requires Nix to be on the PATH on the remote host,
  but that's more expected than the rsyscall bootstrap being there.

  I might go ahead and do that
* alright okay
  moving on, we need to get remoteness working

  for automated tests we'll probably have to set up our own SSH environment, y'know?

  which might be useful for at TS

  anyway remoteness

  we theoretically just need to run a bootstrapper task,
  which will be self-contained,
  and the parent of that task will be the access task,
  and the resulting task will be the connecting task.

  that task can't exec, right

  oh, right, my first step was to run the socket_binder

  On the local host

  And connect to it from Python and get the bound socket.

  Hmm no we will run it over ssh, and do the ssh forwarding, and all that.

  And then launch a process using that as our connection.

  Ah, we'll fake the bootstrapping.

  We'll just start a new child task which is local instead of remote.

  I want the minimal task required to start a child.

  Well.

  That's nothing.

  What I want really is the minimal task required to start a child *which can exec*.

  That needs a whole buncha things.
  I need child monitoring,
  I need thread trampoline...

  Well, if I don't monitor the child, there's no trouble.

  I still need the trampoline though.

  I also need a memory gateway and allocator, of course.

  Why do I ask for this?

  Well, exec_full needs to spawn a thread, so that it can exec into the rsyscall process.

  Hmm I need not just the trampoline but also the syscall function.

  Oh I don't actually need the memory gateway do I?
  Oh, I do because of the trampoline. Argh.

  Hmmm.
  Can I avoid that somehow?

  By making something specialized for rsyscall_server?
  So, I need to communicate the fds that it will use.

  If it's in the same fd namespace, those can't be hardcoded.

  OK so I need the memory gateway just to make a thread.

  But then, shouldn't I have everything I need?

  Yes, I do.

  But the resulting task does not have a memory gateway.

  When I start a task that's going to go across ssh,
  what's the relationship between access task, connecting task, parent task, new task?

  Usually the parent task is the access task.
  But if that's not the case,
  then we have to make a memory-gateway which connects two others.

  Let's just pass down StandardTask and call spawn, maybe?

  this task is implemented in userspace?

  the complexity of rsyscall_spawn is that it needs to work with remote processes?
  but bootstrapping is in userspace?
** ok it seems like it worked kinda
   I was able to spawn a child and exec and everything. Which is cool...

   So now the next step is adding actual ssh?
   And a bootstrap binary?

   Ah, hmm, no, we can do the real bootstrap binary first.

   Then we'll add ssh after that - ssh is gonna require building an ssh test env.
   Which will be the first real useful application - nginx wasn't actually motivated by anything.
** bootstrap binary
   So this is just a C program, I can write this.
   
   I wrote it.
* locations of things in the filesystem
  Do we really want to look up standard utilities in PATH?
  That seems lame.

  Should we bind ourselves to Nix?

  We can have FilesystemResources be an interface which Nix or other things can implement.

  For now, let's just find the locations of executables we depend on,
  at build time.
  That will work with Nix and also some other systems - namely, when it's installed system-wide.

  So we'll do this in ffibuilder?
** building standardtask
   ok, so, surprisingly, it worked without a nightmare of debugging.

   so, now we wrap in ssh, I guess?
  
   yes, we've done the bootstrap binary, so now we need to build an ssh test env.
* ssh environment
   this is the kinda thing that Nix should be able to do

   y'know it would be cool if we got a whole, local environment thing, set up

   your own self-contained userspace

   OK so we can use sshd -i, that's a good improvement.

   Is it? Well, it at least prevents using the network... well, whatever...

   Let's just run regular sshd, ummmmm wait no let's not. I hate hardcoded ports!

   So we'll spawn sshd -i from us, that's fine.

   This guy's legit engineering concerns about efficiency, are,
   well,
   we can get a global view from our python script no problem.
   Saving on startup work is trickier, but if ssh was designed differently,
   we could achieve it.
   by, like, pre-setting-up everything in our own thread,
   and forking it off each time,
   or something.

   So sshd -i, sure.

   Something like:
/nix/store/2kbxp7w3wfmisn71pkf5cx721hsp5gk6-openssh-7.7p1/bin/sshd -d -e -h host_rsa_key -f /dev/null -i

But actually:
openssl genpkey thing | sshd -d -e -h /dev/stdin -f /dev/null -i
   
also viable for key generation is:
echo y | ssh-keygen -t rsa -N '' -f /dev/stdout
but that's not a very nice interface

I guess we generate keys for both halves and let 'er rip?

Let's go ahead and do this with a test_ssh file.

That way we can use -i?

Ah so we need an absolute path to the host key, mhm


OH Neat cool!!!

I could just have ssh -o ProxyCommand 'sshd -i'!!

That would be super mega rad!!!
* friendly api
  So, it would be nice to have a local_stdtask just sitting there ready to go.
  And I can just use it, and call .fork to get another thread.

  And that would be great.

  But! I need to handle ownership! Closing resources, y'know?

  Don't I need a thread running in the background to do that?

  Well, hmm.

  So somewhere down the stack, for async things,
  there's a call to wait_readable - or possibly other trio functions - that's the core primitive.

  And that is only valid in a specific event loop.


  Hmm I should ask the trio people how to approach this.

  Also whatever I do should work in asyncio as well, hmm.

  Running is all fine. Probably.

  It's really the garbage collection that's tricky.

  Hmm. System thread?

  I guess theoretically I can start a coroutine as soon as someone allocates a resource that needs to be freed.

  Maybe this isn't so important really

  If I start a coroutine as soon as someone allocates..

  But there's still the issue of, how do I myself clean up my resources?

  Well... I guess they are never cleaned up.

  I can say, "put these in a garbage collector which never runs"?
  Could be manually flushed.

  So yeah, that's no issue.
* inheriting epollfd and signalfd
  hey neat, according to some random guy on the internet,
  signalfd on epollfd results in wakeups based on where it's registered.

  perfect!
  that's exactly what I want

  so I can inherit the epoller,
  and I can inherit the signalfd that is part of the ChildTaskMonitor.

  I could even inherit the entire SignalBlock, hmm.
  since blocked signals are inherited.

  that would really reduce the number of syscalls required for a new thread!

  Though, I still have to register the signalfd on the epollfd in the new thread.
* signals and fds and inheritance
  So with cloexec and fd inheritance and all that stuff,

  the core problem is that we start in space A, shared by other threads/libraries,
  and we want to go to space B, where only we exist and we know explicitly all the things we have.

  So, in going to space B, we explicitly specify which resources from space A will be inherited into it.

  That works for both fds and signal blocks.

  And close the rest.

  But how does this relate to a style where we, um.

  Close everything by default,
  and explicitly denote things which shouldn't be closed,
  by marking them globally before the copy?

  Well, that's not library/thread-safe.

  So this suggests I should have an fd list argument on exec.

  Though, that is not actually library-safe.

  Well, the traditional Unix environment is all about mutating in place.
  So really I should embrace that.

  It's the user's fault if they screw that up!!!

  So then I have two styles.

  Style uno: explicitly pass things into new space (used with clone/fork)
  Style dos: mutate old space to create new space. (used with exec)

  nbd nbd nbd

  So in style uno, I would do the thing with passing in.
  With style two, I would make the user do it themselves.

  Both are good.
  For user convenience I will have exec wipe the mask even in style two.

  Though...

  If i just only optionally set up to start child processes, everything is fine.

  Meh let's go ahead and do that since that will solve the problems,
  without forcing me to contort my interface.

  So how do I do that?

  Before shifting gears, have I fully serialized my thoughts here?

  The core insight/point is this notion of moving between shared space and exclusive space,
  and copying things out of shared space to make exclusive space.
  A to B, selecting some resources from A to go to B.
** okay so now how do I only optionally start a ChildTaskMonitor?
   So, I guess one way to do it would be to have a different Task class,
   which only has enough things to exec, not to clone.

   But I guess it would have all the things to exec anyway, so...

   Oh and we inherit the signal block anyway... fug...
   Unsetting it and resetting it is dumb...

   So if it was done by signalfd...

   I'd just inherit the signalfd...

   By unsetting cloexec on it...

   And otherwise it would be closed automatically.

   Now for signal mask, there's nothing that will close it automatically.

   So I'd have to unset in the case where I wanted to not pass down any blocks.

   Which is opposite of the direction I want, I guess.

   Bah! Okay! exec will take a signal set! Fine!

   Or a list of signal blocks?

   Yes, a list of signal blocks! Fine!
* ssh
** detecting that forwarding is done
   OK, let's list my desiderata.

   1. I want to be able to use the .ssh/config that people already have, including if they are already using ControlMaster.
   2. I want to be able to detect when the forwarding is done.

   lol we know the socket is ready once the command starts

   and for bootstrap, we know the command is started once the first command exits.
* thought
TODO consider using pdeathsig to kill off rsyscall threads instead of fd hangups
though, how do we detect death from the parent?
I guess we need to closely intertwine child monitoring and sending syscalls.

(For also the reason of, that's how we detect exec)

If we have every syscall go through the,
"either the child terminates or this syscall returns",
routine...

That will nicely allow us to stay single-fd-spaced.
Which is, efficient!

Then I can use only unshare as my unsharing mechanism.
Which allows for much more type-specific stuff!
Which would be rad.

I also need to do the fd sharing efficiency thing.

Signals will still be automatically unshared, I guess we can't prevent that.

Which should I do first?

I guess I should separate out the exec-rsyscall-server thing from the
make-thread thing.

The exec-rsyscall-server thing can just take a thread and exec in it.
Like with ssh.
Actually. With ssh can we just go ahead and do that completely?

Instead of spawning a child thread,
just directly exec from the passed-in rsyscallthread.

Well, yeah, we could do that,
but I don't see the use right now,
and also it reduces abstraction;
currently we actually spawn a separate subprocess for socket_binder.
Which, we could work around I guess.

WEll
So yeah that seems good
Let's convert the full_exec into something taking a thread and execing in it.

behind syscallinterface, childtask thing.
** connecting task
   I don't think I actually need the connecting task.
   I can just pass down the listening socket wherever.

   I'll defer doing that for now though.
** bah okay priority is to put the childtask behind the syscallinterface
   Then we can detect exits easily and throw.

   And then we can stop unsharing the fd table when spawning a new thread.

** what do we want for child cleanup
   okay so we don't want to force not orphaning children,
   we want to expose all the functionality that is really there.

   but we still want the rsyscall threads to be killed on exit.
   so, sending them sigkill on parent death seems best for that.

   tempting to enforce the same invariant for all processes, but nah, we don't want to force that on people.
* we can write a nice supervise library that just does a double fork
  and gives you that as its interface - neat

  and it will just clone

  oh and the library can um I guess
   
* derp
  Oh I really am getting mismatched syscall return values now.
  I need to make syscalling cancel-safe.

** making syscalls cancel-safe
   So I need to have the write done as one unit,
   because I don't know if it's interrupted, when it is done.

   right right

   and then once it's done, I need to have the read handled right.
   right right.

   so really I don't even want to cancel

   I just want to stop blocking on the operation, but still run it in the background.

   Because if I interrupt it, that would result in nondeterministic behavior.

   Hmm mmm mm

   Can I special case the things that I want to stop waiting on?

   Well, any syscall can be interrupted by ChildExit.

   When that happens, we can close the connection and we're done - all good.

   Likewise if we interrupt execveat.
   
   We will just close the connection.

   On the other hand, if we need to interrupt wait, then...

   In these wait cases, we are just waiting on a specific thing, right?

   When we cancel one of those,
   it's because...
   what?

   Well, in the detecting hangup case,
   it's because we've successfully returned from another syscall.

   Hmmmmmmmm

   I could have a baackground task doing the
   waiting.

   And sending it to a queue.
   And I just cancel the queue fetch.
   Or condition variable or whatever.

   And I guess then when I cancel a syscall I can just close the connection.

   Rather than leave it indeterminate whether the syscall is complete.

   In both of the cases where *I* cancel something, I want to close the connection.

   But what about just, y'know, straightforward cancellation of a system call or process or thread or whatever?

   Well I can do that with OS signals I guess.

   OK so if I go ahead and embrace cancellation == close, then...

   As long as waiting is on a condition variable, I'm good.

   Now, how do I avoid that complexification of waiting?

   So, with the multiple-waiters through one-condition-variable thing,
   all the other waiters can be cancelled easily.

   Only the main one has difficulty.

   (Also note that when cancelling the execveat one, I don't want to kill the child task)

   Maybe we can also put the futex task behind the syscallinterface

   And then we just throw different kinds of exceptions up out of it

   Yeah that seems like a reasonable interface.
   We'll just throw MMRelease or Exit out of the SyscallInterface. Or Hangup.
   I guess Hangup just implies Release. So we can have Hangup/Release and Exit.

   Anyway, so.

   Let's think about a low level way to ensure cancellable waiting.

   We, um...

   Instead of blocking on waiting, of course we are on a signalfd.

   And we're on epollfd on that.

   Maybe we can send a signal into the epollfd to stop waiting?

   Hmmm...

   I guess the notion is, how do we cancel an epollfd wait?

   The same technique can be used for child waiting

   I guess we can do, maybe, some kind of...

   Passed in event fd?
   Which we monitor and which causes us to return early...

   If we, hmm.

   Then if we actually 

   I guess we want asyncfiledescriptor blocking to be cancel-safe.

   Well.

   We want to be able to stop blocking.

   In ChildTaskMonitor, stopping blocking means going into AsyncFileDescriptor.

   Stopping the blocking of AsyncFileDescriptor means going into Epoller.

   We need to figure out how to stop an epoller from blocking.

   HOW DO????
   This is the same annoying stuff we had to do before.
   Figure out how to manage blocking of the epoll...

   We just have an additional requirement now...

   Well, so, one obvious way to "cancel" the epoll wait is,
   just let it run and ignore the result.

   But, that's not truly usefully cancelling it.

   Well. If we let it run and ignore the result,
   while sending a new system call to cause it to return early,
   then... everything is relatively fine.

   But if we're talking really immediately cancelling it,
   in a way that is really structured,
   then...

   Well, one way would be to send a signal interrupting it.
   That's fairly unreliable though.

   Another way would be to send a byte on another pipe.
   If we genuinely did that and had to clean it up,
   what would that look like?
** making epoll calls interruptible
   Well, so, just interrupting the syscall and letting it finish latter is kind of hard.

   Because that memory that we were using for the syscall,
   is now leaked.

   I guess the better way to do it is to just keep running it in the background.

   That sure would be ideal.

   Maybe if we could suspend it on cancellation instead of actually stopping it?

   Then the next time someone came around, continue?

   But we're blocking other syscalls though.

   Seems like we have to run to completion, just not on the user's thread.

   If not there, then when?

   If we don't run on the user's thread,
   maybe we run on the thread of the next person to make a syscall?

   The thing is, we won't actually return for sure until the next user comes along.

   What we really want is to be able to invoke the system call return continuation independently,
   but not actually do anything with that result until the next user comes along to run the Epoller logic on their thread.

   Hmm I guess we could achieve that by contorting ourselves here.
   If we did a bunch of stack ripping, anyway...
   
   Man! trio does not sufficiently prevent the need for stack ripping.

   So what would I do

   So, I guess, um.

   I'd have the syscall interface create a list of system call requests,
   and satisfy them in that order,

   using the "shared thread" functionality.

   Again, this seems really lame.
   
   I really want shift/reset.

   OK so let's reimplement shift/reset, eh?

   Instead of calling a function,
   I spawn a thread with a future.

   Let's state what I really need.

   EpolledFileDescriptor calling into
   Epoller calling into
   SyscallInterface

   I want to, um.

   OK so um.

   The cancel interface is outside memory abstracted syscall.

   The memsys has to happen anyway. Right?

   Or, I guess this is just a bit pointless.
   We want the whole thing to actually continue running, just not blocking the higher thing.

   Well, if the higher thing doesn't run...

   Well, at each level, we want the higher thing to continue running,
   we just don't want to be blocked on it.

   So, um.When we have a thread of A,
   which is blocked deep into B/C/D etc.
   We want to stop A at some reset boundary above B/C/D,
   and continue running B/C/D.

   The more I think about it,
   the more I think that what I really need is,
   this core abstraction of,
   a thread which runs only while some other thread is registered as waiting for its outputs.
   And just sits suspended while no other thread is waiting for it.

   This would be transitive of course;
   if shared thread A was waiting on another shared thread B,
   and no real thread X was waiting on shared thread A,
   then shared thread A would not run,
   and therfore shared thread B would not run also.

In more trio-specific terms,
a shared thread runs as long as there's at least one nursery available for it to run in
(nurseries made available through some "registration" mechanism to say that this shared thread can run as long as this nursery lives,
and likewise a deregistration mechanism)

oooooooooookaaaaaaaaaaaaaay
soooooooooooooo

Ugh I don't think I can abstract over the stack-ripping here

In particular because I need to support stopping in the middle while waiting for an async call to return.

OK so, how are we going to handle this?
We'll manually stack rip I guess,
using events to communicate.

blah
blah blah

ok so hm

we can use yield to avoid stack ripping.

but we need to support, um, not blocking.

so, okay.

remembering,
the thread needs to run the syscall then.

um blah

what if we literally just use callbacks?

well, that won't help.
we'll still need to, um.
support cancelling all the way up when we're blocked on readiness.

argh, all I want to do is support stopping waiting

okay okay okay, so, I just want to run this task when I'm interested,
and stop running it when I'm not.
Nothing else.

Specifically, I guess, I need to drive the event loop.
But suspend driving it when I'm done.
Hmm.

Well, I could put myself in the middle of the yield stream.
And forward things up.

It's inefficient, but I can do it.

Then... I have the ability to suspend, don't I?
I don't quite remember... I think there's something that breaks here.

Anyway, I can forward up at each step,
and choose to suspend whenever I want.

And, and, hmm.
What if I choose to suspend while I'm blocked?
Well that's a tricky one!

Ugh I bet I can't do this without extending trio.

If we were in the event loop we could park something indefinitely, but otherwise...

This is why I need to do the async generator thing, I guess.
Because then I can control the blocking on my own.

Hmm, alternatively I guess I could decode the yield protocol and handle it myself.

Actually, I guess I only ever actually block on one thing...
Hmm not true, I do also block on trio datastructures.

OK so if I only ever yield up an event to block on.

And then I myself await on that event.

And then when I'm cancelled I can stop awaiting.

That works, right?

I just have to, instead of awaiting directly on a thing,
have it return an event to await on.

Then...

Well, what happens when I hit ~the lower level~?

OK so the syscall will return a future, yeah, boring, straightforward.

I'll have to drive it.
With some helper method I guess.

I'll call some helper on the future.
Which will run the async generator.

The generator will, as always, yield up events.

The rsyscall connection will yield up events.

The asyncfiledescriptor will yield up events.

Lol fml

Can I fix this with asyncio?

Well, no. I still have the same problem of wanting a suspended task outside an event loop.

If I reluctantly degrade myself by assuming I am running within an event loop, then I can have an easy fix.
Sigh.

I guess I will just do that.

God Python sucks

So let's just make them as system tasks
bah maybe we could just have a nursery?

bah bah bah

bah okay we'll just have a nursery. stupid boilerplate

and then a get local task thing

which creates the part of the task that needs background threads
bah   


** running inside a nursery

OK we're decided:
We have to run inside an explicit trio nursery and pass it down.

let's start with thread_nest

** childtaskmonitor
   okay so things are working fine right now because we're passing down the parent's childtaskmonitor,
   which means we're spawning a child from the parent.

   but that's not what we want!

   ohhh!!

   we could!! yeah!!

   we could enforce use of CLONE_PARENT!

   which then makes everything transitively fine!
   cuz we keep having the same parent!

   and if we want to start a new monitor place!
   we can just do that!
   and stop setting CLONE_PARENT!

   excellent!!!
   this saves us the need to set that up!!

   hokay, so.

   I guess I'll save the parent pid in the task.

   Though note that this can change without our intervention.

   Hmm, maybe I should instead have some kinda indicator in the StandardTask,
   saying that I should use CLONE_PARENT,
   and pointing to my parent's ChildTaskMonitor.


   OK so if I have a Union[ChildTaskMonitor, ChildTask],
   indicating either I myself am ready to monitor child tasks,
   or I am at least being monitored by someone else,
   then I can choose based on that either to directly clone,
   or clone with clone_parent.
   ChildTaskMonitor needs to take a task to clone off of, then.

   Hmm it would be nice to have an abstraction that just lets me clone directly and adds CLONE_PARENT if necessary,
   instead of having to do this casing every time.

   Meh I could just pass task into ChildTaskMonitor and do an equality check and if it's not right, add CLONE_PARENT.

   Flub

** how to model parent task thing
   I really don't know how to model "parent task namespace".

   Oh hmm.

   It really is like a namespace I guess.
   If we don't unshare our parent task.

   Well, um, hmm, all the things in the same "parent namespace"
   really are being monitored by the same thing.

   But we don't know what that is.

   And, importantly, it's not safe to unshare the parent namespace if,
   we don't have a monitor ready.

   Really in general I guess the notion would be, yeah.
   Default to CLONE_PARENT.
   Default to telling our parent about our children.

   If we can't parent them properly,
   then we have to let our parent do it.

   Well, that makes sense, but, in the case of rsyscall,
   we still need to actually get the child task resulting from when we make our parent do monitoring.

** why do I need connecting task
   Oh, because if I'm local, and can just use socketpair() directly instead of connecting to a unix socket,
   I still need some way of getting the other end of the socketpair down to the other end.

   But... really I only need either the listening socket or connecting pair.

** FUG child task gets signal before clone returns

   And since we are waiting for signals in the parent task,
   we get breakage

   that is inevitable?

   if we had some means of not collecting the zombie, maybe we could do it?

   maybe we could just not collect zombies while a clone is in progress?

   not possible.

   maybe

   so if I grant that I need to take the wait lock,
   is there some other way to get notified of exit?

   so that, y'know, things work properly?

*** two thoughts
    This is really tightly related to SUBREAPER, because both cases are times where pid wraps hurt us.

    In fact I think we have this problem with subreaper alone!
    Well no we don't, if we're single-threaded.

    But being multi-threaded, we do!

*** locking
    OK so if we lock clone relative to each other,
    and we don't have subreaper,
    then we can't have problems.

    But if we have subreaper at all, and we're multi-threaded, then we're in dire straits.

    Well...

    More that, if we clone,
    and we wait also simultaneously

    possibly useful things!

    I could peek for a child without waiting for it,
    then decide whether or not to wait.

    Also, I can selectively only wait for EXITED/STOPPED/CONTINUED children

    Really the problem is with multiple people mutating what's in my child set at the same time.

    Hmmmmmmmmmmmmmmm

    I want to leave children still as zombies while I am going to fork.

    Except! For things that I am actually looking for!

    For those, I...

    Well, for children I know about, I can freely dezombify them, right?

    If I know about the child, a

    if I know about the child and I collect it, can fork problem happen?

    No.

    The problem only happens if I collect the zombie of an unknown child.

    It's all about mutation

    OK so what are the requirements?

    Threads, and being pid 1?

    Why not just being pid 1 at all?

    well, because it's serialized relative to other things, y'know!

    If all the forkers fork fork fork,
    and we pid wrap...

    Well, we can't pid wrap while we aren't collecting. Is that the issue?

    No, we can pid wrap;
    if someone below us is spawning children and wrapping them,
    that will wrap.
    But that's not the annoying thing.

    The annoying thing is, I guess, if we see a wrap?

    Well, hey, wait a second.

    If we collect a child, we already know it's dead.

    If we then return, we know that must not have been our child.

    But wait, that doesn't work; because we don't actually serialize clone with wait.

    The whole issue....

    OK, so!

    If we wait specifically for what we get a signal for,
    iff we know about it,
    then we do an additional unrestricted wait(WNOWAIT) to see if there's anything more,
    then wait for those things as well if we know about them,
    then...

    Well!

    Doing an unrestricted wait(WNOWAIT) won't help us,
    if the head of the queue turns out to be!
    A very vexing!
    Child we don't know about!

    Hm!


    OK, so what if we just consider the problem of CLONE_PARENT alone?
    Can we robustly solve it?

    Bah, no!
    The collection causes pid wraps!

    We really need to serialize cloning relative to waiting.
    If we don't, we have many problems.


    So okay we could do the wait for what we got the signal for thing,
    and then if we get blocked by seeing some unknown thinger...

    Oh, hmm, we're not too blocked though.

    Oh, we are very blocked though, if waitid(P_ALL) returns a child we don't know.

    We could do the PGID thing,
    grouping our children together in that way.

    But still we could get blocked there.
    
    Our only option is to waitid for each one individually!!!!

    We have to do this every time, if we see an unknown pid in a waitid.

    Only while actively forking, though.

*** bah!
    OK, I can't use CLONE_PARENT.

    Well. That's so defeatist :(

    If only there was some way to serialize cloning and waiting...

    But all syscalls require waiting on the task making the syscall.

    I'm just now for the first time running into a Linux feature that just isn't usable.

    many people are vulnerable to this;
    I could cause my parent to get a child it doesn't know at any time!
    just through using clone_parent!

    and, any multi-threaded program which doesn't serialize waitid and clone,
    is vulnerable to this too!

* doing it full
  I need to just go ahead and pass down the things.

  I'll construct a new ChildTaskMonitor I guess

** fd ownership when there are multiple tasks using it

   In other words:

   There are multiple different ways we could access the namespace,
   and multiple things all using the thingy in the namespace.

   So...
   Do we want some kind of hyperactive fd?

   Which has multiple owners, any of which could use it?

   And, I guess, we'd have a wrapper for each task using it?
   And when that wrapper goes away, we remove that task from the owner list?

   So... I guess we could have that, like, underneath the active fd.

   Like, ownership thingy, right?

   okay so:

   I've got some number of file descriptors F,
   file descriptor tables T,
   processes P,
   and objects O.

   A file descriptor F is in a single file descriptor table T,
   and a process P can access a single file descriptor table T.

   A file descriptor table T has many file descriptors in it, and many processes able to access it.

   There is one thing we can do with file descriptors, which is close them.
   To close a file descriptor F in table T, you need a process P.

   An object O has a reference to a single file descriptor F and a single process P.
   An object can be destructed at any time.

   We want to ensure that when all objects referencing file descriptor F are destructed,
   that file descriptor F is closed.
   What additional structure and design should we use to achieve this?




   I've got a file descriptor, and some number of tasks, which are threads under my control,
   and some number of various types of object which hold references to file descriptors and tasks.
   I want to close the file descriptor when all references to it disappear.
   To close a file descriptor, I need to have a task

** fd ownership
   so there's a major issue:
   we have multiple things wanting to use the same fd,
   through different tasks.

   if we say when object O is destructed, we close fd F,
   then we can't have multiple objects referencing the same fd with different

   Conventional multithreading can just refcount the fd F,
   so that for each object O referencing F,
   we increment a counter,
   and decrement the counter when the object is destructed;
   then, when the counter reaches 0,
   we close the fd F.
   But, this relies on implicit authority:
   We need to use a process to close the fd.

   Unlike conventional garbage collection.
   Hmm.

   Actually, could we just have multiple active things?
   And track them inside the garbage collector?

   So we're already talking about,
   "when an active fd is destructed,
   we use that process to close it".

   But what if instead we keep track of,
   if we are the last object referencing the fd?

   Like...
   We have some kind of registry, I guess?

   When the last active fd is closed,
   then we close the fd.

   We store a refcount inside the fd.

   We want to have other processes with active fds for an fd,
   automatically take over responsibility for closing the fd.

   So when we go to close a process, I guess we...

   We need to get the list of active fds for that process, right?

   Then we go through that list,
   and look at each fd that has no other active fds,
   and close them.

   To find whether an fd has no other active fds,
   we need to get the list of active fds for the fd.

   So when I go to close an fd, I guess I'd do the similar thing.

   Well, closing an fd doesn't require closing a process.
   It's not my last chance to close the process.

   Um...

   When I close the process, I might be losing the ability to close an fd.
   So I need to check and close any that need closing.

   When I close an fd, I might be losing the ability to close a process?

   No, processes can be closed on their own without anything else.

   So why would I need a map from fds to active fds?

   I just need the refcount, I guess.

   I guess the core issue is that I have a bunch of things A, which reference some things B.
   And before the last reference to B is removed, I need to perform some operation using the last A.

   And also I have a bunch of things C,
   which reference a bunch of things A,
   and which when a thing C is removed,
   that might remove - that definitely removes the last reference to A.
   And then I need to perform those operations, I guess.

   But the issue, I guess, is that I can also explicitly close these things.

   Which should invalidate all the objects that reference them.

   So, okay, it really does seem like I need a back-list, right?
   I need a list of Os referencing F, and a list of Os referencing P.

   I guess I could just do that in the constructor of active fd.

   I'd need to key the dict with the far fd, not the active fd.
   So as to make sure that __del__ is appropriately called.

   or a weakref to the activefd, I guess.

   oh. um.
   when I construct an O,
   I guess I need to add a, um.

   I have an fdmap: Dict[far.FD, WeakRef[active.FD]],
   and an taskmap: Dict[far.Task, WeakRef[active.FD]].

   When I explicitly close an active.FD, I explicitly close the far.FD

   When I explicitly close a far.FD, I need to set the invalid bit on all the active FDs.

   When I __del__ an active.FD, I need to remove it from the fdmap and the taskmap.
   If it's the last thing in the fdmap, then I need to explicitly close the far.FD.

   When I __del__ a far.FD, I do nothing.

   When I explicitly close a far.Task, I need to call __del__ on all the active.FDs in taskmap.
   Then I delete the taskmap entry, which should be empty.

   When I __del__ a far.Task, I do nothing.

   OK so we can achieve this without further modifications.

** epoller
Need... to... split this...
We'll have an activefd which is the thing we register on, I guess.
And... that will return an EpolledFileDescriptor which...
references the EpollWaiter for epolling, and another task for actually operating.
Or, I guess, the EpolledFileDescriptor is just for epolling.
And, I guess, it needs to reference the activefd, because it needs to del that off the epoll.
So the EpolledFileDescriptor references the epolling task, and the registering task.
But not the operating task.
And not the operating fd.
So there are three active.FDs:
The epolling (task,fd), which is an epollfd
The registered (task,fd), which is some unknown fd
The operating (task,fd), which is probably the same unknown fd and task, but maybe not.
Registering just needs the first two.
Actually...
We really need:
The epolling (taskA,fdA), which is an epollfd
The registering epollfd (taskB,fdA) and used fd (taskB,fdB)
The operating (taskC,fdC) - completely unrelated.

* okay so it's nice and inheriting now
** add unshare_files
   inheriting...

   should I do it in place?

   hmm how do I work this inheriting thing with active fds?

   like...

   I guess the old ones have to be all deleted?

   OK so I think this is actually closely related to the garbage collection problem.

   In both cases, we're losing access to a namespace.

   Ideally, in the case of unshare_files,
   we'd have a CLOEXEC thingy to close cloexec files.

   Which would mean we couldn't then! Close them manually...

   Well, yeah! If we're leaving a shared space,
   and we're the only ones with a reference to a file,
   we need to close that file in the old space.
*** question for proglangdesign
    OK proglangdesign,
    I've boiled down the big complex beast of fork which does many things,
    through careful use of Linux primitives,
    into several distinct primitives.
    Now I don't know how to represent, in a programming language,
    the primitive of:

    I'm in a thread,
    and I have some fds in a file descriptor table,
    and I call unshare_files on some list of fds,
    and I get the new fds. thing.

    I guess the fact that no-one else has a reference to these files in the new space,
    means they need to be collected.
*** space
    cloexec seems very relevant to garbage collection, I guess.

    cloexec is a way to get garbage collection.

    since, presumably, after an exec, all references are gone.

    it's all about gc

    if we know about the whole system,
    we could handle doing all the closing ourselves

    we'd have to lock around unshare, I guess.
    hmm.

    and...
    yes, lock around unshare.
    we'd ideally build a list of fds to preserve,
    and close everything else.

    so CLOEXEC really means, "garbage collect me".
    well, that's how we're overloading it anyway.
*** think of a whole new ownership model for file descriptors
    so, okay, obviously we want to,
    have,
    fds that are unused in a new space be closed.

    so okay, so, we need to think of unshare and exec separately.

    so if the call to unshare was separate from the exec, then...

    it's only when we enter the new address space,
    that we remove the references,
    and need to exec.

    but, I mean, I guess we need to explicitly transition some things into the new fd space.
    or do we? why can't we just automatically work in the new space?
    why can't we just update everything under the hood?

    with the active fds, we can, i guess.

    so if we did that, that would be cool.

    what about no-longer-needed fds in the previous space?

    I guess we can close those.

    okay so if we do this then everything that gets the farfd out of active, needs a lock.

    okay, I think having a proper garbage collection model for these is too much.
    Or, wait, we'll have to do it anyway since we decided on GC, so might as well.

    So, I guess we:
    Do the unshare.

    And then. Well.

    There may be file descriptors which we were the only reference to?

    And how would we close them? HOW HOW HOW?

    Even if we go with the unshared fd thing.

    How do we close the fds in the previous space before the unshare???

    Our signalfd and stuff is there!
*** hard core problem
    So, yeah, if we have some thing with a reference to a space,
    owning some things in it,
    and it copies those things out,
    then loses its reference to the previous space,
    how does it close the things in the previous space?

    ask osdev?

    think about language-based-system? memory management?

    i guess in a LBS we'd just be able to have references to two spaces...
    we'd do the copy into a fresh space,
    and then delete from the old space.

    well, why wouldn't we be able to do that?
    what would make us need to not do that

    okay so I've got some space A, containing a bunch of things X.
    I can close any thing X,
    and I can also make a copy of space A, space B;
    but that consumes my reference to space A.

    If I'm a person with a reference to some space,
    how do I make a copy of that space containing a thing X that I want,
    and close it in the original space?

    i.e. how do I move something from space A to space B.

    how....
    how.........

    AHA!
    Kai landed upon the solution!
    I'll just make another helper thread!

    Just as I have a helper thread to do cloexecing things!
    I need a helper thread to do closing in the space I'm leaving!

    And this can be a syscall in the future!!

    and we can have a single abstraction that takes a list of fds to close,
    does the unshare,
    and takes a list of fds to preserve through cloexec (or something)

    yes perfect yes

    ok so yeah that makes sense
    and the signature will be someting like:

    unshare_files(int *closed, int closed_count, int *preserved, int preserved_count, int flags)

    Where I guess flags determines whether CLOEXEC is set on the new copies?

    Alternatively we could disable CLOEXEC in exec temporarily,
    so that some fds are passed down.

    unshare_files(List[far.FileDescriptor], List[far.FileDescriptor], set_cloexec=False)

    Though I guess I will leave off the flag.

    Anyway, that's my primitive.
    So then how do I build the GC/unshare system on top of that?

    I still need a lock, don't I?

    Hmm, annoying

    well, let's think about it.

    no let's gc with active fds

    and have a bunch of helpers

    which take locks
*** can we just use far fds and take a lock on the task?
    well, no, because it's the active fd that is going to be updated
*** relocating memory
    Do linear types help with this?

    I guess I'm hoping for something else

    Bah!

    I think handle is a good name!

    And pinning?
    Or just locking?

    This is highly intriguing!

    I also have to think about multiple references to the same handle

    But, yeah! I'm excited!

    To put it into these terms:
    The reason I need handles is, to support far pointers moving between segments.

    Then, I can have multiple references to a single handle?
    And, when it's done, it's done?

    OK so:
    Is active FD the right thing to call handle FD?

    A handle is something which is auto-updated.

    Can we do auto-updating without having the task around?

    It wouldn't even make sense to auto-updated, right?

    When something is copied to a new segment...

    Well, I guess it doesn't disappear from the old segment.

    But, we're only able to access the new segment.

    It's like...

    We make a new segment, and um, update our segment register based on it?

    Our only operation is,
    switch our segment register to contain a new segment that has a copy of the old segment.

    And we therefore need to update all the handles when that happens.

    Well, yeah. We need some kind of list of handles, which are associated with a given task, that need to be updated when that task does.

    So really, yes, the handle FD could be allowed to not contain the task pointer.

    Instead we'd have a list of fds inside the Task. Or something.

    That would work, and is also very interesting because we already need a list of fds for every task.
    So it could be fine.

    But, let's consider the garbage collection needs.

    If we have a handle.
    We can update it, of course, when the task is closed.

    Oh, we also need to lock it though.
    How would we lock it?
    We'd need to lock the task as a whole.

    Yeah, how do we lock the task?
    I guess we are just locking the handle?
    But we can't unshare if any handle is locked,
    so we might as well have a task-level lock.

    Yeah.
    A reader-writer lock makes sense too.
    Because any number of people can be using the fd (reading) simultaneously,
    but they can't use it while someone is unsharing (writing),
    and there can only be one person unsharing (writing) at a time.
*** setting a namespace rather than unsharing
    When we do this, I guess that we will just invalidate all the previous handles.

    And we can create new handles from the existing handles in the new namespace.
*** api    
    So we'll explicitly list the things we want to close,

    but we'll preserve everything.

    We... won't call cloexec?

    I guess we could explicitly list the things we want to preserve too, sure.

    Or, no... I guess we just cloexec away everything that doesn't have a handle. Relatively easy.

    Yeah so the auto-closing only happens in the new namespace, makes sense.

    maybe I can have the handle object contain only a near fd?

    OK! I'll convert all my APIs this weekend.

    For now let's do real work.

    Wait so if we just contain a near fd, do we even need the lock?

    Assuming that we don't check far/near.

    We'd need to serialize relative to setns calls.
    But not unshare calls.

    Unshare calls...

    Yeah, so I mean, as long as we don't check far/near, then we're good.

    We can do the unshare and things will just continue working fine, right?

    No locks needed...

    If we have the invariant that everything was valid before,
    everything will continue to be valid after an unshare.

    What about the unshare API?

    Again, we'll list the things we want to close in the old space.
    Right...?

    And we'll automatically preserve things that we have handles for in the new space,
    and cloexec away everything else.

    So, because of setns, we still need a lock.
    But... oh, maybe we don't need one for files at all!
    I guess indeed we can go without a lock, possibly.

    Which makes sense, I guess.

    Well, there's still the issue of multiple coroutines, or multiple threads, closing something as we're using it.
    But, no, that's not something we need to handle.

    I guess we don't even need the fd_handles list either.
    Hmm! This is promising. Less infrastructure!

    Oh, no, we do need that list, to determine what to not cloexec in the new task.
    That's the real thing we need to handle:
    The GC of fds in the new space.

    Hmm!!!!

    Should I take handles in the handle helper functions?
    Or should I not require ownership in that way?

    And, if I do take handles,
    how does it work if it's coming from another task?

    Well.
    I guess I need to take a temporary reference to it, right?
    Well, I already have a reference to it.

    But, I need a reference to it, in the same space as me.

    So, hmm.
    Maybe I do need a lock?

    OK so say I don't take a reference to it with my task.

    Then, the following events could happen before the syscall:

    I could unshare.
    And I could GC away the fd.

    And then execute my syscall.
    And it would be on some random thing.

    So really, I do need to keep the fd around.
    Not with a lock, but at least with an active fd.

    So, if the space matches,
    then I'll adopt the fd.
    Making a new active fd,
    and then shortly thereafter deleting it.

    This isn't a matter of space movement.
    This is a matter of references going away once I've moved into a new space.

    Which, is related to space movement,
    but it's really about making sure that I preserve a reference to the thing,
    even when I move into a new space.

    So, yeah, I think it's just a matter of making sure I hold a reference while I'm doing the syscall.

    Then invalidate it immediately after.
** add unshare_vm
** make full_exec use them
** thinking about mapping
   okay so, environment variable keys.

   I'll just make them strings.
** unsharing memory
   Lots of cool things

   You could have segment based handles kind of things, where pointers are offsets into memory mappings,
   which could be actually mapped at different addresses,
   but are recognized as being the same.

   More importantly:
   Why not just having the data gateway be shared?
   That would be nice and efficient.

   We're already talking about locking over the connecting connection.
   Why not the same for the data pipes?

   Then I just have to inherit them down, and all is good.

   Does this result in tasks blocking each other, though?

   It kinda does...

   Well, for the most part I won't be using multiple address spaces for the same pipes, I guess...

   I mean, the real use case is that I'll have a single data sock,
   shared between a bunch of... fork_shared tasks... which use them to exec.

   Dang! I already need a lock on it!

   OK, so then let's just go ahead and inherit it!
   Creating a new one is an optimization.

   Like we did with the connecting connection,
   we'll make one in the root task,
   and just as an optimization,
   not use it.

   What about this describe sock?
   That's genuinely new, innit?

   We could take the lock on some descriptor maybe...

   If we don't signal end of message with eof, then we could reuse something else, yeah...

   The pair inside the memory gateway seems like a pretty reasonable thing.

   It's already async and everything.

   Seems fine to support an API of temporarily borrowing it, right?
   Hmm.

   That does block all syscalls on the remote host though :)

   How would I work around that?


   OK, keep in mind we aren't talking about sharing the syscall sock pair.
   Just, because it's, really hard to do that.
   We don't control when the response comes back,
   so there'd be lotsa interleaving.

   But for, say, this data transfer stuff,
   we do control the responses.

   Well, it's not that expensive to make a connection, let's just do it.

   Sharing the gateway...

   We'll need some backing class which holds the lock,
   then a class in front which is inherited around to different tasks.

   Basically the same split between "access task object" and "user task object" as with Epoller.

   And, fancy fancy, it will just keep working when we change memory namespace.
   As long as we appropriately take ownership at task creation time.

   OK so actually maybe the memory gateway should indeed have a "bytes" interface?

   No no no we discussed this, pointer is best.
** extensible bootstrap
   It would be nice we could have specific applications,
   add additional things to the bootstrap.

   I guess it's just a matter of,
   do we dynamically bootstrap?
   Or rely on the bootstrap to have everything statically?

   Nix allows us to be static,
   but if we aren't using Nix that we want dynamic.

   Also, if we want to manage the deployment with rsyscall,
   we want dynamic.
   Though we can copy over the bootstrap without much problems.

   Well, to copy something over, we'd want to be bootstrapped first.

   So maybe it's a matter of multiple layers of bootstrap?

   Instead of trying to stick everything into the standardtask.

   mm, yes.

   I guess when we bootstrap to the other side,
   we'll hardcode the path of the rsyscall bootstrap,
   but also the path of the application bootstrap.

   Or, actually, the everything bootstrap.

   Meh so I can have it take the bootstrap location as a parameter.

   And the Nix wrapper hardcodes it.

   Yeah I don't think it makes sense to have this utilities stuff.

   Though I guess having a tmpdir is nice, hmm.

   It would be best for those to be in a separate class.

   At least the socket binder and rsyscall bootstrap and ssh, definitely.

   Probably sh and rm too.
** next step
   Writing a Nix-daemon-running script/library thing.

   Considering doing it makes me realize that self-bootstrap is very important.
   Depending on the bootstrap to exist on the other side is lame.
   So let's just figure out a way to run where,
   we don't have that dependency on already being deployed on the other side.


   Maybe have a small shell snippet?
   Which takes the executable from stdin and runs it?

   fexecve would be nice here :)
   But that's not an option I guess.
   We have to have something on disk.

   Is there some way I can manage to call exec on a binary I supply,
   without a dependency on anything in the filesystem?

   Yes: execveat.
   But, then I have another problem: How do I call execveat?

   hey #openssh, so, with execveat and memfd_create on Linux,
   I can exec into a (static) binary of my creation without touching the filesystem.

   I want to be able to bootstrap into a static binary of my creation,
   over ssh,
   without touching the filesystem.

   If I could call execveat and memfd_create from sh,
   then this would work.
   But I can't.
   Any ideas?

   Hey #openssh, I want to run a static "bootstrap binary" on a remote host over ssh,
   which will talk to m

   Oh, I already have a dependence on the filesystem. Dang.
   From socketbinder...

   I could exec a heredoc :)

   Haha, yes! I can just do that! Awesome!

#+BEGIN_SRC bash
exec {fd}<<<foo; cat >/dev/fd/$fd; chmod +x /dev/fd/$fd; exec /dev/fd/$fd
#+END_SRC   

OK so I do this and...

Hmm...

How do I preserve stdin for interactive use?

I can do this for socket-binder,
and it just works,
very naturally.

But not the rsyscall bootstrap, hmm.

I guess I could have a single bootstrap.

Er hmm.

I could embed the binary on the command line?

Then use that to connect to the socket path and exec?

Bah...

I already have the filesystem dependence one I've got the socket binder.

I'll just have the socket binder contain the stage 2 bootstrap as well.

Which...
I guess I can copy into the filesystem tmpdir place.

OK, so that's the next step for me:
Hacking together this static binary ssh bootstrapping thing.

hmm, depending on bash does seem bad though.

we depend on cat and chmod

OK, I reduced the dependency to just POSIX sh + cat + chmod + mktemp.
That should be minimal enough.

Now let's make that static binary.

Note that it also must contain the rsyscall server. Hmm.

I guess we were talking before about the rsyscall server being a static binary,
embedded into the library.

I don't think we want the socket_binder to itself also be the rsyscall bootstrap.

We'll pass it its own fd number as an arg I guess,
so it can search its binary...

Or, really, everything should be linked into memory, so maybe we don't need that.

hm mmm

if we had true ssh control,
we could do it all at once:
stream the binary over on a higher fd,
then exec into the right thing.

but, we don't, and that will make us less portable,
so let's continue not having it

OK, so I guess we could in fact do the two-stage bootstrap in a single binary.

We stream it over, then we copy it out and save it as "bootstrap".
That invocation causes it to do the second stage of the bootstrapping.

Seems good.

I guess we could have the shell script do the temp dir creation...

Well, it won't take XDG_RUNTIME_DIR into account.
But I suppose I could do it manually.

Oh that won't allow for long-path unix sockets.

Oh, well, yes it will, I just have to open the dirfd.

Oh, I can't use long-path Unix sockets anyway.
Because ssh doesn't support forwarding them.
Drat.

Well, it could be patched to support it at some point, shrug.

OK, so yeah, let's have the shell script do the temp dir creation.
Hmm, will it also print it?

Yes, I guess.
We'll print the temp dir location.

Oh, we can only actually do it once the bind is done.
So I guess it will print the executable path.

Nah, we'll just copy it twice.

Or, I guess we could do a linkat.

No we can't do a linkat.
Let's just copy it, it's not a major thing.

Ugh, but I don't want to write that loop.
Let's not then.

Let's indeed have the shell script make the tempdir.
And change directory to it, I guess?

Then we don't have to open it, which is nice.
It also removes that specific dependence on /proc.

Though execing stdin still requires proc... wait no it won't! Because we'll be execing something in the filesystem!

connect unix socket still uses proc, but we can get rid of that too.

Nice, removing that proc dependency is good.

It will be nice to be able to just start up on an arbitrary host,
it's obvious in retrospect that that is what we want.
** working on things
   OK so

   Why is this exec from stdin thing,
   getting an empty stdin???

   Oh, that's right, I remember now, it's because I set cloexec on stdin,
   with replace_with.

   I need to do a new thing.

   Ugh what a nightmare.

   I'll just ignore the issue and stop setting cloexec.

   I think some kind of set-inheritable API is nicer anyway.
** epoll problems
   OK so if we non-blocking epoll_wait and get nothing,
   then someone else runs and picks up an event so it doesn't go through epoll,
   then we wait_readable,
   we can block forever in wait_readable,
   even though we were waiting for the event that is already picked up.

   Annoying.

   so yeah.

   Hmm, this is kind of a major issue with an "eager" epoll strategy.

   If I ever "eagerly" look for events,
   I'll run into this.

   So if I instead just go straight to the blocking thing...

   Right, so, the issue is, I'm someone monitoring some event source.
   And my code allows for multiple people to look at that event source,
   and even eagerly pull events out of it.

   And if one of my threads is blocked in epoll while I do that,
   it won't be woken up.

   Since the event won't show up through epoll.

   Specifically...
   One of my threads could be suspended, about to epoll again.
   Or it could be selecting on epoll.

   In either case, there's a window for another thread to pluck the event before it actually is visible in epoll.

   Hmmm.

   A thread could be woken up for something else,
   and go back to blocking on epoll without first checking for the event.

   Hmm, but perhaps it should check for the event rather than block on epoll?

   I guess if the notion is,
   we receive neg-edges from EAGAIN,
   we can also receive pos-edges from successful reads.

   Aha, then this behavior is fine.

   I just need to make sure to catch posedges outside epoll.

   Hmm, how to structure that?

   posedges!
   Hm!!!!

   Hmm!!

   HMMMMM!!!!

   Ah no I do need to structure it so all posedges come from epoll.
* introduction of usage
  Hmmm.

  I could just use this stuff locally.

  After all a bunch of this is stuff that is useful for me locally.
  The thread-library-ish stuff, say.

  Yes mm
* debug weird thing with signals
  OK, so the signal seems to be pending,
  but we're still waiting for a posedge from epoll.
  Why?

  is there a possibility that we, um...
  read from it and... um...

  Ugh, what if there's a bug in epoll monitoring multiple signalfds?

  oh god, what if when I epoll_wait I don't get notifications for signalfds from other processes?

  ugh, that may be it.

  Aaaaaah, I only have kenton's word for it being otherwise!

  I need to confirm it! Argh!

  same as with the other epoll test.

  maybe it was only working before because I was optimistically reading.

  baaaaaaaaaaaaah


  okay, okay....

  at least some kernel changelogs seem to suggest that signalfd should indeed be registered at ADD time,
  not epoll_wait time.

  UGH! it doesn't seem to work

  I need to confirm this manually...
  With a test C program.

can you guess which of these happens:
epoll returns an event whenever
1. a signal is pending for process A or 2. process B gets a signal or 3. 
** scenarios
*** working
kill child
kill parent
wait

kill parent
kill child
wait

kill parent
wait
kill child

wait
kill parent
kill child
*** hanging
kill child
wait
kill parent

wait
kill child
kill parent
*** what
yes, the order seems to matter.
what...

LOL if I actually kill the child, then it seems to work...
*** while we're fixing kernel bugs
    I'd like to be able to register the same signalfd on epoll twice,
    from different processes...

    Rather than create a new signalfd each time

    Also, come on!

    I want to be able to process thread-directed signals with my signalfd!
    How the heck is this not possible???
*** ugh
    this is known insanity, 
    the discussion on the last patch to signalfd+epollfd had Linus and Oleg Nesterov,
    explicitly saying that people who use signalfd+epollfd across SIGHAND boundaries
    can keep both pieces.

    and furthermore, when I do CLONE_SIGHAND,
    I get nice consistent obvious behavior:
    only the signals in the person calling epoll_wait matter.
    UGH!

    I'll have to resort to CLONE_PARENT.

    Or, I guess, just don't bother with trying to make it possible to double fork.
    That seems easiest.

    Well, it's convenient to double fork for supervise, maybe...

    OK, so then I'll just have forking not be possibly by default.
    It's an additional capability.
*** new design
    I'll have perhaps something called BaseTask.

    RsyscallThread only actually needs the io.Task.

    And the environment I guess.

    We can probably get rid of taskresources, filesystemresources, all the access/connecting things.

    I think we can call it BaseTask?
    Or perhaps RootTask?

    Or something else?

    I'm naming the thing which will contain all the stuff required to fork off a task.

    Since it is now optional and separate from StandardTask.

    BaseTask

    ParentingKit

    ThreadMaker

    Hmm

    I could indeed call it ThreadMaker, I suppose.

    Aw crap
    I'm relying on starting child threads for unshare.

    God damn it

    Okay, okay...

    So, both do_cloexec and stop_then_close require starting child threads.
    Specifically, do_cloexec means we have to start a child thread in our new sub-space.

    Also, if we ever want to run C function in a sub-space,
    that is, a namespace we've unshare'd or setns'd into in a thread,
    then we need the ability to start children.

    So, I guess, let's think about how to do that.

    CLONE_PARENT is the obvious mechanism.

    Another alternative is making epolls and event loops in every task, but that will be annoying.


    Let's just do CLONE_PARENT.

    Bah! Need to think about it more.

    If a child gets a signal,
    is there any way that we can get that signal to be directed to the parent?
    Say it has its own signal handlers.

    Could we actually use signal handlers?

    Ugh, the question then is,
    do I really want to deal with EINTR?

    If I deal with EINTR, then I could support interrupting blocked threads.

    uggghhhh

    signalfd only handles threads from current context.

    hmmm.

    okay, so...

    I could have rsyscall monitor both signalfd and infd.
    and on signalfd yielding something... what would I do?
    I could have another fd which I write to on signal...

    I guess the same would be true if I actually used signal handlers.

    Hmmm.

    Ah, I've got it.

    I could have just a conventional pipe,
    which I copy signalfd events to.
    And, I add that on to the epollfd.

    No, an eventfd, so we don't block.
    And then we'll have to use um, that thing.

    Oh wow, sigwaitinfo doesn't require memory!

    Oh no it does actually.
    But at least less memory, since we don't care about siginfo.

    OK, sigh, so to plot out the userspace workaround:

    I'll block signals as normal for using signalfd.
    I'll have each thread block both on signalfd and the "perform new syscall" pipe.
    If it gets a signal from signalfd, it'll read it off and discard it,
    and write to an eventfd.
    That eventfd will be registered on an epoll instance like before.
    I'll see the signal has been received,
    and call sigwaitinfo to actually handle it.

    The reason I use an eventfd instead of a pipe is to avoid blocking;
    note that if I made the pipe nonblocking, signals could be lost.

    I also, don't think this is really any worse than using a pipe.

    Well, actually it is.
    I have to read both the signal with sigwaitinfo, and from the eventfd.

    What if I did block, would that be fine...
    probably not...
    it would be nice if I could coalesce pending signals, sigh...

    I guess I can make the signal dequeuing efficient by getting the signal number as the integer return value,
    so I don't have to copy any memory back.

    Mmm
    At least I figured out some hack for how to do this 'v'

    What are the eventfd and epoll semantics?
    I guess I have to read it to EAGAIN?
    Which will just work, okay.

    put short:
    i'll just have each thread block on signalfd among its other things,
    and send it into an eventfd,
    then use sigwaitinfo to actually retrieve signals

    Note: how does this interact with the thread actually blocking on a bunch of things?
    Ah, we need to add the signalfd to the list of extra fds to monitor,
    to cause "spurious" wakeups.

    hmm, signals interrupting system calls actually seems better from that point of view.
    heh, could we convert incoming data on the infd to a signal? :)

    anyway... I think it's fine.
    Even with old signalfd plan,
    we still needed to make a waitid call in thread to actually collect children.
    So, if the thread is blocked forever, we can't collect children.

    The same is true now:
    if the thread is blocked forever, we can't get sigwaitinfo to get signals,
    so we can't collect children.

    Oh, wait! Damn!
    When we read from the signalfd, we collect the signal.
    Hmm.
    Tricky.

    We could store the signal in memory,
    then just copy that memory with write?
    I guess that's equivalent, so we can do that.

    Though... we really just need a single 64-bit mask, hm.

    Well, again, we could store that mask in memory.
    And when eventfd is read, we write it.

    Meh, can't we have multiple eventfds for different signals?

    That seems relatively easy, sure.
    But! How do we register an eventfd with our little hack?
    So that signals get sent to the right eventfd for the right signal?

    But we don't waste time on other signals?

    Well... we could maintain some memory for them.

    It seems like either way, we need memory! Ugly.

    Either we have memory where we record which signal we got,
    and read that memory it on eventfd readability.

    Or we have memory where we register multiple eventfds,
    and have signalfd read that to dispatch a signal.

    Ok, or I could just use signalfd as a wakeup?
    Is there some way I can avoid consuming the signal from signalfd?

    Like...
    Maybe if it's ET, and whenever all the signals are consumed,
    the next signal indicates an edge?

    I don't really want a new epfd for each individual thread.

    Well, then I can't be edge-triggered right?

    Hmm.
    OK so if we were ET-waiting on the signalfd,
    then we could propagate the edge up with the eventfd.
    Without reading the signalfd.
    Which would allow us to then read the signalfd from our own code.

    But, we can't ET-wait on the signalfd.
    So unfortunately, we...
    Hm. Tempting.
    Maybe we can ET-wait on the signalfd after all.
    I don't really care about propagating epollfds.

    So, let's try an ET wait on a signalfd.
    I guess we'll naturally, um...
    I guess the eventfd will be the AsyncFD.
    Then we'll have the signalqueue wrapped around that and the signalfd.

    And the signalqueue will reproduce a bit of the logic in the AFD.
    So that it reads the sigfd until EAGAIN.

    Then the syscalls made remotely are:
    epoll_wait -> 1
    read(eventfd, remote_memory) -> 1
    read(signalfd, remote_memory) -> 1
    ...
    read(signalfd, remote_memory) -> 1
    ...
    read(signalfd, remote_memory) -> EAGAIN
    epoll_wait -> 1
    read(eventfd, remote_memory) -> 1
    read(signalfd, remote_memory) -> 1

    Or, alternatively:
    um, if we write configuration in memory,
    there's a risk of atomicity screwups.
    that's pretty frightening!

    if we instead have just a 64-bit mask,
    and read it at eventfd read time...
    then we'll be fine.

    No, this is all dumb.

    The fix is easy!
    Just make a new epoller from Python and use that for the childtask.

    I'll still share the overall epoller,
    this one will just be thread-specific, for the purpose of child monitoring.
** closing fds on exit or exec
    Though what about leaking fds when a thread exits?

    How do we handle that?

    Well, when we exit... I guess we...
    Need to go through the handle?

    Hm.

    Also when we exec, we also need to go through the handle, don't we?

    To close things in the old namespace.

    I guess we could just unshare first before execing or exiting. Hmm.
    Forcing an unshare before an exec or an exit seems weird.

    Hm. Curious, curious.

    I guess when we exec or exit, we know for sure that we have a parent which can clean up after us.

    Perhaps we should use that.

    Actually, the same is true for unshare, isn't it?
    Well, I guess we could spawn children and then unshare away from them.
    We could also setns in the parent, too.

    OK, so we need to have exec/exit routed through the thread.

    So there are two cases:
    Either we continue to exist after the operation,
    in which case we can have a child clean up,
    or we don't continue to exist after the operation,
    in which case we must have a parent monitoring us.

    Hmm, I think having to manually manage cloexec is a drag.

    Instead we want to just explicitly say what to (additionally) inherit.
    Everything else will be closed.

    But that further abstracts us away - is it a good idea?

    Plus I really hate that Python makes everything non-inheritable by default.
    Do I really want to follow that?

    Yes, it makes sense to explicitly list things to be inherited down.
    Fine.

    I can only do that when unshared, though, right?

    I guess unsharing should give me a token that allows me to inherit additional FDs down...
    Or at least unset cloexec on some things.

    Maybe it should have the replace_with method.

    That would be clean I guess.

    And, if I'm already in an isolated namespace... I guess I... already have unshared files stuff...

    Oh, and furthermore I don't need any magic when I exec or exit to clean up left over fds -
    I'm already in my own space!

    Hmm okay all this is tricky, but for now, I guess there's no problem with an epfd per thread.
    Since I can solve the leaking later.
** new epoller
   I guess I'll have a thread-specific-epoller field on StandardTask.

   Argh argh

   This is going to be too inefficient - syscalls in any task will require making an epoll call in their parent,
   which applies transitively, so a call deep in the tree requires calls all the way up.

   Though maybe that's fine if we don't habitually nest things?

   HMMMM

   OK, so I guess I'll add a new thing to the interface.
** possible name
   dthreads!
   instead of pthreads!

   Because, this is a distributed thread library.

   Aw, someone's already taken dthreads.
** debugs
   Ok so I don't seem to be copying memory for epoll_ctl correctly.

   ah there was just a bug in my memory allocator, I wasn't keeping the allocations sorted,
   so I was allocating the same thing twice.

   OK so now I need to implement submit_syscall in a nice shareable way.

   Hmmm blug I should really fix these blocks and weirdnesses that happen when I throw an exception.

   OK resolved that.
   I think, anyway.
   By rewriting the ChildTaskMonitor to maintain a bit of state about whether it has seen a sigchld.
** okay so
   back into setting up copying, and truly bootstrappy bootstrap

   The bootstrap is not truly portable, but the interface is correct,
   so I don't care for the moment.

   Let's set up copying over ssh.

   ALSO!
   Let's please rename things so that RsyscallThread is just "Thread".

   OK so we did the cat thing.

   Let's actually list what we need for the Nix bootstrap.

   And... let's maybe even think about how to do it???

   We can probably just do it on localhost.

   Or we can redirect to a different directory.

   OK, so!

   What we need to do is:

   Build the tarball?

   No, we'll have it built already as an input to our script.

   We'll build and that will give us a script to run.

   We'll check if a remote path exists;
   if it doesn't exist, we'll copy over the tarball and run it.

   We could stream over the tarball instead of copying it?

   We make a bunch of directories I guess.

   And we run the nix-daemon directly under ourselves.

   That's it! Pretty easy.

   I guess before we copy the tarball,
   we should check for writability,
   so we don't waste time.

   Streaming the tarball sounds like it will break a lot.

   It's only 30M anyway, we can just copy it to tmp, then delete it once we're done.

   So what's the goal here? "Convergence"? Or just running on a clean slate?

   I guess the answer depends on how we do upgrades.

   I guess I don't have to fully replace all concepts right away.
   I can have this "convergence" concept, that's fine.

   Though I do like this "start from scratch" concept that I have done with these run directories.

   Yeah I mean, if I was going fully "start from scratch", wouldn't the ideal be Nix?

   Well... no, I still need to create a bunch of state and communication directories and things.

   Beyond the reach of Nix.

   Meh, converge for now.

   So how do I handle establishing a connection to the remote host?

   make_connections on remote_stdtask I guess.
** memory caching and batching
   So three things:
   1. Need to implement batch_memcpy.
   2. Need to have Paths store a cached ptr on themselves.
   3. Need to batch faccessat things all the time.

   Handling the batching here is tricky.
* cooperative concurrency
   the reason people like cooperative concurrency is,
   because it allows writing functions which are atomic relative to other threads.

   that's the same writing pure functions, or writing functions that don't throw exceptions.

   they're all the same thing... effects!!!!....
* batching
  I could implement easy batching by just having requests be enqueued,
  and then performing all pending requests at once.
* batch memcpy

  I could sort and batch one side,
  then sort the other side and batch within each newbatch,
  then batch across?
** notes
hmm, I need to inject something on the write side to do this, don't I?
awkward.
Maybe I need to write incrementally
Hmm, dang, when I already have writes in the pipe,
I can't write another iovec.
I could increment my pointer and look at a later part of the iovec,
but that's the most I can do.
I guess I could finish a partial write with a regular read,
and then do the rest by incrementing the iovec.
oh but I need local memory to do that
bah, let's convert the gateway into something which takes bytes and sends them over
OK so!!!
A raw socket connection is better than most others!
So that's what we'll support for reading/writing data to remote places!
But, we still want to be able to go from bytes to data?
Well...
We also want to be able to splice to the remote host!
For all this! We need to have!
The ability to go from bytes to a pointer! Blah!
And a task in which to write to the fd.
And a task in which to read from the fd.
What if we just have the datapair directly on the stdtask?
OK, we can wrap it inside a Gateway helper,
just for ease of use.
Blah! Blah!

Aha! Okay, so, I'll have a socketpair,
which is wrapped in a thingy,
which certifies that one end is a local end and is in the local address space,
so you can use runtime bytes with it.

Then how do I handle the abstraction for local bytes?
I guess I can have "allocate me a readable pointer containing these bytes",
and "allocate a writable pointer of this size".
What about bulk serialization though?
I guess the plan for using asyncness to handle batching could work.
No, wait, that won't work.
We do want a separation between allocating the pointer,
and putting memory in it.
Ok, so we could have a special case in this local socketpair,
where it just falls back to a direct memcpy instead of going through the pipe?
OK, and we can have a read method on the local socketpair,
which directly allocates a local buffer and copies from the remote into it.
Seems fine.
* handles
  To express changing directory, I guess I need paths which will be inherited when I unshare fs.
  So... that's kind of a sense in which paths are attached to a task.

  But, when I chdir, I invalidate all previous cwd-relative paths.
  I guess I can just actually invalidate them.

  I could have the cached serialized path pointer inside the handle path object thing.

  Though I guess, hmm. What about when I want something not yet serialized?

  I could have a distinction between serialized and unserialized paths

  OK maybe Root and CWD need to be inside FSInformation.

  Also I think maybe they don't relate to mountnamespace.

  I guess people usually pivotroot into something.

  OK, so the root directory is the same.
* urgh
  dealing with paths is so annoying

  I guess I really will have my Path serialize and cache a pointer.
* setting up Nix
  I can do it on the localhost, using user and mount namespaces!

  Then automatically move it over to the remote host!

  Cool cool

  hmm, I could even run NixOS inside the container I guess
* namespaces
  OK, so, I need to support user and mout namespaces.

  Obviously we start with user namespaces.

  Let's write it out.

  users

  ugh
  should I figure out the whole data model and everything?

  let's not do that quite yet.
  let's just call getuid and getgid - annoying as it may be.
* neat
  all namespace'd up, now, what do we do with, um.
  How do we deal with bootstrapping Nix into the chroot?

  We can't extract it inside.

  We have to build the chroot from the outside in!!!

  so we need to extract a Nix tarball or something into a dir.
  hmm.

  what if we just copy the tree instead.

  with... rsync?

  what about the dream of deploying locally...

  OK, let's just use tar to copy a tree.

  And, so, yeah, I'll just check for the dirs,
  mkdir them if nonexistent,
  and copy it over with tar.

  I guess I'll use create_connections to do the connecting, so that I'm agnostic to location.
* weird waitid waiting thing
  Why am I waiting twice?

  or rather, why am I not breaking out of the wait?

  oh, maybe it's because of the cancellation not clearing the waiting thing?

  okay, okay, so...
  the issue is some kinda issue with locking on MemoryTransport, sigh.

  so...
  considering concrete details first:

  when I'm in waitid in a task with different address space,
  I need to call MemoryTransport.read in that task to get the response back.

  but! I might be calling waitid in that task already for the purpose of calling MemoryTransport.read!

  right, that's why read is... broken... well wait what.

  oh, I guess I'm calling MemoryTransport.write, well, whatever.

  That could be solved by locking the sides separately - which I suppose I should probably do.

  But the issue of read in read, mm! tricky!

  So I call read,
  which needs to waitid to see if the task exits,
  which needs to... wait a second.

  I waitid in the parent, not in the child!

  Oh, I see, perhaps.

  I'm, um...

  So if I want to make a syscall in the remote hub,

  OK so even though I waitid in the parent,
  and my original syscall is, perhaps, in the child,
  and therefore I can use the child to perform the syscall...

  I'm still causing conflicts because the entire pipe is locked.

  So okay, simplifying it:

  I want to read some memory in task B.

  I call read on the pipe from task B.

  That requires me to call waitid in task A, which is task B's parent.

  If I actually call waitid,
  then I need to read some memory in task B.

  If task A and task B are in the same address space,
  that means I need to lock that pipe so I can call read on the same pipe.

  How do I know if task B is exited or not?

  If it is exited, then, I guess I need to flush the pipe,
  by calling read some more,
  so that others will work.

  Hmm.

  Alternatively I could just only read the pipe in the root task.

  But yeah it's really tricky with flushing the pipe I guess.

  Transferring memory shouldn't require... transferring memory.

  The circular dependency here is the problem, isn't it?

  Well... how else do we detect failure? Hmm.

  Hmm...

  Like, okay, so.

  I'm some task, and I want to make a syscall,
  but..

  Currently we have to read memory every time we make a syscall.

  Well wait a second, isn't this broken in other cases too??

  After all - if I actually exit...

  Then I'll need to make the waitid call...

  Well, that will

  No, this is...

  Oh, hmm, I could defer reading the memory back over to a later time.

  Like I do with epoll wait.


  It's still fairly weird to require copying memory every time I syscall -
  although I guess I don't actually copy memory every time.

  I make an epoll_wait call every time I syscall in a child,
  but that's fine.

  Yeah so if I defer reading the memory back over,
  to a later time,
  then I don't need to be cancel-guarded.

  In general, calls that I might make in the implementation of SyscallInterface,
  need to be cancel-safe.

  Otherwise they'll hang forever.

  The only issue here is that, uh, I guess I'm taking the lock under the cancel shield.

  Also... I guess what if I'm calling waitid but...
  Eh...

  Oh yeah, what if I'm calling waitid,
  and I die,
  er...

  No, the waitid has to return...

  I think there's some problem here but not sure what it is.
** problem
   so I could just have the waitid support cancellation.

   but that won't solve the problem that I'll still be deadlocked reading the child status.

   instead I need to be able to make a syscall in the parent while doing it in the child,
   that's the long and short of it.

   so I'll just not inherit the transport. Easy!
* mounts
  I guess I could have some more specific path knowledge,
  with knowledge of actual mount points.
  Kind of the same thing as using pointers in memory mappings.

  hmm, hmm.

  Hot takes: should we have to explicitly know which memory mapping of virtual memory a pointer is in?
  Should we have to know what mount a path is in?

  I guess um,
  if we had the truly object-oriented interface,
  what would mount points look like?

  We'd open a file in a directory to get a new object back?
  But it would be a different mount?

  Well, why do we even know what mount is which?

  I guess so we can um...

  Well, we can know whether something becomes valid I guess.

  A path becomes valid.

  Shrug.
* recovering children
  So I like the notion of some server,
  listening on a Unix socket
  which I can connect to,
  to get a new thread with complete sharing with that server.

  That allows me to recover most things.
  File descriptors, namespaces, etc.

  But!
  How do I recover children?

  also, it would be nice to be able to monitor the tasks I create when I connect,
  as children.
* next steps for nix deploy
  So I've got the Nix bootstrap working.
  Next step is to run the daemon, and then I guess that's it.

  Let's, um, see if we can create another shell which enters the namespace?

  Can I remount something as readonly?

  Hmm, I can. Or rather, bind mount as readonly.

  So, I guess what I want to do is, hmm.

  I'll enter just the user namespace, then make a new mount namespace.

  I could keep the thread around that is just in the user namespace, but what a hassle.

  I'll use setns and fork into a new mount namespace.

  Golly gee, it works.

  OK, so what's next?

  So, hmm.

  What exactly will we do in the production case?

  Various stuff

  Namely, deploying some config files using Nix,
  and symlinking them into place.

  (setup-nix.sh and nix.conf)

  And also making the user root directories if they don't exist.

  That's it, though!

  And yeah we'll upgrade by just restarting. No big deal.

  I guess an alternative way to upgrade is,
  to not require a persistent connection to run the nix-daemon,
  and auto-gracefully-transition or restart it when the script is run,
  by recovering state from a persistent state storing daemon.
* next step
  OK, next step is to make a shell.

  ct, mkthr, etc.

  maybe I could have something like explicitly saying the thread you want to operate within?

  then a global current working thread?

  I guess that should be a contextvar.

  I can have functions which take that stuff as a contextvar.

  I guess I can do things stuff
** shell shell shell
   ok so a GUI for controlling a process and making syscalls would be cool.

   but, I think I would want to do it through Emacs.

   so I would want to do this stuff all in elisp.

   so, we won't do a full GUI thing.

   we could still do a shell tho
** but a shell is useless
   if I really want a shell I'd just run bash!

   but I can't do cool builtins in bash, I guess.

   but, who needs them? just write Python


   also, I'd want to use tmux or Emacs to switch between shells I guess

   hmmm

   having a command line is cool though

   hmm

   maybe the python repl is better

   hmmmmmmmm

   i want to be able to rewrite my normal scripts into this.

   and maybe use this as my normal shell?

   ugh but I don't want to have reimplement all the bash features I like

   maybe I shouldn't write a shell

   hmmmmmmmmmmmmmmmmmmmm MMMMMMMMMMMMM what do I do

   I guess I want something that

   will allow me to go in and manipulate what code is running, hmm.

   a big space that I can reach into and mess around

   what about compatibility? I don't want to have to use just Python.

   hmmmmmm

   if I have this thread server thing,
   I can just connect to it to do my stuff.

   that would be good and support multiple languages

   just, anyone can connect here and mess with stuff.

   hmmmmmmmm then a shell would be interesting

   can I enumerate my children?

   hmmmm
   
   what is a job, anyway?

   although wait, I can still do this persistent shell without writing my own shell, can't I?

   hmmm, no... yes... maybe...

   well, with resources and CWD...

   oh, but not file descriptors...

   and not children...

   maybe CWD, but mostly not...

   urgh, we mostly can't setns when multithreaded

   oh, I guess we can, we just can't setns for a user namespace. hmm.

   we cannot unshare newpid, but we can setns it.

   oh, no, we can't

   or can we? hmm..

   the thread itself can't be in a new place...

   this is curious. what happens if I CLONE_THREAD after setns?

   I guess I can switch into new user namespaces and things by,
   shifting into a single-threaded mode,
   doing the unshare,
   then execing the daemon again.

   I guess I could alternatively just,
   kill the daemon,
   do the unshare,
   and restart the daemon.

   Then no need for exec.

   Just gotta shrink down to singlethreaded, to fit through the hole ('v')

   i can have a "kids" builtin, like "jobs", which shows all children.

   hmm this is all kinda like gdb...

   with multiple inferiors and such things...

   what if I make gdb do this?

   hmmmm

   essentially we're just making a debugging interface here, hmm.

   yeah I guess it's pretty cool and fine to have to re-start the server when we change namespace.

   we want the server to be in that new namespace.

   it's kinda neatly purely functional actually.

   okay so let's do this thread server thing and then we can write a shell

   yes this is good, such a shell would be genuinely useful I think maybe ish

* what to do next locally
  I guess I should just start using it for random scripts.
  Which means I need to clean up the processes that are left behind

  Using the pdeathsig thing.

  hmm so with the shell, let's not support any fancy python function call stuff

  if you want to do python, use the python shell!
  possibly by sourcing a python file...

  I guess we could also support dropping into a python REPL maybe
** recovery
   I guess we'll need some good recovery stuff to show the status of the task.

   We'll look at the children thing in /proc...

   And the fd list....


   it would be interesting if there was an "allocate memory" effect,
   and/or a "generate code for target architecture" effect,
   and you, hmm.

   so you partially evaluate anything that has no effects,
   but you can gradually add more effects in to increase the number of things you can partially evaluate.

   so essentially partial evaluation is something that is driven by the type?

   like, hmm...

   we run a program,
   and when it tries to use an effect that isn't available,
   we stop it.

   oh but... what if um... the usage of the result of that effect could be partially evaluatated?

   like what if we get a number impurely,
   then add it to (1+1)?

   oh, that's not possible I guess.

   that would implying sequencing, innit?

   er no, so say we have some expression which uses an effect,

   and we immediately apply another function to that.

   and that other function,
   could be partially evaluated first.
* to figure out
  How do we deal with,
  a task in a shared fd space crashes or whatever and we need to close the resources it had?
** analogy
We have thread T, with object O, which has allocated memory M, in space S.

Say T hard-crashes and doesn't get a chance to free memory M.
How do we deal with M?
** hm
do we just run GC in any thread that's in this space?
we just pick an arbitrary thread to do the closing in.

what if we have no thread in this space, but we could get some?

say it's on a remote system say and our connection went down,
so the process is still alive over there and we can reattach when we reconnect,
but at the moment we have nothing.

hmm mmMMMM

so yeah I guess um

if we um

it's on the tip of my tongue

if we have the files sit around and we can manually reconnect them

there's a resource over there and we don't have it here


ok ok ok so

the question is, how do we reuse the resources allocated for one instance of a thread for another?

just practically, how do we handle the state here?

well I guess we can just start out with a straightforward bunch of state at runtime in the process

and later on we can persist the state to disk

anyway we'll connect a thread then mess it up

do all kindsa crazy stuff y'know

** blah
ok so i guess the next fun step, after remote, is task persistence server 

that does make sense I guess

without that we can't really do this stuff.

because we really want different hosts to be in different fault domains

then also we can manually repair things I guess

just a straightforward notion of, the logic and the state are in separate fault domains,

and we can reconnect to the state if the logic blows up,
and repair the state to be correct.

hmm..

i guess the notion is just that we can kill off the *connection* while the state and logic both stay alive

which is useful I guess in the scenario of a remote state thing which has logic locally,
so it's useful to kill off the connection which actually can happen.

I guess we could um...

we could abstract over the connection being interrupted.

but I don't want to do that.

better to realistically represent that a task will die.

and it just so happens that we can make a new task which shares a bunch of state, which is cool.

ok ok let's figure it out and do it.

the issue is still about, how do we, when we make this new task, appropriately represent that the old state is usable?

meh we already have all the correct things. I think we just need to represent that some namespaces are bundled up together,
into an object,
which holds a path we can connect to,
which gives us a new thread,
and then we can make a new handle for the thread.

so the task can be spontaneously killed, and that's fine.
if there still references to its objets, even if the task is dead, we'll still keep them alive.

only when the last reference goes away do we go ahead and free the things.

but also i guess when all the tasks go away...

well, we can't use them anyway so we can't free them either.

but, it is still the case that we can make new references in and use those.

so, I guess the handle is kind of for the purpose only of, moving pointers between namespaces, as a task moves between namespaces.

there's a different layer that handles garbage collection I guess.

it's good for the shutdown of connection to be really exposed because we can see it for real if a thread gets signaled

I think this approach is fine.

We will have We will have handles which just represent a resource which is moving through tasks.

And, yeah, we'll use them to create new references.

working out a data model for all this fancy resource pointer namespace stuff is hard...

** yes!
okay! it's not a matter of state! it's just a matter of making a new task to manage something that was previously in another task!

we can keep all the knowledge around about where things are locally, that's no big deal.

there's no need to think about recovery or any of that stuff.

it's not a matter of state (any more than any of this stuff is a matter of state),
but rather a matter of migrating from one thread to another for doing some task.

ok ok seems good

so the task is just, move from one task to another.

move existing resources from one task to another.

in particular, developing a model for waiting on the thread group's children when using CLONE_THREAD, is necessary. 

and let's start off by just having a thread, and um...

well, let's start off by just having a thread and clone_threading from it and migrating the wait

mmm i had some words to write here

migrating the childtask I suppose

I guess what I really want is some kinda notion of the shared child waiting thingy.

Oh god but don't I run into that old issue again

If I clone from the same thread group as I'm waiting in?
well, I *can* avoid it maybe.

** remember what the clone_thread issue is
ugggh

the fact that clone and waitid really are 100% grossly interfering with each other

is gross!!

so, we could just serialize them relative to each other.
in this thread server use case

but we don't want to in our previous case, where we have one thing calling clone and we want the other thing to be calling waitid to monitor it.

so! how would we fix this API?

how would we fix the clone and waitid API...

well, I guess instead of calling a uniform waitid, we'd only waitid on specific pids.

but...


hmm...

what if I do NOWAIT and then a concrete wait?

er, does that actually even fix it? isn't there still head of line blocking?

like hmm. we could decide to stop waiting but. it wouldn't help.

i guess

** thoughts

okay so what if I just didn't collect zombies mhm.

ummmm...

okay so how does it really work, the child_settid thing? how does it manage to fix this race?

we gotta figure this out.
** other positive note
at least I'm happy that there's no tricky persistent state things to figure out:
the hard problem is just migrating an object from one task to another,
not resuming from persistent state.
all persistent state will be local, and just describe the remote objects.
** hmmm

the child settid race hmm!!!!! hmm!!!!

how to properly solve it!
** solution:
obviously I want some kind of "waitid on a list of pids" thing.

a waitfd that I add pids to with waitfd_ctl would be good (like epoll),
but anything would work.

waitfd would obv be great hmm.

* wait blog
I need to write a blog post explaining it, before this dumb procfd thing gets merged and we're all screwed.

wanting to be able to signal any pid without races is like wanting to be able to read from any process's file descriptors without races.

it's silly.

pids are only valid when used from processes that are themselves the parents of the pid.

any other visibility of pids is useful only for debugging.

also, wanting to be able to pass process ids around between processes like with file descriptors is a great idea,
but the semantics are very complex.

waiting on things and killing things through these fds might be easy,
but getting cleanup through them is tricky.
if we want a process to die when all the fds for it are closed,
what happens when a process has a fd for itself?
do we do some garbage collection to detect cycles?

yes hmm

I am against procfd or other such naive translations of processes into pids.

I think waitfd would be better.

It solves this race, it's good.

It preserves autoreaping, too.

Could I implement this in userspace?

No, definitely not - there's super huge head of line blocking!

Blah, I guess I can call wait for each known child individually.

That's super inefficient, but it can work.

And I can't avoid that, because signals could be coalesced, so I can't use the signal as a hint and just wait on that one.

Every time I have to make O(n) syscalls. That's gross.

Plus, probably most importantly, it doesn't allow avoiding a syscall handler when implemented in userspace.

Hmm, I can implement this in userspace with a clone_thread I guess.

Ugh doesn't that limit me though? I can no longer change user namespace.

I guess I will be able to centralize my calls to wait though, using CLONE_PARENT, so it's not all bad.

Hmm but.

Does this work to avoid sending sigchld?

It doesn't.

That doesn't have to be a problem though I guess

I guess I can require sigchld anyway, whatever.

Well, the issue is, how do we wait on clone children?

Especially on multiple?

There's no way. Blah.

I could use a different signal handler I guess.

I guess I could indeed do this.

I can directly clone myself;
and then send the pid to the other thread for monitoring I guess.

And then waitpid on all the things hm hm.

And then wait for sigchld I suppose.

Blalh

OK I'll do it.
** ok so the reason I was doing this was
to support some notion of the child waiting group thing.

I guess I can't support reparenting.

I guess there's no real workaround for the reparenting case.

So I need to blogpost about all these issues I guess.

Well, first let's go ahead and actually implement it.

A library which does this thing.

Both in threaded and direct call forms.

Though I guess the direct call form doesn't work, hmm why did I think it did?

If we want to add the waitfd to our event loop we can't.

Oh well I guess we can actually, as long as we're actually doing the signalfd thing.

though it won't work later for children that don't get notified by signalfd

also note that for children we don't know about, we'll be notified in advance with sigchlds.

and drop the sigchld. hm.

blah and what if I want to have multiple waitfds? I guess I could have a single thread and have the ability to drive multiple waitfds from it.

hmmmmmmmmmmmmMMMMm

mmm

how do I handle this huh

i could have one task per thingy

the unfortunate thing is that waiting for task exits is not really great

i really don't want to double my number of tasks

and make two tasks every time i want to run one C function

ok ok so if i go full in on sigchld for everything,
then the only problem is that if something is reparented to me, everything breaks.

okay whatever, if I only do one clone at a time and I don't support reparenting, I'm all good.
let's just go with that.
** then we just need to think about the child adoption thing.
monitoring from another thread from the one that started it, hm hm hm.
** lock over child cloning
   I guess having a lock on child cloning would be fine.

   And we can throw an exception when something is reparented.

   Meh. . .

   Yes, this is good.

   OK so, I will also have a notion of what thread group a process is in,
   so that I can clone from one thread while using another as monitor.

   And I guess parent process too, while I'm at it.

   And, a process, I guess, has some specific parent, and processes in that thread group are allowed to monitor it - right?

   Children are tied to a thread group, innit.

   What are the pid things again?

   thread groups, process groups, processes... there was a fourth one...

   oh, sessions, I guess?

   Ugh okay so let's think about this.

   I guess I'll have a notion of thread group.

   mmmmm really weird and annoying, but whatever.

   I disdain the POSIX abstractions! Linuxisms forever!

   Nice, session is associated with task and not thread group.

   I guess it wouldn't be an issue either way;
   I'd just store session in the ThreadGroup object.

   threads are so dumb. ugh. exec in a clone_thread task is weird. blah.

   ugh so okay it does seem that session is associated with thread group

   that is fine.

   i guess it keeps updating it even if the group leader dies?

   okay anyway let's do it.

   oh hey, clone_thread tasks are always autoreaped!

   they don't leave a zombie! neat.
** child processes
   Signaling processes is only allowed if they are your child process!

   If the process is your child, there is no race condition!

   All these attempts to make signaling race-free are seriously misguided.

   It's as if we were trying to make it race-free
   to read from a file descriptor opened and closed by another thread.

   The only task that can operate on the file descriptor, or the pid,
   without races,
   is the task that controls the lifecycle of the fd or pid.
   If some other task wants to operate on the fd or pid,
   they have to synchronize with the owner.

   And we have perfectly good mechanisms to control the lifecycle of a file descriptor:
   =open= and =close=.
   And likewise, we have perfectly good mechanisms to control the lifecycle of a pid:
   =fork= and =wait=.

   There are significant fundamental problems with the Linux process interface,
   but the raciness of signaling processes when you haven't synchronized with the parent of those processes is not a fundamental problem.

   Setting up a good infrastructure to robustly signal processes that are not your direct children is userspace's responsibility.
   And indeed we already have such infrastructure in the form of process supervisors like systemd.

   We've even improved the flexibility of this infrastructure by adding subreapers,
   which makes it possible to write robust process supervisors without coordination with init.

   Further improvements to the process interface are possible and necessary,
   and I would love to have a process interface that makes more use of file descriptors,
   but signaling processes that aren't your children is not the right place to start;
   I don't believe such a capability ever needs to be added to Linux at all.
** improvements

   A further improvement in flexibility might be like adding a way to 

   More interesting would be a robust way to change the parent of a process to a specific target,
   rather than only being able to reparent a process to init or a subreaper.
   That would allow more flexibility in userspace.
   


   opened, owned, and closed 

   It's as if we added some system call like:
#+BEGIN_SRC c
int process_fd_read(pid_t pid, int fd, void *buf, size_t count);
#+END_SRC

   And were trying to make it race-free to read from file descriptors in other processes.
   That's just not possible! Only the owner of the file descriptor
** write a blog post
   Careful synchronization between calls to clone and calls to waitid(ALL) is required.
   
   An analogy:
   Imagine there was a system call with this signature:
#+BEGIN_SRC c
int read_any_until_eof_then_close(int *fd, void *buf, size_t count);
#+END_SRC

   When called, it would start waiting on every open file descriptor to be readable.
   When one fd was readable, it would perform the read into =buf=,
   returning the number of read bytes,
   and store the fd number into the =fd= pointer.
   If the fd has reached EOF (that is, the read returned 0 bytes),
   the system call will also automatically close the fd.

   This is essentially the same behavior as =waitid(ALL)= for child processes.
   =waitid(ALL)= works on any child process,
   and automatically closes (deallocates) the child process pid when a fatal event is returned.

   This system call would be pretty hard to use,
   but with appropriate abstraction it wouldn't be too bad;
   it's not too different from the interface that an event loop provides.

   But there is one critical problem with it,
   an unavoidable race condition when used with multithreading,
   when multiple file descriptors are being allocated in other threads at the same time as a call to =read_any=.


   If I call =open= in another thread,
   or make any other system call which creates a new readable FD,
   at the same time as a call to =read_any= is ongoing,
   then that 
   but the most immediate problem is 
*** solution 1
    Don't waitid and clone at the same time

    # We need to serialize waits and clones, otherwise we could collect the child process zombie
    # before we even create the ChildProcess object. if we did multiple clones and pid wrapped to
    # the same pid, we would then have no idea which child events belong to which child.
    # 
    # For us, wait_lock is sufficient serialization, since the only thing that can create new
    # child for this task, is this task itself. But note that if we're using CLONE_PARENT in a
    # child, or we're cloning in multiple threads, then we can run into this race.
    # 
    # This is mitigated in conventional multithreading using ctid/ptid, so we can look up the
    # pid in memory before clone returns or actually starts the thread. I don't know of any
    # efficient mitigation for the race when using CLONE_PARENT, so we can't use that flag.
    # Which is unfortunate, because CLONE_PARENT would be useful, it would allow us to skip
    # creating a ChildProcessMonitor in child processes.
*** solution 2
    Only call clone once at a time, and don't support reparenting.
* thread server
  hmm how do I detect a hangup in the thread server?

  I think I guess I'll have the thread server mainloop be monitoring the futexes with dummy threads,
  and perform the close when the shutdown happens.

  After all it does need to close those fds anyway, huh.
  to clean up that allocation.

  it's unfortunate that I guess I have to do this with dummy threads

  ok so, doing this in python is a bit of a drag.

  let's just do it in C right away

  and connect to the socket and all that

hmm could we just call directly in this thread?
we could I suppose...
but it would be weird/hard to represent that in Python...
it's better to do threads, meh.
or, no, eh, if we use threads that has all those limitations.
I like the idea of being actually single-threaded.
it's just that... I guess... the syscall interface goes down...
we should just um...
we shouldn't know whether this is actually single-threaded or not.
but yeah let's just go ahead and do it this way.
of course, I guess then we need to definitely not call exit in this thread,
or otherwise crash it.
but that's fine, whatever.
at least that unifies my fault domain or whatever
heh it's funny that the major breakthrough i made was to *not* delete everything on crash
but instead just keep around the state.
** recovery
   OK so if we had multiple threads in the same fd space,
   and one crashed,
   we wouldn't want to automatically invalidate all the references in the crashed thread.

   So, likewise here, the PersistentServer thingy,
   we'll close one thread it emits,
   then recover in the next thread with the old stuff.

   aha! i can just use the rsyscall_exec stuff I already did.

   i'll just a single server executable.
   it'll start out connected,
   and I'll have an optional additional main loop after the first disconnect.

   hmm, why isn't this just a thread I start again?

   that'd be better...

   then I really don't have to unshare my stuff...

   ah right, I immediately jumped from "do it in C" to "do it in an executable",
   which is wrong.

   I can just do it in a function.

   Ugh

   okay and the reason I previous had to do it in C,
   which was to clean up after myself,
   is no longer valid.

   so... I can just make a thread with CLONE_THREAD?

   then switch to a persistent one by starting a thread with the persistent server?

   I can just make a fork that works with threads?

   so there is some trickiness here:
   if i start processes normally,
   then fork,
   they will be children of the monitor task,
   cuz I'm using CLONE_PARENT.

   so i should be explicit and go straight to the persistent task or something

   I guess I can have a clone_thread option

   Oh wait, can I even use clone_thread though?

   I won't know when to, um...

   I won't know when there's a hangup

   Urgh, this manual close thing is weird though
   
   What if we get signaled and die? well that could have happened anyway.
   what if we segfault? well...

   it's true that using a separate thread would allow us to be isolated from that signal.

   but, wait, let's just not have any such thing when clone_threading.

   if we want nice fault isolation, do it with processes!
   only with processes can we actually see why the thread died after all.

   ok sure so um

   I guess we can't make a task which is sharing the wait scope.

   OK so.

   We can only use clone thread with persistent tasks.

   Right? Why though?

   ugh, if we do then the thread gets confused

   oh, what if we just...

   we start the thread

   we treat it as an exec?

   we force the original task to exit?

   basically a swap over of what is running?

   well, can't we do that in some nicer way?

   sigreturn, maybe?

   hmm, should I maybe have my clone gadget just call sigreturn?

   hmm, sigreturn pulls from the stack.

   we want something that takes an address, mebbe

   hmmmmmm

   so it does make sense for clone to call sigreturn.

   well no not really, I guess there's probably some cookie now to make sure it was added by kernel

   ok so let's just do it with clone

   i'll need to allocate a new stack, hmm.

   bah, let's just forget about clone_stack, and make a new task instead..
** did it
   ok so i guess it's persisting now

   but now for the hard part:

   actually shutting down, say, the ssh task, and creating a new one.

   so let's first do persistent over ssh


   hmm argh.

   when we make a new connection we need to be waiting on a different activity fd, argh.

   I guess we can pass in an epollfd,
   and have the thingy,
   perform the, hmm. stuff.

   we might need to have yet another kind of syscall connection.

   this way we can reconnect it through directly mutating the syscallinterface, anyway.

   anyway we pass in an epollfd,
   and the persistent server makes sure that always maps to the real fd we want toread,
   by adding and removing fds.

   I guess this is just because we want to move the EpollWaiter between tasks, hmm.

   maybe if we explicitly inherited it?

   a persistent syscall interface does sound like a good nice thing

   then we have more representation of what is going on;
   like, syscalls in progress can be resumed and stuff,
   and we could log and recover responses and things.
** do we start with a ChildConnection or a PersistentConnection?
   we can't just start with ChildConnection and reassign,
   because of the epoll fd issue.

   well, actually I guess we could if we made a trivial change to childconnection. hm.

   if we do the CLONE_THREAD thing it will never be our child. hm.

   so we migiht as well get used to it.

   hm.

   let's just resign ourselves to leaking the stack, whatever;
   I don't think we necessarily have to leak it if we get a memorymapping GC.
** hmm reconnection
   If I don't have a syscall out, then everything is good.

   But if I have a syscall out, and I get a hangup, then I don't know what to do.

   I guess I just crash!

   yeah, we should just crash in that scenario

   the purpose of this isn't fault tolerance,
   it's being able to shut down the process on the deploy host,
   and restart it.

   and the purpose for that? well, first convenience,
   but also I guess upgrades.
** ok next
   next is to shut down the ssh connection and recreate it and reopen the thing.

   hmm.

   I guess the bootstrap stuff is left around. whatever.

   HM! need to stop creating a futex task for the persistent thread.
** hmmm
   hmmmmmmmmmmmmm it would be cool if we could rejoin the address space of something.

   like, essentially, merge two tasks.
** hm different cwd on second ssh, reuse state from previous, hm
   OK so I was thinking about persisting the ssh state to disk,
   and reuse the previous bootstrap stuff on my next run.

   But that's a bad idea:
   it means I have to take care to make the bootstrap binary and process backwards compatible,
   which is very annoying.
   It just annoyingly ties the versions together - bad!

   Well, it might not be too bad.
   Anyway, this is something that we would only do if we absolutely have to;
   and we don't have to right now, so we won't.

   Also interesting might be connecting straight to the persistent process,
   but I think that makes reconnection too distinct from initial bootstrap connection.
   
   OK so.

   I guess I should use XDG_RUNTIME_DIR, actually.

   Hmm should I make the socket directly in /tmp?

   No yeah I should just, um, have a directory.

   In XDG_RUNTIME_DIR with fallback to /run/user/$UID, then /var/run/user/$UID, then /tmp

   xdg runtime dir is cleaned up by aging???

   and LOGOUT?!?!?!?!?

   ok so let's just do tmp

   okay so. yes. we'll just.

   we'll make a mkdtemp that.
   oh yeah. we can just turn off the collection.
   or, rather, just don't use async with mkdtemp, instead just mkdtemp and use it.

   oh, urgh, um, clone_fs, how about that?
   is it reset by exec???

   am I sharing my cwd with my exec'd children???
** hmm killing the futex thread
   I guess I should have the futex thread set pdeathsig

   or should the trampoline set it?

   no the trampoline shouldn't set it. or should it. hm. huh. hrm.

   the trampoline setting it would greatly increase my reliability. hmm...

   also it would save me a call on each fork.

   yeah let's do it in the trampoline and unset it in persist.

   we'll have a detach call.

   BAH ARGH
   when something is stopped, it doesn't get sigterm.

   HM. that's upsetting.
   should we go to sigkill?

   hmm ugh ugh UGH

   yes let's go to sigkill.
** unrelated point
   datastructure servers so thoroughly outperform other things, blah.

   it's kind of the same issue as with rsyscall:
   i'm providing a model where you can access everything directly from a single place,
   which is costly but more expensive.

   for performance, we should have a datastructure server,
   where you just send your request and get back a response,
   instead of loading the datastructure into local memory and operating on it,
   which is insane.
** so upgrades
   so it would be cooler to, instead of persisting the thing to disk,
   just pass the persistent connection object and whatever, to the next thing in the chain,
   that is, to say the next upgraded version of the thing,
   and then unload the old code.

   basically I guess I want the typechecking, hmm.

   like.

   I keep around the type of the old version's state bundle,
   and I have an entry point that takes that state bundle.

   (like how I have a main function which takes nothing)

   and it can get more stuff it needs through effects I guess - prompting the user, whatever.
** blah upgrades
   blah blah
   blah

   let's um.
** manual intervention
   typical system administration and operation of a distributed system often involves manual intervention to debug and fix bugs
   people go in and type shell commands and mutate the state of the system.
   in my language-based Linux-focused distributed system library,
   I guess I need to support the same ability, which is tricky to do robustly...

   there are two approaches I guess:
   I could provide a REPL so that people can interact through my library.
   Or I could allow people to just mutate things through ordinary shells.
** operator intervention   
   OK so we need to be able to do all the things,
   manually.

   Every part of this script must be possible to do manually.
   Right?
   That's what we can do with the shell after all.

   I *guess* that means I need a REPL. Hmm.

   I want to be integrated with Unix.
   man pages and usable from the shell and all that. hum.

   hmm.
*** notions
    I could have programs callable from bash which connect to the persistent server and do things.


    I could do fancier things with 

    ummmmmmmmmmmmmmmmmmmmm

    everything has to be manually doable

i could have every syscall be doable from a bash commmand thingy

i could set up a task then run some other task um
*** i could just
    go ahead and

    use an async REPL

    but blah, I do want operators to be able to intervene with a *shell*.

    hmm.
    tampering with a running process - is it a good idea?

    what if we support this in the same way as we support upgrades?

    we stop the thing, tamper with it, and restart it?

    that allows for nice straightforward stuff.

    what about the scenario where an exception is thrown?

    maybe we just have conditions?
*** modeling
    how do we model operator intervention

    blah
*** unix modularity things
    hmm and I don't like the notion of having to,
    be very careful about throwing an exception because it might cause others to break.

    hhhhhhhhhhhhhhhhhhhhhhhhhhh
    yeah I need to figure out a way to make all this compatible with traditional unix I think

    not just on the bottom-end, but also on the top-end.

    we are already runnable easily I guess.
    and the notion would be, just restart it when something goes wrong;
    and we can debug it by looking at the filesystem and at the logs and fixing things.

    hmm that's all true and I guess it works.

    maybe it's fine.

    but, yeah, a monolithic server that does everything in one process, which needs to stay up,
    is a bit scary isn't it.

    well... the resources it maintains are persistent, so we can restart without problems...

    the issue of a fault domain can be solved by putting things which need to be up in a separte process...

    then the monolithic process is just an asynchronous supervisor ("remote systemd")
    which doesn't need to be running all the time.

    but...
    this doesn't solve the problem of mutating things

    okay, what if we just bring up all the objects and connections and things,
    but not the main monitoring process that watches for events and handles them?

    then we can mutate those states?
    then start the stuff?

    this is like the idea of mutating the state while the process is stopped.

    so yeah the notion is that we can mutate the state and then start the actual logic running.

    so, we solve the problem of "big and monolithic" with, making it natural to put things into different fault domains,
    i.e., into separate processes which are kept up when we the supervisor crash.

    we *can* do anything in the supervisor, but we shouldn't, because it makes operation harder,
    because it increases the amount of stuff that is only inside the supervisor and can't be easily interacted with.

    I guess the supervisor is a single running task (possibly starting a bunch of subtasks),
    which doesn't start any processes or anything, but rather just starts monitoring for events and responding to them?

    hmm.

    i guess that's what anything does...

    so the supervisor contains everything that isn't critical to continued operation, is that the idea?
    and we move things out of the supervisor as we want to improve reliability?

    but the supervisor has a bunch of state with which it monitors the other things;
    but if we wanted other things to be able to autonomously recover, we can put the state under those other things.
**** unrelated idea
     manual commands and shell commands should have a typechecker and linter
**** hmm maybe I just need a UI?
     like maybe the supervisor should just support a UI,
     wherein I can, tweak things.

     like... a REPL?

     hmm. that's an interesting idea. then I wouldn't need this write-out-state ability, right? hmm though I still need the upgrade ability.

     actually what if we don't worry about preserving state when upgrading? we just shutdown everything and come back up with all new state.

     that can be fine in my use cases.

     hmm. so then the supervisor just keeps running I guess. but how do I operate on things while it runs?


     hmNmnmNMnm

     I guess from one point of view I can just have the supervisor itself *be* the UI for controlling this whole thing. that makes some interesting sense right?


     this can be a command line tool

     hmm notion: isn't what we want just a debugger?
     like we are desiring, to stop everything, and change some variables, and resume.
     hmmm.

     hmm that's tricky

     what if I just cancel individual python-level tasks and mess with their state and restart them
**** aha
     intervention just happens through throwing/setting/whatevering a condition like in common lisp,
     so that if there is some exception, an operator can intervene at that point

     good!
     so that solves the operator intervention problem, and also the problem of allowng just one component to fail without bringing down the whole thing.

     now we need to solve the partial upgrade problem.

     an upgrade is kind of like another kind of intervention: something is wrong but the script doesn't realize it,
     so we need to interrupt it and change things around.

     perhaps then all we need is a functionality to interrupt some component,
     causing it to artificially fail,
     giving us the ability to modify it.

     though this is weird
     if we interrupt it, and then get a REPL - how do we automate upgrades?
     we want to write a program which interrupts it and drives some upgrade with the REPL
     I guess injecting code.

     which is weird!
     it's like little spiders crawling all over the outside of our program, messin' with it

     okay yeah

     hmm well handlers are like things from the above program anyway

     maybe we could find a way to set a handler to something else?

     like maybe a command to say, set this handler to this thing.
**** okay
     regardless of what else,
     we need to make an async repl.
     so that's my next step.

     To support calling "return" from the REPL,
     we can wrap the code in a function I guess,
     and then throw a sentinel at the end of the function
     so we know that we fell through rather than returning early.
     
     We could return a sentinel instead.
     But also in general I guess we want to catch exceptions thrown by the REPL code.

     Because we want to return control to the REPL rather than abort early.
     Since people might make typos...

     If someone really wants to throw an exception out, I guess we can support some kind of wrapped throw.
     raise FromREPL(my_exception)
**** parsing
     okay so I guess we'll use a hack where we make a CommandCompiler

     which is overloaded

     hmm

     okay so 

     how do we get the from __future__ import things?

     do we need to specially detect it?

     when we wrap everything with async def, from __future__ import is not valid.

     hmmmmmmmmmmmmmmmmmmmmmMMMMMMMMMMMM

     let's just forget about from __future__.

     hmm we're wrapping in a function which is good because we want to support return too

     okay so the local scope thing can be dealt with by,
     putting yet more magic stuff inside the function:
     something which set the locals to a passed in dictionary.

     hmm.
     but this will cause "global foo\nfoo = 3" to not work.

     oh I guess it works by default, hm.

     conceptually, what are we trying to do?

     oh hmm I guess we can do, like...

     exec to create the function,
     then directly run it.

     ummm

     we could, maybe, instead...

     ok so we have an expression that contains awaits

     all that stuff needs to be wrapped inside an async def, so we can get the awaitable,
     and then await on the awaitable.

     let's do the locals thing I guess hmm

     ah, I see, exec and eval are equivalent.

     god this ipython code is trash

     okay so let's just save and restore the locals, that's easy enough.

     is there some way we can, like, do this stuff inside a function scope that we genuinely don't leave?

     I guess we can just locals().update maybe

     what would be nice is if we could define a function, and then get it returned to us.
* final tagless style
  I really like the notion of church-encoding a language AST.

  and then we pass an object with all the things to do (in the haskell case this is a typeclass)

  oho, interesting!

  so we use a new... effect?

  we just take a new kind of thing we can do, and use that.

  we get the behaviors as inputs.

  very interesting!

  so we just add a new input.

  if something doesn't use the other inputs/effects/things,
  then.

  it doesn't need em.

  oh, aha, so they use the typeclass auto-lookup thing!

  to get the new input!

  it's really quite interesting.

  if I church-encode my data,
  then only my new data suffers from the expression problem.

  like. the parts of my data which are old. stay around.

  right so I want to think about that I guess.
** make sure to think about,
   the fact that church-encoding my data means that,
   if I add a new variant,
   my enums are automatically extensible.

   church-encoded enums are automatically extensible.

   why is that? very interesting!

   the other stuff about automatically finding the handlers for specific enumerations,
   by using the type,
   is not really important.

   we can explicitly handle them. and anyway, only supporting one pretty printer ever is,
   ridiculously dumb.

   so yeah!
   church-encoded enums are automatically extensible! that's weird!
** but moving on , serialization I guess
   hmm, whoever told me that control and continuations and effects were not important, was super wrong

   serialization, hm.

   the deserializer returns a church-encoded term.

   but, there's no clean way to add new ways to deserialize things.

   like. new kindsa.

   things.

   so what's really going on in this extensible deserializer?

   ohhhh it's so coooll ahhhh

   gotta pace myself
** this thing about context-dependent evaluation
   what if every term was wrapped with the input functions

   like I think it's weird to have this,
   reify the context as data,
   thing.

   what if we just made every term be taking a new, um, typeclass, I guess
   in our notion those are just data dictionaries.

   so, like. the fact that they need to disambiguate with the extra typeclass argument is, silly right.

   but, hm. isn't it weird to, like, pass new processing functions at every step?


   well one thing which is weird is, how do we nest a term here?
   like if I have some inner term and I want to wrap an outer thingy around it

   like wrapping neg around this thing, um.

   can you do that in haskell? seems unlikely... will it host the typeclass argument?
** lol
   lol is this whole style kinda like a visitor

   like I can pass an object with callbacks around. hmm.

   I guess the type is that,
   in a visitor pattern kinda thing where the visitor can be reified or whatever.

   if I have some new kinda term.

   then I just pass the new visitor to it? huh?

   like if I have some new kinda variant in this sum which I'm gonna visit,
   I can just, um, define a new kinda method?

   well so the notion is. um. the visitor returns the next visitor?

   um so. if I have some recursive visitor.

   like, I define what happens inside right.

   so in this pattern, the whole thing, y'know, falls apart.
   we do the whole evaluation in the parent.. ?

   this is neat.

   it's very tricky
** huh
   so it's very important that we have higher-order polymorphism

   because the repr is a type-level function.

   which means it can turn the type into anything else

   weaker type systems that don't have first-class type-level functions, can't do this.

   because, they, can't allow the REPR to do whatever thing it wants.

   so really what we need here is an ML module,
   with a type-level function from the object type to some meta-language type
   (is it really the object type? are these the right words?)
   and the bunch of callbacks
** so
   we're kind of just taking a bunch of functions,

   then doing some thing,

   then the return type is some other thing.

   well, each function has a signature that is, um, repr T kinda thing.

   the types of the functions also undergo this type-level function thing.
* human handler thing
  we'll store it in a dynvar.

  it'll have a list of things blocking in it, but only one thread.

  we'll, I guess, make its scope closer when um, we, um,
  do some kind of component thing.

  we can have a with, which does a barrier off of a component.
  it both creates a new component scoping thing, nesting the human handler thing,
  and also catches exceptions and sends them to the human handlers.
** thing
  ok so.

  we'll have this thing and we'll connect to this thing with a standalone tool.

  easy, good, simple.

  i'll write a command line tool I guess at first

  then add emacs support.

  actually let's just have emacs invoke the command line tool, I suppose.
  easier.

  OK so yes.

  human delegater, and tool.
** hmm
   But wouldn't it make more sense to just have this stuff be,
   directly in the terminal that we start the script in?

   But, well, we do need to have multiplexing I guess. And I don't want to handle that myself.

   Well... if we started the script under the multiplexer... hm, that's kinda gross.

   we want the script to do everything, including have the UI.

   hmm.

   well, we probably aren't going to, like, have the script start your browser or anything.

   it provides an HTTP server interface that you connect to.

   and I guess we'll have the same thing for terminals?

   I guess we can provide both the HTTP server and terminal interface, and things connect to that, hmm...

   hmm it really sounds like having a lisp machine interface, AKA emacs, would be nice.

   because then we could just call functions to create buffers and talk to the toplevel aka the user.

   hmm. tricky.

   our entire world would be in one place.

   we'd be able to send a notification directly to the user.

   which is good...
** good point from kai
   ok so say the script runs a terminal UI,
   how do they get to that terminal UI huh?!???!

   the script is running on some production box!!
   they gotta ssh to that box and attach to some tmux or whatever thing!!
   boop!

   so really a client is necessary anyway

   my answer is that they run it from their dev box or UI or whatever??

   but what if there are multiple users...
   who want to be able to mess with this thing
** hmm
   so what's the ideal way for a script to yield to the thing above it?

   well obviously by just printing a prompt on the terminal and waiting for input.

   but what if we have multiple things wanting to yield up?

   well that's what job control is supposed to fix isn't it!

   multiple things reading from the terminal is solved
** hummm
   so, the real representation of what's going on is,

   someone passes down some object into the script and,
   it calls methods on that to get things from humans.

   it's the human-access object.

   but each way of doing human access has its own special API.

   so, really we want to have customization in the script itself.

   but, then we have to modify the script when we want to use a new UI - which is fine.

   we could make an abstraction layer over UIs, but that's obviously silly.

   so, I guess the easiest way, is,
   just have unique modules for all the different UI notification ways?

   but what about when we have multiple people with multiple UI preferences?

   I guess we want to be able to support all these different UIs simultaneously?
   We notify them all?

   So, I guess the question is,
   do we put all the UI detail into the script,
   or do we have the script have some abstract interface where the UI has the details of,
   interfacing with that interface?
*** abstract interface
    Basically some pipes

    The advantage is that we can make new interfaces without messing with the script.
*** UI details into the script
    We have the script, um.

    Well the script I guess gets the UI-specific bare minimum details,
    and then launches things/sets up things based on those.

    Well, this is clearly more in line with the, "the script runs a webserver for admin tasks" idea.

    Yeah clearly we want this because it allows more programmatic stuff.
** ok so ui details in the script
   we'll have it run an emacsclient, which seems fine, if crazy.

   then, right, like we were saying before, what's the fallback UI?
   if the emacsclient can't be started...

   well wait a second, we very much want to be able to use all the UIs simultaneously.

   so.

   also note that we can put files in a folder to have details

   and just ping the thing with a notification of changes.

   or I guess we would actually send updates.

   ok so if we're able to use all the UIs simultaneously,
   then when we run the emacsclient, I guess we...

   pass -e to run an expression of course, but.
   we probably just notify of a terminal I guess,
   and allow some code to handle it.

   the logic would be essentially,
   "hey here's a... unix socket?"
   which I can connect to.

   and...

   no meh let's do a terminal instead i guess?

   this is all tricky. let's do an external command instead.

   so the external command will be invoked by emacs

   and, I guess can be invoked on the terminal too.

   blah so that seems like a fine way to interface, we can't control the whole world

   so I have this thing, which I guess connects to some unix socket or maybe a terminal?

   and you connect to it once, and you can do whatever stuff, and if you succeed the terminal goes away.

   so we get notifications of new things, and also things going away.


   so, how about the lock on terminals? isn't that tricky?

   right like, how do we expose, only one UI can be messing with the REPL at a time?

   well maybe we can just remove that restriction?

   what if we have a shared variables thing?

   that would be cool...

   yeah maybe I just should allow multiple at a time.

   okay so that's easy

   and I guess I can print a notification of other people being attached

   and I suppose I can even print the evaluation results and inputs of other people onto all the REPLs.

   yes this seems useful

   though at some point I guess I'm, uh... just having multiple terminals attached to the same tmux session,
   kinda thing.

   well!

   I guess the nice thing I'm talking some common abstracted thing,
   so that, um,
   I don't just have to use terminals or anything.

   though I guess maybe I should have my own local variables? hmm

   yeah okay so what makes more sense is,

   everyone gets their own REPL,
   the locals() of the surrounding scope are in some variable,
   but they are their own REPLs with their own variables

   the commands are serialized relative to each other, um, I guess.

   well, what's the scope of this serialization? hmm.

   I guess the scope of that shared locals() dict.

   a single call to "prompt_humans".

   and okay so. within there, commands are serialized.

   and evaluation results are displayed everywhere.

   oh hmm we probably can just make the other REPL variables visible in some other variable,

   like, _other_repls.

   then you can easily access each other and share and all that.

   and yeah, evaluation results are displayed everywhere.

   so we just have a unix socket and you can connect to it,
   and on reutrn everything is disconnected.

   we could have the unix socket be nested in some directories,
   based on calling the thingy to do the nesting.

   yeah when you do a nesting thingy, you make a dir.

   we could associate our directories with this.

   mmmm

   I guess we could default to just encoding the nesting in the socket name.

   And if we need more, we could provide the directory to make the socket in,
   as an argument to the nesting thing.

   okaaay

   so yeah.

   for now we'll encode the nesting in the socket name - or heck, not even have nesting.

   we'll just support the basic thing:

   a socket. which we serve REPLs on. and have some shared stuff.

   ez pz.


   hmm we also probably want the ability to Ctrl-C things.

   how do we do that.

   over a socket.

   it's an out-of-band thing, innit.

   well, we could just embed the c-c into the stream.

   i... guess that could work

   ew no, hangups will determine it.
** stuff
okay so let's make a better helper for starting processes and redirecting their stdout/stderr,
for use in the repl
** naming
   what should I call this class which does anything?

   wish handler?

   genie 'v'

   there needs to be some easy representation of a hierarchy

   where like, hm.

   we need to represent that we make our own special genie, I suppose.

   like. what do we really want to do?

   we just want to be able to attach things to a hierarchy, um.

   i guess we want to be able to attach a new genie at some level,
   and have it, if it fails, go to a higher level.
** getting the default genies set up
   hmm

   soooo hmm

   we can't really have a default genie without a sockdir, right?

   hmm

   we do need the local stdtask as well.

   it's tricky.

   what can we do to have a sockdir?

   maybe we can just have a contextmanager set up the stdtask.

   and also the genie.

   but, do we make a sockdir every time we run? isn't that a bit much?

   really, we'd prefer to not touch the filesystem

   and for that matter, what's with this dependency on socat? that's also undesirable!

   well, we could include socat into the binary, I guess. but that's getting to be a little silly.

   I guess we look for socat at startup, so it's not too much of an issue.

   oh, we can also have a Nix dep on socat. that would be good too.

   I guess we can handle Nix deps on things by patching the source at build time. that seems fine.

   well, it's a bit gross. but fine.

   well, we can have some globals, maybe. which are either a path to the thing, or None.
   and then we format out a file with the globals.

   seems good.

   okay, so no need to write our own socat then.

   still we have the issue of making a sockdir when we run. that's undesirable dependency.

   well, what if we just went to the console instead?

   that's the real default, eh?

   we could have a genie that just goes straight to the console (using some binary to forward some pipes to the console).

   then on top of that we can make a sockdir genie.

   so, we can make that straight to the console genie always, right? then wishing always works.

   ok so that seems fine.

   and we can have a thing that makes the sockdir genie,
   given a path,
   and it delegates to the straight-to-the-console genie.

   so let's make this consolegenie.
** beautiful
   it works

   I should make sure recursive wishing works okay.

   I think it naively would just go to the genie above, which is perfect!

   But, when you're the topmost genie, you'll deadlock yourself, hm.
   
   Ho hum herm hom.

   he he hu hu

   now let's teach the thing to support wrapping a console server around it?

   I guess we can lock stdin/out in the stdtask maybe?

   Then we can just replace the base consolegenie? we don't have to forward to it.

   nah let's stick with what we have, we can enhance it later
** what next
   so I was doing this to support upgrading and operator intervention.

   upgrading is at least theoretically possible through this mechanism, so I'm happy with this, it's enough.

   what do I do next?

   well, I guess I at least want to support some contextmanager to intercept exceptions,
   and wish to continue running.

   then I can do some multi-system thing, innit.

   well, I want to start adopting this for integration.

   I kinda need to do something more complex first, though, hmm...

   also I think I want a contextmanager that. oh, well, it can always wish, I guess.

   but when we're running some automated test, we disable wishing.

   anyway, so. i need something non-trivial.

   yeah anyway. so this whole wishing thing, repl thing, is great.
   but maybe I need a stronger demo.
** okay
   the next step is the nix runner.

   how do I send an email? I guess I shell out to mail maybe?

   and we have a function which runs nix daemons
** wish on exit
   what would we do/what are we doing in more programmatic terms?
   like, there's an exception, and we recover and restart?
   tail recursion is really what we want here...
   we want to have wish be able to just never return, just take over the new functionality.
   but, we can't do that, so instead, the wish can reassign f and args and kwargs,
   instead of doing a tail call
   so this seems fine.

   right, there's just a driver loop,
   and we can return up (by reassigning),
   a function (and some arguments) to call in the driver loop.
** fail fast
   should we pull this out as well, and fail fast if we can't do the deployment?
   Yes, I think so.
   In fact, we should also run the executable.
   Only after the executable is actually run, should we enter the long-lived stage.

   Although, hmm. What exactly is the right thing to do here? Hm....
   Dropping to a wish-repl is useful on any failure...
   Well, some failures should bring down the system...
   Should any failures bring down the system? Should we just keep sshing to the other hosts,
   even if one host or even most hosts can't be reached?
   Maybe not. Maybe we should just keep going.
   If we decide to bring down everything, we can do that after the wish.
   Yeah that makes sense...
   but, don't we want any exception then to cause a wish? wouldn't we prefer the exceptions to immediately drop to the REPL in place,
   if no exception handler is registered for them?
   for example if we failed on forking the second time, we'd want to keep the deployed nix daemon around.
   or if we failed on wait_for_exit... no, in that case we would want to reconnect the stdtask or something.

   right, so we'll never fail fast. we'll just run and wish on failure.
** avoid with statements
   The thing is, we don't want to use with statements,
   because those are hard to use in a REPL I suppose.
   well, we can always attach an asyncexitstack to the REPL, I guess.
   then we just use enter_context

   it would be nice to attach an asyncexitstack to every REPL run.

   though that would prevent us from making and returning things.

   we should just strive to support garbage collection of everything - with is a hack.
** build time program dependencies
   Hmm we should also have a thing... I guess in the library...
   Where we find program dependencies with a file which Nix formats out.
   I guess we have some function which finds all the deps, and outputs a thing,
   and we can run that at build time.
   then we'll have a nice robust thing.
   and if it's not already run at build time, we run at import time
   and if it's already run, we don't 
   Right, I guess we just, um. Yes.
   We're able to run it at build time and it will output a JSON file.
   And at runtime it tries to load that JSON file, and instead runs directly if it can't

   Should we actually make this a library service? Like, we collect together all the deps,
   and build it all at once?

   No no, each library/module/program builds on its own, and has a separate one.
   Which is in its own package. All fine.
** ctrl-c
   So I could do this by, the user disconnects,
   and then reconnects,
   and they get the locals of their old connection.

   It's a bit tricky, hmm.

   that's a bit gross relative to bash though. hm.

   plus I don't really want to be intercepting each bit and checking for Ctrl-c

   hm hm

   how does emacs do it? I guess it's its own terminal. hm.

   hm.

   b l a h

   so okay I'd need to take over the terminal or what?

   I guess if the Ctrl-C was on my terminal I'd need to intercept SIGINT.

   So, hm.

   So if I receive SIGINT on my terminal, I know things went wrong.

   But what about the case where we've got multiple REPLs running?

   We can't just do the, send SIGINT thing.

   Meh, let's forget about it.
* hmm
  so we seem to be having some memory-reading issues, we're reading out random memory.

  I believe we are at least reading the right addresses,
  but maybe due to races, things are not set right.

  like we're reading out of order somehow
* generic solution
  OK so let's make an object base class thing,
  which can be inherited from,
  which provides the feature of,
  running an async self.run method and,
  delivering other method calls as messages to that.

  So that self.run can batch things together and all that.

  And also, so that when the method call that is currently running self.run gets cancelled,
  that doesn't stop self.run, but instead self.run just stops running,
  and is run again by some other method call when one comes around.

  Incoming method calls can be represented by objects,
  with a method on them to return from them.

  When a cancellation happens, the method call object will have a flag set,
  but no other change will happen.

  Normal awaits can happen normally:
  They'll just be propagated up through.
  (with special handling to avoid cancels)

  It's only returns that need to be special,
  We'll do a return on the object,
  which will set a flag then yield up.
  That flag will be checked before processing yields,
  and if it's set then we intercept the yield as the return value.

  So then now we need to think about the other side:
  what if I'm some other task which doesn't want to be cancelled,
  and I call into this uncancelling object?
  I want my method call to persist even if I'm cancelled above...

  Oh!
  Doesn't it just work, though?
  When I get cancelled, I get suspended,
  and the cancel doesn't propagate down into the objects I'm calling into.

  Well... there's the issue that,
  done naively,
  I'll get suspended - which will suspend the method I'm calling into.

  Which means I'll be blocking others.

  Hmm, very tricky...

  We want the functions I call into to continue running. Whatever that means. Hm.
** thoughts
  So what exactly is going on here?

  We're some thread calling into some object,
  and we trust that the other thread that has the lock and is inside the critical section is going to make forward progress.

  But, it might be permanently descheduled!
  So, what do we do?

  Hmmmmm

  This is kind of an issue of priority inheritance I suppose.

  So how do we deal with this in other circumstance?

  Well, when we get cancelled, um.

  I see, we can, um.

  Wait, okay, so.

  When we make a real call,

  we do the yield up ourselves.

  And if we get cancelled,

  we save it and do it elsewhere. Right...?

  We just do it... again??

  Can we really sustain that?

  So okay, someone beneath us is making some call with some effect,
  and we're seeing the effect in our handler loop.
  If it throws an exception or returns a value,
  we re-inject that into the generator.

  I guess we could then catch cancelled,
  and suspend ourselves, by throwing that up to the user. Hmm.

  Right, so we get the cancelled,
  and we skip over our call stack,
  sending it straight back to the user.

  Which makes sense.

  So that's all good, if we're a server,
  and we don't want to die when a caller is cancelled.

  But what if we're a user,
  and we're calling a server,
  and we don't want our call to the server to be interrupted,
  just because we were cancelled?

  In other words.

  Hmm.

  So okay, with the approach of,
  we get a cancelled from it,
  and propagate it up, skipping us.

  How else are we going to learn that we got cancelled?

  Essentially, the idea is,
  some exceptions only terminate a specific request,
  and not all of them.

  But, the problem is.
  when user A sends a request to server B,
  which sends a request to server C,
  then user A gets cancelled,
  then server C sees that cancellation,
  it cancels server B's request to C.

  But we want that request to stay around.

  That request is not *really* cancelled.

  So maybe we want to make the request under a cancel guard,
  and have another thing waiting to receive the cancellation exception.

  But, wait, wait.

  While the request is not really cancelled,
  we really might need to schedule it onto another thread.

  Hmm.

  That is to say...

  When this exception is injected,
  it's an indication that we should stop running.

  Um, hm.

  More specifically, we will indeed, stop running.
  If we propagate the return value up.

  So, okay.

  If I return something up,
  then, obviously I can't also be in the middle of a call to something else.
  With a nursery or whatever.

  So the call to something else has to terminate.
  And be resumed, I suppose.

  So, okay, I can have this completion thing.

  Basically have the methods return a future or whatever.

  Which I can await on to get my thing.

  Yeah I mean, that's easiest, innit.

  Plus there's nice symmetry - the user awaits fut.get(), the server awaits fut.return().

  And then if the .get() is cancelled,
  we can always later go and call .get() again.

  Most will use a wrapper which auto-cancels the fut on cancellation up. I guess. Hm.

  Blah, okay, so the issue really is just,
  what if the guy whose responsible for running, isn't running?

  Can we solve that without fancy resuming stuff?

  Well so

  I guess
  Um

  instead of even having the cancelled propagate down to us,
  we screen it off at the method level.

  We get the cancelled exception,
  we stop running the thread,
  and we 

  We cancel that specific request,
  and stop running the thread

  ugh trio you are shit, it's too complicated for all this.

  Suppose we screened off the cancelled at the method level.
  We suspend the thread underneath us - it never runs again.

  If it has a lock, what do we do?

  Looks like we would indeed want to do some priority inheritance. Blah.

  Blah this is all so complicated.

  Bah okay

  Thread A calls object B which calls object C.

  Thread A gets cancelled, and stops calling into object B.

  Object B's state stays around, so it is still the designated caller for object C.
  It still holds the lock.

  How do we deal with this?

  I don't, in fact, want to unwind the stack,
  because I want to keep all the state around.

  So I don't want to execute a bunch of handlers to release locks or whatever.

  Instead. Hmm.

  Object B holds the lock for object C.
  It should really give them up.

  Blah this is tricky.
** summary
  OK, so.

  I have a system where objects have a coroutine inside them,
  which receives (blocking) method calls from the outside world as requests,
  and processes them,
  and sends responses back,
  possibly processing method calls in batches, possibly responding to method calls out of order, etc.

  This is implemented as a library, without requiring a global scheduler in the runtime.
  So, the coroutine inside an object only runs when there's at least one active method call from some thread;
  the coroutine runs directly on the thread that's making the method call.

  Now, suppose I have thread A, calling into object B, which in turn is calling into object C.

  Suppose I kill off thread A.

  How... how do I handle that...?
  Well if I just flat-out kill it, then y'know, nothing works.

  Maybe I want to steal things from the kill-safe abstractions paper, hm.
  What did they do?

  Oh, wait.
  Maybe they don't handle the issue of,
  I have thread A calling B calling C,
  and X calling C also.
  And A suspends, so B doesn't run.
  Hmm.

  Like, I need to somehow notify at the B-C interface to reparent to X.

  I guess the notification would be that B is not running or something. Hmm. But that's ugly.

  Like, I want to be fully generic, right?

  If, um.

  So say we're in B, and we're calling C.

  Doesn't C learn when the thread dies?
  Like, it can see cancellations?

  Maybe it should just see that,
  and reparent to something else.

  That works, *if* we're willing to interrupt the method call.
  Because interrupting the method call is the straightforward way to signal that,
  this method call isn't going to run you any more.

  how else can I inform it, hmm.

  oh also note, like, what if the thread above us gets stuck in a lock somewhere else or something, right?
  can that happen?

  well no we're directly running on that thread, right?

  therefore that can't happen

  well I guess the thread could be holding a lock, um.

  well we're blocked until this operation is done, so...
** again, summary
  I have a system where objects have a coroutine inside them,
  which receives (blocking) method calls from the outside world as requests,
  and processes them,
  and sends responses back,
  possibly processing method calls in batches, possibly responding to method calls out of order, etc.

  This is implemented as a library, without requiring a global scheduler in the runtime.
  So, the coroutine inside an object only runs when there's at least one active method call from some thread;
  the coroutine runs directly on the thread that's making the method call.

  Now, suppose I have thread A, calling into object B, which in turn is calling into object C.

  And suppose I kill off thread A.

  How?
  POSIX cancellation semantics...?

  I guess I can inject an exception into it,
  which will unwind up the whole thing.

  Or I suppose I can just stop running it ever again.
  Yeah sure let's suppose I just stop running it.
  How do I clean up?
*** just execute atomically
    this doesn't work because we do actually need to be able to cancel
*** primitive
    ok so what if I just have the obvious system of invoking stacks and all that

    stack A invokes stack B which invokes stack C.

    if stack C yields up to stack K,
    and stack K decides not to re-invoke stack C.
    then what do?

    I guess what we'd need to do is,
    stack C would also yield up, like, what?
    all the things necessary to unwind?

    well, okay, so one approach is for K to re-invoke stack C with an exception.

    then stack C can invoke stack B with the same exception,
    which can invoke stack A with that exception,
    all good.

    except, this annoyingly requires us to re-invoke a stack to do unwinding.

    hm.

    also I guess problematically we have to retry everywhere in those stacks.

    we'd rather not retry, right?

    so stack B shouldn't even change just because stack C is running elsewhere. hmm.

    well. hmm.

    this is kinda priority inheritance isn't it?

    we just never run A/B/C again, sure.

    except someone else is waiting for C, so we need to run it. um.

    wait wait okay so.

    stack B shouldn't change just because we need to run stack C on top of a different thread/stack.

    well, so, we're in K, and we run something else X.
    and X wants to call into C.
    or maybe even X is already on some lock in C.

    well, let's go with the "X wants to call into C" approach.
    X wants to call into C,
    so um,
    it goes and, um, tells K to continue running C until it gets to a point where it's handled whatever,
    and can return.

    I guess when C yields up to B,
    it has to go through K,
    so that K can redirect the thing away.

    well okay so the notion is, right, that,

    we've got some A0, which starts A1, A2, etc.

    And it multiplexes between them.

    And it eventually decides to kill A1

    But A1 is inside B and C.

    And we want to free up resources.

    Ugh, and the standard solution is throwing an exception up.

    But we don't want to destroy B and C's state.

    I guess if we want to give B and C the chance to actually interrupt their processing, then we do. argh.

    Hmm. Isn't cancelled just, like, eintr?

    Maybe we just want some kinda notion of cancellation um...

    This is all nuts and hard.
    Let's just do it with stack ripping.

    goooooooooooooooooooooood damn it

    blah
    ok!
    stack ripping it is.
* readlink -f
  so why do we need this again?
  why can't we just open the file?

  well...

  nix-store sticks the thing on the remote filesystem,
  as a tarball.

  then we extract that

  and that creates some stuff on the remote filesystem.

  so ok so we can probably pass down a file to nix-store --export or --requisites.

  then basically that dumps a list of filesystem paths we need on the remote system to use that file

  then, I suppose if we're really committed, we can just copy the file across???

  we just copy the file to the remote host, to anywhere,
  and then use it. blah.

  but it also needs to be in the right place, so...

  so it doesn't even work

  hmmmmmmmmmmmmmmmmmmmmmmmmmm
** obvious means using proc
   open the path with O_PATH,
   and readlink(/proc/self/fd/n)

   or readlinkat(n, "")?
   don't think that will work, but maybe...

   the readlinkat(n, "") doesn't seem to work
* doing stuff
  we want to start supporting multiple run_nix calls

  we did it, cool

  now I wanna know what other miscellaneous small things have to be done to deploy Nix


  OK, I need to support sudo it looks like.

  As expected, I suppose!

  Annoyingly I'll have to type in my password.

  But I probably can share some stuff with the ssh bootstrap maybe?
* sudo bootstrap

  So it can be similar but probably simpler.

  We just need to run...

  OK so -C/closefrom_override is not supported by default on Debian so let's not bother with it.

  We'll *probably* need, um....

  Oh wait, does sudo 

  sudo passes stdin/stdout/stderr verbatim!

  So we could just pass a socket and then do socket-fd-passing.

  Let's do that. Then we can avoid touching the filesystem.
** simple scenario
   Just start a new bootstrap process under sudo,
   pass it a socketpair as stdin and inherit stdout/stderr,
   and send the additional channels over SCM_RIGHTS
** interesting thought
   what if we could sidestep all this separate process stuff,
   and just use the root-powers to force-setuid one of our existing threads?

   that would be convenient:
   we could share fd table and address space.

   is there anything we could use for this?

   ptrace perhaps?

   so if we start as root we can setuid to whatever, of course.

   and if we exec, of course we can gain root through the setuid bit.

   but I guess I want to elevate privileges some other way.
** ways to elevate privileges of other threads
   it looks like setcap used to have this ability

   oh that's right I need to be unprivileged anyway. hmmmmmmMMM

   that means the user I'm sudoing to, doesn't have authority to modify my own threads anyway. hmm.

   so I want to have a task of user A, be able to setuid to user B, by cooperating with a task of user B.
   hm.

   oh I could also use sudo to just make a setuid binary.
   that could be nice, hm.
   but that's very sketchy.

   hmm, with two unprivileged users cooperating,
   is there a way for a task of user A to setuid to user B without the task having to exec and lose its state?

   how would I design it if I had the choice?

   well, I guess it would be something like setns, right?

   although setns doesn't grant caps, which is why it's unpriv'd...

   setns is kinda dumb really
** userns stuff
   Hmm.

   If you enter a userns, root can put other users in the uid_map,
   and then you can setuid to those users.

   You have to stay inside the userns, but that's not a super huge issue.

   hmm so if I had some other user create the userns, and write to the uid_map.

   the same user has to both create and write to uid_map because of the restriction that,
   "The writing process must have the same effective user ID as the process that created the user namespace."

   how could I then enter that userns and get privs and stuff?

   hmm

   what about even getting two user processes in the same user namespace?

   it's tricky, oh so tricky.

   i probably can't do it

   blah
** sudo
   ok let's just do the bootstrap thing

   bootstrap process, socketpair as stdin, send channels over SCM_RIGHTS

   I guess, hmm.

   we can't pass fds down but we can pass arguments

   so we can say, here's how many fds to read off.

   and it reads off that many, then writes back the fd numbers,
   then.

   i guess we can write um...

   oh, it could just start using stdin as the syscall connection.

   wait no, bad idea, we want to tell it which one to use,
   because we want to have a direct connection.

   so yeah I guess what we'll do is,
   we'll,
   write back the fd numbers to use as infd and outfd.
* stuff
  okay so we mostly can just do things in the SyscallInterface, and GC them away

  but what about differentiating things we can and can't exec in?

  well, how would we exec in a ssh scenario?

  Like, how would we make our own ssh which works nice with rsyscall and supports execing?

  So I guess one way is the standard thread server approach.

  How would we then be able to exec in one of those threads?

  So if it was local, we could move the futex around and start a thread monitoring it.

  But what if execing was a child event, as I think it should maybe be?

  Well we wouldn't be the parent of these threads in the thread server...

  So...

  I guess we could serialize out the child events in an fd.

  We could write out the futex event too I suppose.

  That would be much like a childfd/procfd thing.

  Kinda related to the persistent server...
  Oh but for the persistent server, I just gave up and actually reconnected to the same thread.
  Which is better anyway.
  Then I don't need CLONE_THREAD.

  So anyway, the thread server...
  What is its... PURPOSE....

  Starting a process can only be done if you already have a process running.
  I guess the thread server is what I'd run under a user account when I want to support others accessing it.

  Basically, um...
  Well, we can achieve "let someone enter this namespace or this user" using setns kind of stuff, and really that's better.

  But if we really want the last word in creating a thread in some existing environment,
  we want the thread server.

  The thread server is fine ideally anyway - ideally you can move all your resources over to the thread created by the server.

  So yeah, it's this thing that sits there, and...
  
  Ideally for local things we just pass a childfd around,
  then for remote things we have something which serializes the childfd to the remote side.

  So the question is, do we have this server which serializes the child events to the remote side?
  Or do we just have an actual task that we can control, but which we aren't the parent of?

  So, on one side, it makes sense to serialize fds to send over the network, that's fine.

  On the other hand, having a task which we control but aren't the parent of, is something we have in every program:
  The root of the task tree, the Python process.

  On the other other hand, if the root exits, or execs, our code terminates.
  In other words, we can assume that we can track root exiting or execing even though we can't,
  because we'll never run in a scenario where root has exited or exec'd.

  So the root is special in that regard.
** reminder of what I'm doing
   I'm trying to make it possible to exec in an ssh server,
   so we don't need to have different classes of things which can and can't exec.

   Which... is fine...

   Although wait a second, how will I get the ChildProcess without this stuff?

   I guess the ChildProcess could be on the SyscallInterface..?
** blah!
   let's not change anything, it works fine as is!
* why can't we use the syscall channel as the data channel?
  Because of the possibility of partial reads and writes.

  Partial reads can be dealt with: we just use recvmsg(MSG_WAITALL) and everything's good.

  But there's no corresponding flag for sendmsg to deal with partial writes.
  And I don't see a way to work around it - we'll get the syscall response in the middle of what we think is a bunch of data.

  The partial reads or writes are kind of neat anyway, it means theoretically we could add more data mid-transfer, if we get a partial read or write.
* stuff

async def make_execable(
        child_process: ChildProcess,
        child_stdtask: StandardTask,
        child_memfd: FileDescriptor,
        monitor_stdtask: StandardTask,
        monitor_memfd: FileDescriptor,
) -> RsyscallThread:
    # map the memfd in child and monitor
    # set robust futex in child
    # launch futex monitor from task
    # return thread
    # TODO hmm, note that we can use CHILD_CLEARTID if we're in the same address space...
    # hmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm
    # we want to detect when it leaves our address space, basically
    # so we need this code so that we can run arbitrary code in it
    # so that if it leaves our address space (for example by execing) we can free the resources.
    # I know all this...
    # In particular we want to be able to know when it leaves its address space,
    # even if it's the only thing there.
    # That way, um...
    # well, for instance, that way we know whether an exec actually succeeded!
    # C'mon! It's pretty straightforward!
    # the fact that any thread can monitor for another thread's exec is cool...
    # using child_cleartid for posix_spawn would be cool...
    # I guess you could do it with vfork though...
    # just posix_spawn with CLONE_VFORK|CLONE_CHILD_CLEARTID,
    # and if the exec is successful then the tid is cleared,
    # otherwise there was a failure before exec.
    # but you'd want to be able to do this even with full fork!
    # because, because!
    # well, hm. if it's got... child_cleartid...
    # blllllaaaaaaaaaaaaaaaaaaahhhhhhhhhhhh
    # starting a thread is annoying and expensive
    # ok. probably it will be at least two years until I am able to fix this.
    # but, argh! what if I think of a better solution?
    # the issue is all just the fact that I need to make a futex monitoring thread.
    # the fact that I'm using futexes is minor
    # the cleartid thing is fine, other than that it doesn't work when not sharing memory,
    # which I can deal with by using the robust list.
    # mmmmmmmmmmmmmmmmmmmmm
    # blah
    # so exec will hang 4ever mmm
    # how do we tell the difference between dying from a signal or something, and really execing tho
    # dunno, guess we can't.
    # blah okay so I guess I should probably not bake in the futex thread
    # I should support a notion of something that is my child,
    # but not futex-thready.
    # that's possible with ssh, and exec.
    # should I claim it's my child, when it's not actually my direct child, though?
    # for ssh we need to put the forwarding in another process anyway, blah.
    # actually we don't even need to support the child thing, um, hmm.
    # if it's my child and there are no other resources, I can just stick it into the SyscallInterface
    # let's just defer this problem:
    # we know we want this make_execable thing, so let's do that.
    # a thread is actually just a child task that we can watch for exec on... I guess...
    pass
* hmm troubles
  so we aren't inheriting the transport, which makes exec not work.
  because the transport isn't present in the unshared address space.

  let's figure this out.

  okay so yeah for each address space we need a transport,
  and that transport is only driven by one task in that address space

  fixed!
* now
  let's switch to using the datafd instead of describe.

  ok done
* connecting
  soooo

  I guess ummm

  well...

  Ah we'll just pass down the connecting connection, then close the socketpair.
* bytes vs string
  I should use str more because bytes displays awkwardly on the command line.
* testing sudo
  hmm not sure how we can test sudo, hmm.

  what if we sudo to our own user?

  hmm no it looks like sudo really want to be owned by 0 and have setuid bit set

  oh, we could do that with user namespaces maybe!

  next problem: how does the user actually input the password?

  hmmmmmmmmmmmmm

  I'd like for the password to never be in my memory...

  The obvious way is to prompt for it.

  And, maybe, store it?

  oh god damn it, sudo will read the password from stdin...........

  which is what I'm using for passing fds. ugh anyway, let's focus on the password thing

  ok... so...

  I guess what I could do is, go ahead and do it myself.

  Prompt for the password and enter it in.

  Blah.

  OK so if we just support some kinda authentication thing.

  Where, y'know, the password for the sudo is in our python process,
  and streamed over to sudo.

  And we prompt for it.

  Or, rather, we just let the user handle it I guess.

  We optionally provide the password.

  Uggghhh I'm scared of that

  Then I can have security problems.

  OK sooooooooooooooo

  So I could do something factotum style...

  How do people robustly handle sudo in deployment scripts?!

  I guess I could query a password manager, hm..

  I guess I could have an fd containing the password.

  Then, um, that fd can be provided however I want.

  I guess we can do it interactively... read the prompt...

  right, how do we... detect that a password is needed...

  I guess we monitor stderr...
  and if we see a password prompt we write the password to stdin.
  but then that requires that I know the password...

  and pass the bootstrap socketpair on stdout.

  alternatively I could use askpass I guess.

  I guess that's how I could get a callback.

  seems good.

  I suppose that notion of, "exec this program to callback into me!" is fine. . .

  It could be cool.
  We could have one per instance.

  It could access a socket, I guess.

  What exactly is its protocol?

  There's a message, and then we write the password to stdout.
  Blah.
  
  not particularly structured...

  doing it with stdin would be nice but we don't know when the password is not needed, blah

  I guess I could just always write the provided password, and then close stdin when done.
* give up??
hmmmmmmmmmmmmmmm all this distributed execution stuff is troublingly hard
maybe I should have a demonstration case that isn't remote
I already got ssh working, so I know it can be done
and I don't really care about sudo

so then I should find a use case to test on

My real use case is running TS software for integration.

But I don't think I'm quite ready for that.

So instead I want to run something big and complex on my local system,
which is open source...

Hmm...

Even running all the services in a system, like systemd, is too simple.

Well. Actually that's a bit too strong to say.

Maybe I should go ahead and do the, run all my stuff, thing.

well for the most part I work inside TS, so...

hmmmmmmmmmmmmmmmmmmmmmmmm

well I can run emacs, um....

the tricky thing is that there's no big complicated system to demonstrate a supervisor system

hey s6, is there any relatively complicated system that demonstrates the full functionality of the s6 suite?
or at least a lot of functionality; 
like, setting up stateful directories and configuration,
supervising several applications with s6,
launching things with s6-tcpserver-socketbinder, etc


hmmmmmmmmmmmmm I guess I could run as an init system, hmmmmmmmmmmm

I could run a container and stuff.


maybe I need to just look at some ancient hairy Unix service

like NFS or Kerberos or LVM

I should do one that I actually care about though, hmm.

a mailserver?

hmmmmmmmmmmmmmmMMMMMmm

I could do a pretty rich mailserver, with DNS authoritative being delegated to me...

...except SMTP SRV records aren't actually used

okay, how about public-inbox then?

I could host NNTP, hmm.

OK, I care about public-inbox for Nix and stuff.
I'll set it up for nix-devel.

Oh, also Discourse I guess...

I guess I can have Discourse 

I need to set up my mailserver again so I can properly participate.

okay... okay!!!

set all dis up!!!

we'll have, hmm.
basically, hmm.

We'll run the mail server,
and it'll deliver to dovecot I guess,
and also invoke procmail to,
fill up the public-inbox.

The public-inbox will be public,
and the IMAP will be private.

yes this is a perfect thing to set up with rsyscall

okay, so how do I handle the privileged ports?

I'd prefer to do it with the SRV records, so let's just patch all my software to do that.

We can't do that for SMTP, though. So what will we do there?

Well, we want to request the SMTP port from some system-wide thing.

I can do that with authbind, but that requires setuid - gross.

Instead perhaps I'll write a small service which accepts fds and calls bind on them.
It would be very cute I guess.

It would just accept:
1. An fd.
2. A length.
3. That many bytes of address.

And then call bind with that.

Oh but I guess we want to validate the bind.
Curious, hmm.

I guess we just want to bind to 0,
and some port.

I suppose we only support this for AF_INET and AF_INET6.

Well, how's authbind work?

Oh, clever, it uses permissions. Hm!

How could we represent a cap?

okay so we thought about this a lot, innit.

connect and bind and things.

connect to a place,

should be the same as bindat

we should have an fd for a thing we can connect to;
just as we have an fd we can accept on.

hmm.

we call connect on an fd and get a new fd out.

I guess that fd represents a host and port.
Or a service or something.

Where do we get that fd from?

Because that's isomorphic to where we get the fd that we call accept on from.

Hmm.

We *could* just have a socketpair and pass fds along it for each new connection.

But that's a bit silly, better to just pass the listening fd.

Alternatively we could just have some identified service and, y'know, you just send the name.

Well, hm, I do like this notion of,
just use filesystem permissions.

We can just have a unix socket,
which when you connect to it,
just sends you the listening socket and hangs up.

That's fun.

Meh! Let's do it!

Wait, couldn't I just use flower directly??

hm m mm

postfix really wants root eh

hmmmmm
mail is annoying though, hmm...

no! mail is rad!
remember how awesome it was to just use that mailing list just now?
that was awesome.

and this is a big large component

It deserves the same effort as I put into TS crapware.
* new thought
  okay! it's obvious what I should do!

  I should integration test some open source software!

  A mailserver would be easy, send some mails, all that stuff! Awesome!

  For more easier stuff, we can do, a, webapp! Some random open source webapp!

  hmm, what's a good random open source webapp?

  Nix people suggested Hydra. That would be cool.

  Maybe even useful...

  I could start running it internally, or something...

  Also suggested: Gitlab, matrix-synapse.

  Don't really care about either of those.

  Let's do Hydra.
* how to configure
  so obviously we should have this like, a single user is running it,
  and they have some source control for the configuration,
  and all that.

  we can do the same kind of sync tool as COIN has, I suppose...
  we organize things in a hierarchy then sync them...

  updating things via the rest API

  configuration via plain text is better

  if we want a configuration web interface API it should be separate, I guess

  we just, restart it, when we want to change config. no biggie.

  i guess we can support, reloading, y'know

  okay no we'll use the actual interfaces it provides:
  mutability

  and we'll have a JobConfig object that we can submit to make a Job I guess
* hm, callback executable thing
  So a generic tool that execs and then calls back to us,
  and gives us control over it,
  seems like it would be useful.

  This would be a pretty cool tool in general for testing actually.
  You could easily mock an executable.

  So okay, how do we do this robustly?

  OK we'll just have a binary,
  which takes an env var containing a socket path,
  and conencts to that socket and passes control over it.

  Then we can pass down that env var if that is okay,
  and if not we can wrap the binary with a script to set the env var.

  We should probably also do this instead of the sudo stdin thing.
  The stdin thing is too spooky.

  The ssh bootstrap is similar too, hmm.
  It relies on being in the right directory though.

  Well, it's backwards actually.

  It accepts instead of connecting.
  I did that for efficiency reasons, and also so that the listening fd could be inherited,
  but maybe it would be better to connect...

  If we can uniformly connect, that's really cool and powerful.
  We might only need one binary for sudo, ssh, and generic callbacking!

  OK so suppose ssh did connect. Still we'd have the issue of needing to specify the socket to connect to.
  But I guess that's easy:
  We can do that in the shell wrapper run by ssh.

  (oh well with ssh the ideal end goal is to not use a listening socket in the filesystem at all,
  so that would be a lot better yes...)
** name
  what to call it?

  rsyscall_remote_

  slave really is the best term!

  follower isn't right

  secondary, replica, both are domain-specific and wrong.

  client? yeah let's just call it client?

  well it's a client that is just a stub for us.

  how about stub then?
  rsyscall_stub?

  incarnation?

  we run it, and it connects back, and it is filled with the light of rsyscall

  it's like a, shell? in the sense of, it's hollow?

  conduit?

  agent?

  minion?

  stub, I think.
** stuff
  hmm

  this is a good example of a time when the exec monitoring not being tied to being the child,
  is really good.

  since it'll let us monitor this guy for execing. . .

  ffs

  /bin/sh doesn't support -a

  fine, fine. we'll just pass the full args.
* inotify near/far
  So okay, we would have far.WD(File, near.WD),
  and far.FD(FDTable, near.FD)

  Then for handles, we do:
  File -> FD
  FDTable -> Task

  But the far code, basically, we have to provide those references ourselves.

  Well um theoretically we could um

  Well if we want to reference some FD table, we want to be attached to that, not the task, innit...

  Also the question is, what does a WD actually reference? A "watch", sure, but..
  I guess that's an inode?
* memory or handle on top
  hmmm so for handle.FD methods to be usable,
  we need an additional thing to handle pointers.

  right? cuz.

  if the memsys uses the handle syscalls,
  then it needs more stuff to support memory allocation and GC.

  hmm.

  but if handle uses memsys,
  then handle stuff needs to, um, have memory abstraction stuff below it.

  but that's not good because we want memory stuff to be using handle.

  hmm.
  handle can provide mmap-level memory allocation helper things.

  and more realistic/efficient allocators can be built on top.

  also remember, pointers should be inside a memory mapping! that's cool!
  and paths inside mounts!
  so explicit y'know

  suppose we did have that kind of thing in the handle API, that would be fine.

  and then on top of it we'd do memory allocation, fine.

  right right the memsys thing would get passed um

  a task? and an fd?

  I guess the fact that handle contains a task is only a convenience.

  tis not something to rely upon, I suppose!
  maybe.
  whatever.
* nginx
    # so how will we manage nice flexible nginx?
    # if we want to add new ways to forward to an existing server,
    # we can upgrade the configuration.
    # if we want to add new listening sockets,
    # then we use an entirely different server.
    # simple as!
    # we could achieve adding new listening sockets with one server by,
    # making the new config,
    # HUPing the old nginx,
    # and passing its listening sockets to the new one.
    # that requires we keep the listening sockets around though, hmm
    # that makes sense I think
    # ok so we can start nginx with no servers I guess
    # oh wait if we have no servers we can just do nothing :) not start it
    # then, so, when someone calls a method to add a server,
    # we um.
    # we start the new one with all our sockets,
    # and HUP (or QUIT?) the old one
    # well, HUP at first, then QUIT I guess? hmm.
    # we'd like to surface what's really going on to the user...
    # the ideal thing would be, I guess, some server which we don't need to restart,
    # which we can send new socket connections flexibly over IPC?
    # well - that would be very monolithic
    # it's fine to use different processes for each!
    ## okay
    # okay so the API is just,
    # you can create an Nginx process with some number of listening sockets addresses servers, etc
    # and if you want to add more servers, you have to create a new one with the sockets you,
    # presumably, saved.
    # and tell the old server to should down gracefully with the method.
    # new forwardings, though, can happen by mutating an existing server -
    # you can update the config and tell it to reload, no problem
    ## hm
    # hmm so there doesn't seem to be a clear mapping between a listen directive and a listening socket
    # so maybe we'll need to have some NginxConfig object which we can stick listening sockets in,
    # and get back out something we can use for a listen directive,
    # to ensure that the sharing works
    # yeah so we'll have a nice API that takes a listening socket and makes it some object,
    # then you use that in other calls
* fixing hydra
  OK hmmmm

  the google login

  not exactly sure how this works, hmm.

  waaait a second

  wait a second, hydra isn't vulnerable to a CSRF at all!

  it does this via JSON!

  right? hmm.

  How interesting...

  OK so maybe we can check the existence X-Requested-With header? or, hm.
  doing that would require specifying that in the rest API
  it would be better to check content-type json?

  hmmmmmmmm content-type json is dubious protection

  ugh what about signing out

  ugh this flash stuff allows bypassing all these protections

  a token really is the best

  ughghhhhhg

  Origin doesn't seem to be reliable apparently

  and this Same-Site cookies thing is too recent

  but a token doesn't work for login

  ahhhhhhhhhhhhhhhh

  OK, Content-Type seems like the best thing

  Because a token doesn't really work for login: you need a second step

  right?

  yeah.

  because, if you do it before login,
  they can just query,
  and get the token,
  and then include it.

  you need a double-submit.

  ok ok so let's do the JSON check

  ok finally done
* getting path literal
  Hmm so it looks like only things which define a storePath are on -I things.

  HmMm

  So I guess NIX_PATH/-I only contains literals

  So we'd need to, um, we'd need to evaluate something ahead of time.

  Hm.

  I guess a more general approach would be,
  instead of taking a NIX_PATH thing in hydra-eval-jobs,
  instead just,
  take an expression and evaluate it.

  And the expression would be in the jobset configuration.

  Ughhhhh

  And it would usually just be import <foo/bar>

  But we could do more.

  OK so we want to be able to do a build without depending on any network things.

  well. then what's the point? it's pure.

  well we can evaluate it with arbitrary params.

  basically, yeah, the entire description is literally included in the inputs. Hm.

  Well, alternatively I could have a dummy path input thing plugin.
  Which just looks at an argument, and... evals... that?

  OK let's just use paths, ffs
* cleaning up leaking processes
  OK so let's do it with pid namespaces, that's clearly best

  Even if we appear as root or something, it's fine.

  I think

  OK so the major issue is the child reparenting.

  The API will just be, fork into a new user namespace.

  The main issue is the inherent raciness of child spawning and monitoring.

  So how can we get things to work, when we can have children be reparented to us at any moment?

  Haha what if we use CLONE_PARENT as pid 1.

  That would be hilarious and great.

  I guess we'd need a separate child process monitor to clean up zombies...

  hey can't we just track all events while we're cloning,
  and then the last one we get is the one for our child?

  wait a second we don't even get events while cloning, tho

  wait wait, how are we even cloning in a child process

  oh, we don't lock clone and wait against each other, right

  so we track every wait event,
  and then store them.
  (and purge them when nothing is cloning)

  then we don't need any locking at all? maybe?
** other race actually even with ctid
  I think even with ctid we have races.

  Cuz, when do you check the ctid pointer? If it's after wait, then the event you got could be,
  from some other process,
  and then you get a pid wrap,
  and check the ctid pointer.

  But I guess this is related to reparenting, hmm.
** oh
   oh no my approach doesn't work

   even if we store every event,
   that doesn't help.

   because, when we clone,
   if we're waiting simultaneously,
   we can get bad events.

   well okay so if we lock waiting against cloning,
   so that we never wait and clone simultaneously,
   but can do both as much as we like if we're doing just one.

   then everything is fine?

   but blah I want to support reparaneting

   I hope I can clone_parent

   But yeah we'll just have fork take additional flags for user and pid namespace unsharing.


   ugh okay so we can't use clone_parent as init in the pidns.
* bug?
  some kind of weird issue where sigign is set

  That is to say, signal(SIGCHLD, SIG_IGN) seems to be set
  when I kill the pid namespace child.

  Hmm.

  Guess I gotta write a C program to reproduce.

  hmmm could it be related to sighand?

  ok i can just disable sighand and bug goes away, so ignoring it.
* ok how do I use it now
  Oh I probably can actually use nginx without the intermediary.

  Cuz nginx will reap its own stuff.

  So yeah let's exec it
* uid map thing
  HMmmm

  So Nix, when it's setting up the build sandbox,
  opens the /proc/pid/uid_map entry for its child,
  by *pid*.
  Hmm.
* ok
  ok ok ok

  so all this is fixed and good

  we have a nice integration test for hydra, with just a few missing things.

  I guess I should figure out a solution to this tmpdir issue.

  I like what I have at TS quite a lot.

  but that's kinda boring so nevermind it.

  like, i want some external thing to decide whether the test should take place in a persistent directory.

  and also handle cleaning it up.

  mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm

  i guess tmpdir is a useful thing that we can set.

  but then how do we persist?

  also tmpdir gets set badly by nix-shell

  ok so that's fixed now.

  so we can use tmpdir. hmm.

  and then how do we decide whether to persist?

  well, i guess we can have an environment flag which says whether or not to persist.

  and we don't persist by default.

  should we set a sub-tmpdir in the test?

  hmm, that is a bit tempting.

  hmm, we can't do sequentially incrementing build files cuz that's tricky.

  We can do the symlink to latest though.

  Is there a trick we can do to keep them ordered?

  What if we put a timestamp into the filenames?

  In addition to a randomizing bit, of course.

  OK so that sounds fun and good.

  So what kind of timestamp do we want?

  Seconds since epoch?

  Hour, minute, and second?

  Do we want millis or less? No, I don't think we need milli resolution.

  If we do seconds since epoch, then we'll have year.

  hmm I want to put a helper timestamp on these test run directories, which doesn't need to be precise

  it's just so that there's a number which increments on the test run,
  so you can see approximately which order the directories were created in.

  hmm wait a second, I get that from ctime

  well so that gives me the order

  but if I have two directory paths, I don't know what order I ran them in

  but do I really need that?

  OK, let's just stamp it with seconds since start of day.

  That costs nothing and gives us some useful stuff.

  OK, so the name will be something like:

  run.68920.1z4KTC

  I'm fine with that.

  We can maybe even shorten the random bits.

  Ah looks even better now.

  So we'll mod the current time by the number of seconds in the day.

  Let's not bother with the syscall, it's just a helper, and it's a bit pointless.

  Actually.

  Let's do it, just do it on local_stdtask always.

  No maybe I shouldn't do it in the first place.

  Ugh but so useful.

  No let's not do it. We'll just have current.

  And I can look at ctime if I really want to.

  Make symlink in tmp as well, with specific name

  Maybe don't even delete them?

Or maybe don't delete them if the test fails?

No, no, delete them, c'mon.

And we'll have an env var to say don't delete.

Or something. It's not a big deal.
* hmm removing unwritable directories
  how to remove directories containing -w files?

  very tricky

b l a r g h

I wish there was some place that I could ask for nice, Linux-specific C programming tips

should I just use sh -c chmod rm?

This is a reason not to clean up after myself.

No let's just do it.

And uh, I guess we need a DSL thingy maybe?

b l a h

let's just write it down!

ok easily fixed with a shell snippet
* mail
  okay so an individual user can run dovecot.

  that sounds more secure and nice.
  
  so we'll do that.

  and. hmm.

  maybe we should run dovecot for the system as well?

  oh hmm we could run dovecot with local IMAP for downloading the mail to users?

  hmm that allows infinite queue length, not good

  I guess my philosophy would be for the user to run some server that accepts mail,
  and puts it into the home directory.

  hmm! we could run Dovecot for each user,
  and connect to them with LMAP?

  Hmm so really we want one thing which receives email from the internet,
  and delivers it internally.

  And one thing which receive local email,
  and delivers it to the internet.

  Oh but local email can need to go internal. I see.

  But we want to rule out also the possibility of receive email from the internet,
  and sending it to the internet...

  Oh, but, actually that's fine I guess:
  That's what we use to send email from another client.

  Hmmmm

  I guess what would be nice is domain-based SRV-recordy thing

  Oh right SMTP works with MX records, so port 25 really is needed.

  Mail is delivered to a domain after all.

  So OK, thing on the server has got the certificate for receiving mail.

  It receives the mail, then sends it over to individual users. Over... LMTP?

  OK yeah this is simple and easy.

  We'll deliver mail to users over LMTP.
  Each user runs an LMTP server (Dovecot I guess).

  And we can run mailman too, presenting an LMTP server.

  Or deliver mail to public-inbox.
  Which is nicer than traditional mailing lists anyway.

  How is mail submitted?
  Hmm.

  I could force all submission to go through SMTP. Sounds weird.
* conversation
that's a very defensible and modern position,
in a world where most systems are single-user,
and have to go to another system to actually submit mail.
but the thing is that I'm trying to set up a multi-user system


interesting point about Kerb, yeah

no, there's definitely a third alternative there: using Unix permissions to control access to the submission socket. you can optimize this alternative further to only have a single submission socket that everyone can access, if you can do a SCM_CREDENTIALS handshake in your protocol
* mail transfer agent

why???
* hum
Come on. The problems you list are:

- control characters in filenames (doesn't affect Python)
- leading dashes in filenames (doesn't affect Python; you don't walk filesystem trees using "find" in Python, you use , and the lack of a standard character encoding scheme (instead of using UTF-8). These three problems impact programs written in any language on Unix/Linux/POSIX system. There are other problems, of course. Spaces in filenames can cause problems; its probably hopeless to ban them outright, but resolving some of the other issues will simplify handling spaces in filenames. For example, when using a Bourne shell, you can use an IFS trick (using IFS=`printf '\n\t'`) to eliminate some problems with spaces. Similarly, special metacharacters in filenames cause some problems; I suspect few if any metacharacters could be forbidden on all POSIX systems, but itd be great if administrators could locally configure systems so that they could prevent or escape such filenames when they want to. I then discuss some other tricks that can help.

The only problem which you list which could cause a bug in a script written in Python, is the issue with confusion between flags and filenames starting with "-". 

But a normal Python script will not be spawning external programs. A normal Python script doesn't spawn "find" to walk a filesystem tree, as in your example; it uses os.walk.

The rest of the issues you list are moot in Python.
* hmmm
  So if I have to make a proxy anyway...

  What I could do is...

  When someone invokes my sendmail command,
  I connect to the server,
  and send the args and stdin stuff,
  and mmm


  passing arbitrary args ain't gonna work cuz 

  then they could override some privileged arg
* alternative projects
  I could actually run one of these big fancy distributed system runner things.
  Like Kubernetes. Or OpenStack. Or libvirt.
  Or Nginx.

  They're all too complicated tho

  But I am thinkin, would be nice to have something that requires multiple systems.

  I could still do public-inbox.

  Or I could actually do this sendmail proxy, sigh.

  SMTP is tricky...

  I could do the SMTP thing.
* SMTP proxy trick thing
  OK fine I'll just do this.

  And maybe run postfix? Or exim? Hmm.

  And I'll have the single SMTP socket,
  which forwards to 
* opensmtpd!!!
opensmtpd supports it!! hooray!!!

no setuid required!!

ok so we'll figure out all the stuff and patch it and it will be all good.

yay mmm 'v'

ok so it requires to run as root

whatever, not a big deal.

I'd like to run opensmtpd as an unprivileged user; would you accept patches to add functionality for making this possible?
* hmm
  building code is declaratively specifying a program
  and then sending it through a pure function to get some artifacts out

  which we then run in the appropriate place

  the ultimate in disintermediation would be... what?
  the code you run at build time directly operating on the hosts it will deploy on?
* mail
  ok so how am I gonna do mail

  i can bypass the root requirement with user namespaces.

  i can bypass the system-wide config with mount namespaces.

  opensmtpd requires the user accounts.

  which is tricky.

  hmm.

  okay, so maybe it's fine to set up a bunch of container stuff.

  that proves viability of container world.

  ok so i'll start up opensmtpd

  that's the goal

  I'll add the ability to specify the user to use for the queue and things in the configuration.
** setgroups
   okay, lol, so.

   we can't setgroups in the user namespace.

   I wonder if I can convince them to not setgroups?
   after all it can increase privileges.

   lol.


   hmmmmm so it gets my uid outside the namespace, 1002.
   then tries to setuid things to it. hmm.

   can we map everything to that uid?

   no.

   ok so...
* dovecot

s6-ipcserver ~/mail/lmtp sh -c 'tee ~/mail/stdin | lmtp -c dovecot.conf | tee ~/mail/stdout' &


hmmmmm

ok so dovecot is confusing

let's go ahead and script all this?

let's just use s6-ipcserver + dovecot with no protocols

ok so for opensmtpd, I should patch it to listen on two sockets, I guess:
one for mail submission, and one for administration.

then it doesn't need to cred-check on the admin one or the submission one:
it just only allows submission on one, and admin on the other.


oh, if you had separate sockets then...

you could make separate binaries: one for mail submission, which doesn't allow any other things, and one for admin.

well you could do that anyway I guess.
just, y'know, it's a bit bad to have all this functionality in a setgid binary


ok so it's ready-ish now to be automated

so we'll, start a user namespace, where we map ourselves to root, and run opensmtpd in there

we'll start dovecot.

we'll start the s6-ipc thing. or well, we'll just do it in python for now maybe.

so what kind of API?

can we dynamically add forwardings?

possibly, but I don't want to mess with config reloading, so let's assume no.

so we'll use a builder api, where adding an LMTP action gives us back a socket that we need to accept on.

with context?

hmm, but how do we get the socket to the actual user?

passing down at startup is a hassle

and practically difficult

could we do it dynamically? they connect to me and then I start routing mail to them over a socket I give them?

that's cool, but also currently we just have a static set of users.
hm.

ok so even allocating new users, we want to modify the SMTP router.

So yeah, let's just go ahead and do the dynamic thing.

OK!
So we aren't going to run opensmtpd with privileges.

So we need to have a way to pass down a socket to it.

OK so I did that. Meh!
* inheriting
ok so it's tricky, hmm.

what I can do is, um,

move the fds to the first place,
then have pony assume that fds it inherits are those, in order of appearance in the config file.

so i need to, after the fork, rearrange the fds to the start. hm.
* dovecot
ok got smtpd running

we'll run both dovecot and s6
* dns
okay so everything is running now

now to further enhance my testing I think I need to make some kind of DNS service

what would be nice is:

<anything>.1.2.3.4.8053.rec.rslv.to

gets recursively resolved to a DNS server at 1.2.3.4:8053

1.2.3.4.ns.rslv.to

just has an NS record for 1.2.3.4.a.rslv.to

and

1.2.3.4.a.rslv.to

has an A record for it.
** alternatively, RFC2136
use RFC2136

say, I generate a random key locally, then hash it, and I can then update whatever.<thathash>.somedomain
** mail

(hmm is there some way we can receive SMTP on nonstandard ports?)

(what if we say, hey this guy is our MX, and, it's set up to look at SRV records? then we can have nonstandard port mail...)

well, a prerequisite for that is flexible DNS. so let's do that first.

oh, is this an open relay?
no, it would check that the MX for the destination domain is itself,
and then just forward to the SRV record.

so it's not an open relay. plus it requires the SRV record! so it's definitely not an open relay.

well. we can spam those sites though. hm.
** IP address
Well, it would be even simpler if I could just a private IP allocated to me.
(maybe IPv6?)

Then I could use NS records, MX records, everything.

A userspace proxy would be fine I suppose:
I can make everything listen on Unix sockets :)

Though, I guess delivery will be difficult.
Well, I have real connection out.
It's just the connection in that's tricky.
So delivery is fine.

Or, can I actually use network namespaces to get an IP?

I could ask geofft maybe?

HmMMMmmMMMMMM

So I can set up a tunnel in a network namespace...

But I can't have both that tunnel and the normal networking stuff at the same time.

That's fine and cool!
Luckily I have all this socket-passing magic!

slirp?
tap?
** hmm
OK so what am I really trying to do though?

I guess, what might be nice...

is to spawn two opensmtpds, and have them send mail to each other?

that's better than using the real internet

but we want to use the real internet because, ultimately,
we want to be listening for mail on the internet.

so, meh, if I use the open internet I think that's fine

that's just a bit of a compromise to accelerate things
* next phase: getting an IP
whew! okay! let's do it!

vpns or something inside the network namespace

the next stage of my adventure!

hmm okay so the hard part is going to be setting up the tunnel in such a way that it works,
even through the multiple network namespaces

oh! miredo! that is such a cool program!
you just install it and bam, you get an public ipv6

I guess that's essentially what we want. can we do it?

miredo even seems to be userspace, hmm, cool

hmm, we could also allow ipv4 stuff if we exist behind NAT

but, oh, wait, we want an address where we can bind to 25.
that won't work if we're behind a NAT or something.

ok so I guess the need for tunneling is obvious:
bridging/sending arbitrary packets is something that shouldn't be allowed for arbitrary users

root controls who can send packets.
and we don't want to allow arbitrary users to send arbitrary packets.
um. do we?

well I guess sending arbitrary packets breaks the BSD socket API;
a user can intercept/mitm/fake TCP packets for another user.

so yeah we clearly need to be using tunneling, not bridging.
** miredo
   okay looking at the miredo API, it should be relatively straightforward

   we can bind the socket outside the netns,
   then pass it down to miredo inside the netns.

   ez pz, lemon sqzy

   and we don't need to worry about reverse DNS maybe
   apparently it's not that important in IPv6 land

   okay and I guess we can have separate receipt IPs and sending IPs.
* okay cool basic miredo is working
  there are several directions now
** setns/get proper usage working
   so that we can bind a socket in the netns and pull it out
** clean up total hacks and sandbox properly
   ok i'll just do this :)

   hmm okay i don't care about cleaning up hacks,
   but I do care about refactoring it to be properly sandboxed.c
** dns forwarding thing
** actually test things
   hmm, we can't test this in a self-contained way

   should I also set up miredo server so that we can actually test this?

   yeah, yeah I should.
   also we can do the DNS all on one box, with two smtp servers, thing.
   that's important for actually testing.
*** that means setns
    okay so why can't I just setns to a user namespace I have privs over?

    well, I could fake out the environment of a setuid executable then.

    it's all because of caps, blah

    In fact why can't I just arbitrarily unshare without privileges?

    Caps, mostly, I guess...
* migrate pid namespace to task
  cool I did it
* thoughts
  rename rsyscallthread to thread
  have stdtask maybe inherit directly?

  maybe remove far FDs

  garbage collect automatically when calling invalidate/close/borrow:
  send a bunch of closes not just one.
  it should be the same speed since close messages are small and we can pipeline them all together.

  pipelining, we could start doing that maybe.
  i think it's relatively safe...

  hmm okay so for ioctls, and things in general,
  I think...

  I think I should have io.Task inherit from handle.Task

  And stdtask, maybe just compose.

  And then for ioctls,
  we'll.


  So the annoying thing is that handle FDs contain a task.

  But we want them to contain the right task.

  We could I guess have them be a generic...

  I mean we can also regard the task they contain as,
  not actually important, just a convenience.

  ok, so this all seems fine.

  we'll have the handle fd, and the memory fd containing it and the memory task.

  and we'll have the handle task (which has syscalls using pointers)
  and the memory task (which has memory-abstracted syscalls)

  and the memory task will be what we pass to memsys

  ugh but then how do we reduce this nesting

  I guess we can just proxy things methods which aren't memory-using anyway.
  Yeah that seems best.
  I'll just proxy it, whatever. That's more code but a better world.

  Now what about StandardTask, though??
  Well I could keep proxying...

  That would be a hassle though.

  I could inherit?

  I could have thread inherit from Stdtask?
* gdb
  ok so we need a good gdb set up

  I guess I can just spawn a shell, that seems fine

  oh nah we don't actually need this yet
* rtnetlink
  we'll use this to detect when the miredo route is created

  seems fine to do it this external way.
  it's generically useful
* allocatedpointer
ok this is the next thing to finish

memory! the kernel has a memory-oriented API:
I have to put data into main memory for it to read it,
and it puts data in main memory for me to read it.

It can't go direct - we should expose

should I have a sizedpointer?

ifreq will just store a tag and be assignable with everything
* pyroute2
OK, I can handle binding and addresses on my own.

And use IPBatch for message serialization and deserialization - with this handle thing?

hmm so it's rmtgrp_link etc

I guess I'll nicely encode this stuff as an enum.

using pyroute2

seems good and nice


hmm.
the address can't be interpreted on its own. dang.
you need to know the netlink protocol
* sandboxing
unmount everything
enter another user namespace (though this blocks the CAP.NET_RAW usage, hmm)
* sigwinch
need to handle sigwinch I guess

this is a pretty important TODO


UGH stupid python establishes a signal handler for sigwinch

which uhhhh I guess breaks us in the rsyscall threads

hmmmmmmmmmmmm

maybe we shouldn't share signal handlers?

hmm no just doing that doesn't help

hmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm

so I guess the signal handle probably does things like access thread locals

so it can't run in the thread

which is fine, but:
that means we need to disable it

which is fine, but:
won't that break some python stuff?

okaaaaaaaaaaaaay sooooooooo
I could unshare the sighand table, and also clear it at startup?
I kind of think that might be how things Should Work?

signal handlers are really very dubious, they assume quite a bit about their stack...

maybe we could set this up with sigaltstack?
right, so that instead of breaking in our threads, it successfully works

hmmmmmmmmmm

so every process is getting sigwinch because of the terminal resize. hmmmmmm.
* dns
a word list for DNS translation

hopefully they don't block the IPv6s, just the domain, so then some later thing could work,
where users can send from domains they actually own

I think the IPv4 is encoded in the IPv6 address that is generated by teredo
so since responses have to come back that might be secure?

yes the IPv4 is encoded so that's fine

teredo IPs are 96 bits long

Maybe we can get 11 bits and 9 words?

yeah

oh, 96 bits is exactly 8 12-bit words

we can probably get that?

anyway for now let's just try getting a DNS server up and sending a query to it
* chroot mounting stuff
ok so we can't umount the world
annoying

but, I did get a good idea from geofft: chroot to /proc/self.
which is what chromium does

hmm sooo

how do I expose /nix but nothing else?

hmm. what if I use some other program to do the sandboxing?

that would be robust and it would save me effort.

yeah that sounds smart.
then I don't get blamed :^)

none of this worrying about how to drop privs

just using namespaces as a privilege-enhancing mechanism,
not a sandboxing mechanism

seems good. then I don't have to think about seccomp or all that stuff.


yeah generically it makes sense to use some existing sandboxing thing.

ok so none of them support socket-passing

but powerdns is probably what I'll use, so.
* powerdns
hmm okay sql
hmmmmm so I could either write some raw sql, or dnsutil load-zone some thingsput 

ok so I finally have the bind backend working so that's all good now

so let's go ahead and do the passing-down

hmmmmmmm

creating this stuff and passing down sockets is a hassle

let's start with making it run from python

ah but then we have to recompile slowly which is a hassle, hmm...

let's support more dynamic injection or something....

we can set, uh... hmm.

yeah just set up some thing where we pull the binaries from ~/.local/src/whatever, it's fine
oh and we don't need the sql scripts! nice

so we just need the binaries which should be easy

ok so...

maybe a separate fd array for UDP/TCP IPv4/IPv6? so four new config variables?

hmm they could reasonably want some addresses passed in and others not

ok, so a map then? one map per variable?

yeah and then you can put whatever you want in local-address and it can be passed down

eh no, we'll require something real I guess
but still, a map

oh maybe let's strtok an = away

yeah that would be more expressive since then someone could come along later and add more

oh but actually it's not great because it can't be passed by env var.

so let's do it with the map, because that's more flexible

ok so we'll turn it into a std::map?
* sandboxing
   hmm, my user really is essentially root;
   it would be trivial to intercept a sudo call I make,
   or even if I do passwordless sudo.

   hmm.
   i need to take sandboxing seriously.

   hmm so how to integrate sandboxing?
   I can just always sandbox, I guess?
   That seems best actually, yeah - assuming I can find a sandboxing tool that's low-overhead.

   Or... maybe I shouldn't always sandbox?
   No yeah let's always sandbox for now,
   then later we can tweak it.
* wildcard NS records time

sslip.io and ip6.name seem to be my options

I don't think I can use lua records - unless wildcard records work?

oh hey

I guess I can use letsencrypt to get a cert for email too?

NS record...

maybe we can just redirect?

hm, true genericness:
any record you give me, I'll spit back a deterministic NS record for that

and an SOA record?

oh maybe I don't need SOA records

ok so let's assume just NS record for now

so as expected, I just need to spit out an NS record on some domain,
then the other guy can have SOA records or whatever

oh yeah I definitely don't need to generate an SOA record, looks like that can be delegated

so...

NS record for foo.bar.ns.rsv.to
is NS foo.bar

I guess?

So we do...

(long ipv6).ip6.name.ns.rsv.to

Or....

(shorter ipv6).tere.to.ns.to
** lua record
ok so we'll just shorten the ip, seems good

wow this was easy!

waaaaaaaait a second
if I ask for... foo.whatever

then...

that's broken...

ok hmmmmm

so:

if I had a path =/proc/magic/[some absolute path to unix socket]/more/stuff=

I wouldn't know when to stop following the path segment.

ok...

so how about...

ewwwwwwwwwwwwww

OK so how about I start with a prefix indicating the number of components????

yuck...


ok so how does resolution work?
we start from the bottom I guess?

ok so this should work then... maybe?

we request com.

uhhh do I need glue records???

I guess I query NS records for this domain...

ok the problem is that I want to delegate:

foo.bar.example.com.

and I want bar.example.com. to... not be delegated I guess?

so I guess I just keep returning myself until they get the full thing?

then above that, I return um...

the full thing?

oh it's not just a matter of the full thing, it's that I need to tell them to keep requesting,
rather than return an NS record.

ok so I guess I do need an explicit length

hmm how do we say "don't return a record, this is invalid"?
** testing
   ok so I've got the NS record thing working

   even if I'm not sure it's quite right

   now how do I test?

   I want to do an iterative lookup, starting from one place

   theoretically once I have this running,
   all I need to do is... write a test which starts a nameserver on IPv6 and then...
   normal DNS iteration will find it and resolve with it.

   but, if I don't do that...

   can I override my DNS server maybe?

   HMmmm okay so what exactly am I wanting to test...

   Server A says, talk to server B.

   Locating server B requires asking server root.
   (Or we can just make that static in /etc/hosts, that would be nice)

   Then server B says, ok here's the record.

   Oh, but wait... server B does need to know what it's serving right? Yeah that should be fine.

   So we need two servers,
   and a host configuration so that we know how to reach each of them without going to the root.

   I guess I can make two IP addresses in a netns and run with listening sockets on them.

   OK so apparently dig isn't super good at this.
   So we need a real recursive resolver.
   And we can override root-hints?

   Cool okay yeah we'll do that.
   It looks like pdns essentially does that too, heh.

   OK so I guess we can, um... yeah we don't need to run this as root as this thing believes,
   but we do need to bind the ports while privileged in a netns.
   And... we'll run the things in that netns too.

   So yeah we'll essentially reproduce that. Meh, fine!

   Yeah so we'll have all these different IPs and things, all good

   This is a good thing to port, the existing pdns regression tests are quite familiar,
   they are a mix of shell and python like v1 (shell)/v2 (single python script) of my stuff (currently on v5)

   So would be cool to demonstrate that we can regression test these.
   In a ~good way~.

   OK so.

   Hah!
   They have v1 tests (shell),
   and one of the more recent tests (I assume) is v2 (single python script).

   Time to bring them into the future.
** things we need
   so, okay.

   we, I guess, need the ability to spin up a fair number of these servers.
   and we need a recursor, I guess.

   We'll operate in a network namespace I guess

   And we can add a large range of IPs to loopback, that's neat.

   Wait, hmm

   Is it already possible to bind to 127.0.0.2?
   Whaat!!! It is!!!???

   I didn't know that...

   OK so... can we use a random port?

   no, we need to use 53, innit

   OK so we need to use a random *address*.
   And we need to have cap_net_bind anyway so we'll have to be in a netns.
   So technically we can just increment and that will be safe.

   it would be cool to do it with inaddr_any or something...

   Whatever, not a big deal.
   OK so we do our cap_net_admin,
   then we bind inside there.

   Seems cool and good.

   First we need a recursor though?

   Yeaaaah we'll run the recursor,
   then we'll run recursor + hints to normal nameserver,
   then we'll run (in ns) recursor + hints to my own 1 authoritative,
   then we'll run (in ns) recursor + hints + multiple authoritative.
** run recursor
   ok so I did this, it's good, neat
** run recursor plus hints
   yeaaah let's do it

   done!
** run recursor plus auth
   ok so we need to start everything in a netns,
   and pass down the socket fds

   seems... fairly straightforward

   woo it's working

   now let's test the lua records, hmm...

   cool seems to work
** run recursur plus two auth
   OK so we need some object that represents the magic domain.
   This can just be some data and formatting.

   And we need another object which represents, a domain and the authority to serve it.
   This can be a domain plus some sockets.

   I guess we can create the latter first hmm.

   Or hmm. Maybe it should be a domain, plus the ability to connect that domain to some sockets.

   Hmmmmmmmmm.

   I guess one domain per is fine.

   yeah sure and I'll give it the sockets too.
   I'll just run one instance per domain, that's fine

   Ummm okay so we made domain plus socket and passed it down.

   Maybe we need something to... bind a zone??
   Well we could make a method which starts a powerdns instance,
   which we can pass a zone.

   hmm we want to send NODATA if the record doesn't exist
** dns server which doesn't care about the origin
   ummm... is the idea that at any path, it gives us the same stuff?

   well, no...

   i dunno, doesn't make sense

   this is a nice free domain idea :)
   all you need is an A record and you can serve DNS -
   and you can get an A record for any IP.
* extracted comment about wish infrastructure
# hmm we can't make stdin/stdout async. hm. hm.
# HM hm hm what a drag.
# having a REPL that is concurrent with the other stuff is tricky. because it's hard to make terminals concurrent.
# h m m.
# very frustrating. hm.
# ok so if we just instead force some kinda client thing, instead of directly working with stdin/stdout/stderr?
# maybe that's a good idea?
# hmm, mostly we'll be wanting to operate on the localhost anyway.
# we do want to support multiple REPLs in multiple places in the task tree though, hm.
# blah. hm.
# what's a nice clean way of handling it? the fact that we want to start a REPL,
# in a simple way?
# I guess someone can sit on connecting to a Unix socket we have open, and we give them the REPL when they connect?
# or, no, that's hacky.

# what if, hm.
# we have the management interface,
# and it has a simple protocol where when some REPL starts, we send the fds and the info over.
# that's complicated though, hm.
# so, we have some fds, which one end of which, um, are probably in the runtime task,
# but maybe are in a remote task and transported to the runtime, or whatever
# and um, we want to move them around. hmm.
# well, so, we could put the other ends of the REPL fds anywhere we want, really.
# so we should do it in a place where we know users are.
# so we should have some kind of object which allows us to start a REPL, and surface it to users.
# hmm. we need to notify the user, and open the UI element, or at least allow them to connect to the REPL somehow.
# should the REPLs be persistent? if they disconnect between each statement, should we save the globals?
# nah...
# okay, so we should surface the REPLs through Emacs, obviously, because that's the best.
# so I guess I could have some list of current REPLs,
# and um, I can open them, I guess.
# what would I do if the program was itself written in Elisp?
# well, I would... I guess, when there's an issue,
# I'd open a buffer with the REPL in it?
# and then maintain a list of issue-buffers?
# and probably have a key to cycle between them?
# and if I closed the buffer, the issue wouldn't go away I suppose.
# well.
# I guess if I closed the buffer, yeah, it would throw an exception up from that REPL.
# How do things in Emacs usually notify?
# so I guess I get circe notifications through them appearing in the modeline,
# they open a buffer in the background - possibly in the foreground optionally.
# and then, when I kill the buffer, presumably nothing happens - we just lose the history of that buffer,
# but it doesn't, like, kick someone.
# though when there's a process behind it, we get prompted when killing.
# so I guess we can do the, "kill throws exception, but also prompts before killing"
# eh, or we can just let the user do it themselves
# so yeah I just have some thing which notifies me on error.
# and I keep an active connection open, and when some REPL goes away, I get notified. hmm.
# seems straightforward.
# basically - we're having Emacs act as the handler for this effect of, go to REPL, right?
# so Emacs needs to be able to accept these notifications, and handle things right.
# but, if there is no active handler, things just hang - also fine.
* problems to fix
** I'm doing all this work without fixing problems!
   I should fix problems instead of ignoring badness.
   I've definitely proved to myself that we can do real things with this.

   Let's hit miredo.
   We'll fix these issues here.
** DONE pointers should carry sizes
   Manually matching up sizes is a bug waiting to happen.
   Maybe we should have some notion of memory region?
*** design
    So, it's simple enough to attach a size to a pointer.
    But what does that really mean?
    What *is* a pointer?

    Also, why do I call them pointers instead of addresses? Hmm.
    Whatever that's fine

    So, ptr + length, what's that similar to?

    It's kind of like IP address + netmask.
    That's also a way to specify a range.
    Though in the pointer case you can specify an arbitrary integer-contiguous range.
    Instead of a mask which says a range to stay within.

    Pointers could have masks instead.

    Hmmmmmmm tagged pointersss

    I guess lisp-machines tag their pointers, so you can only access things in a type-safe way.

    And you could embed the size into the pointer, capability-pointers style.
    That's what I'm talking about anyway.

    Let's read this CHERI paper.
*** cheri insights
    oh interesting, we should probably also have caps like read/write...

    because some pointers aren't writable... or readable...

    or executable - that's interesting... maybe we should track that...

    so that we can understand function pointers as executable.

    oh I see, the reason they need memory to understand capabilities is because,
    memory isn't intermediated by the kernel,
    but instead is directly accessed by instructions
    so there's no way for the kernel to maintain invariants.

    (hmm but could the kernel maintain invariants for memory at paging time perhaps?)
    (guess that's what it does. hm. hmmm.)
    (yeah, cool! I guess it makes sense to associate the bits with physical memory)
    (i mean, the kernel could set them, but, it's the architecture that maintains them)

    yeah it's important to maintain permissions otherwise I could pass constant memory,
    to a syscall that writes...
    although would that really be a big deal...

    oh hmm should we track perms on mmap?
    no, I don't think we should. this would be like tracking perms on fds.
    ok so we shouldn't track permissions, then.
    so what are we even trying to do?
    just be convenient, not capsecure?
    yes. hmm.
*** essentially
i'm just wondering what interface to memory I should provide in my high level language.

arbitrary runtime costs are acceptable

basically, i'm in a nice capability-secure language - and then I want to provide access to memory.

how I do it?

ok so in my nice capsec language,
i don't actually want to be totally capsec:
I just want to be convenient.
but also, principled.
*** masked pointer thing

    ok so you can always cut down the size of your pointer

    so it's like

    4 bits, then...

    an index, of bit-size increments...

    well no it's the full
*** hmm
    so it's just a list of pointers to bytes

    which I can turn into a bytestring
    which I can feed into a program to get back an object.

    it's a pointer to a serialization.
    which, when partnered with a deserializer,
    gives me back an object. hmm.

    I can I guess append these things with a designated combiner,
    which will deserialize one then the next.

    Mmmmmmmmm

    If I have an object thing,
    and I serialize it,
    that tells me how much space I need.

    Well, I want to work with fixed-size things easily.
    So I can write them to a pointer and read them from a pointer...

    Oh also so I don't need to read incremental,
    I can just read in one batch.

    So hm.
    If the interface is, I take this fixed-size string...
    And output that fixed-size string...

    Mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm

    A span is just a bytestring at a remove.

    A storage space for a bytestring.

    I pass in a bytestring to the kernel.
    Well, suppose that were the case!
    Suppose I was just passing bytestrings in.
    Which is where I want to be!

    Would that be a good API? No!
    I would want to constrain to... to...
    the right type!

    Well...
    I can do that constraint for the pointers!
    I can take a pointer to the right type.
    And for things that can't take a pointer in such a way...
    I pass in a bytestring.

    Yes, yes...

    OK so this seems fine.
    It's by analogy to bytestrings...

    And, we're passing in the cap to store a bytestring,
    instead of just getting it returned into random crazy implicit memory.

    And likewise with bytestrings,
    where we can be more specific, we will be.

    So, okay.
    What about the handle API? should that take these nice typed pointers?
    Or... not quite that, but... should the handle.Pointer be sized?
    Hm...
    This to_free is bad - it should be an Allocation,
    so we can check if it's valid.
    That way we can do pointer arithmetic. Well, do we even want to do that?
    No. We shouldn't do pointer arithmetic, ever.

    OK so should we have the size on the handle?

    no pointer arithmetic, sure - but what about reducing the bounds?
*** should we have size on handle.Pointer?
*** What about variable sized pointers that are typed?
    for example, epoll's events thing

    what if we just made everything typed?
*** types
    Okay, yes, simple:
    We just put a type on the handle.Pointer

    We don't have a sized pointer, we just have typed pointers.
    Python runtime is sufficiently expressive for that to work.

    We'll have vectors of types,
    and bytes, and things! It will be grand!

    The type that we'll return will be specified by the type argument, I guess.

    Directly, maybe?
    The thing is, we need a serializer.

    So I guess, we can have a serializer...
    for itself????

    Yeah the issue is, if the serializer is the type argument,
    then the return type isn't directly the type argument.
    cuz bytes won't work for that

    If the serializer is not the type argument,
    then where does the serializer come from?
    And how do we know it's compatible

    wait hmm

    okay so

    I want a type Ptr[C]

    Which will have methods like

    read() -> C and write(C) -> None,

    and which on intialization takes a Serializer[C],

    which should have to_bytes(C) -> bytes and from_bytes(bytes) -> C

    and also sizeof(C) -> int?

    because otherwise we don't know how much to allocate up front.

    ummm hmmm

    well don't we store the size

    oh yeah and here's the thing, we want to be able to malloc a location to write one of these,
    before having one.

    since we might get one returned. okay.

    Well we can do a protocol thing.

    OK! This is simple enough!

    We'll start by just doing it for basic structs!
    Then more later.
** DONE mirror C in the layout of types and classes and things
   This allows just reading manpages for docs.

   Yeah this is a great organization, very convenient.
** DONE disable_cloexec is awkward
  it's awkward to disable_cloexec for multiple fds, easy to forget something.
  would be nice if we were required to disable_cloexec when stringifying the fd number.
  like, as_argument?
** DONE sigwinch segfault
   ok so we'll clear the handlers on thread startup? or what?

   I mean, it seems pretty weird:
   if I have a signal handler registered that uses my thread locals,
   how do I create a new process that doesn't have that same thread local setup?

   it's inherently racy!
   I have to do it before clone, mm.

   ok so what are my options?
   I could block all signals before cloning?
   I *guess* that would be fine...

   Hmmmmmm

   It would be nice if new processes started out with all signals blocked, I guess?

   I could just block sigwinch maybe.
   HmMmMM

   I could just remove the handler...

   Oh hey I can just manually pump readline, HMMM.

   Should I require users to do that? That would be nicely efficient...

   But having signal handlers is a real valid use case.
   And using TLS is a real valid thing to do.

   OK so Python is installing a sigwinch handler, because they are stupid.
   Maybe.

   UMmmmmmmmmmm hmmmmmmmmmmmmm

   Having signal handlers

   Guess it would be nice if everything started out with all signals blocked,
   Which I already said.

   I could block before cloning.
   Which I already said as well.

   OK let's just deregister it in python,
   we can do it robustly later *if* we ever need it.

   We have to do it in python, we can't do it in the thread because it's racy

   oh god there are multiple of them!!

   what about these other ones...

   32 and 33 must be pthreads, oh god...

   what do I do.... what do I do!!!!!!!!

   I can't just block them.
   I need to unset the handlers.
   Reset them all.

   Uggghh Python also SIG_IGN's SIGPIPE, which gets inherited down over exec, hmm.
   HMMM that is actually a real issue.

   Ugh this is just like the signal blocks thing.

   HMMM

   We do want to ignore sigpipe, so it's fine to inherit across fork.

   But we do also want to not ignore it after exec.
   Hmmmmmmmmmmmmmmmmmmmmmmmmmm

   Argh, I'd have to loop through all handlers resetting them.

   Why don't we just go ahead and ban signal handlers?
   They are a nightmare!!

   sigblock is fine though

   OK so... I'll drop all the handlers, I'll drop the sigign...
   for both sigpipe and sigxfsz...

   No handlers for anything...

   arggggggh

   32 and 33 are used by pthreads!!!

   aieeeeeee

   ughghghguhgu

   okay so we could remove those handlers and further ban usage of pthreads?
   that sounds like an actual badness though.
   hmmmmmmmmmmmmmmm
   hMMMMMMMMMMMMMMMMMMMMM

   well they don't get inherited across exec at least

   and... they PROBABLY won't be sent to my child?
   but they could be, which would cause a segfault.

   so... we need a way to... get rid of them...


   ok so we'll unignore these signals, right...
   ummm...
   well...

   we can do it in the thread or in the main python process
   we need to reset handles in the thread anyway,
   because of these pthreads handlers.

   what if... hypothetically...

   we did set TLS?

   and we set it to our own TLS?
   i.e. we inherited TLS.

   that wouldn't help.
   we still need to remove these sigign signals.

   well we could remove those in python theoretically.
*** hm
    So it's kind of like fds leaking over a FILES unshare

    If some thread set SIG_IGN,
    and we unshare SIGHAND,
    we'll have that SIG_IGN in our new SIGHAND space,
    whether that's by clone or unshare or exec.

    And so we need to clean it up.
    (by unsharing first then running code and only then execing)

    And there's a race where we could drop signals that have been SIG_IGN'd.
    But that would also happen before,
    so not a real issue.

    But then what about...

    UGH! This is the answer.
    signal handlers should be reset to default across an unshare sighand.

    (hah we can't even unshare sighand lol)

    the question is, what about our own stuff
    um

    signals with handlers established
    should be reset to the default disposition across fork

    that's what happens in exec, it should be the same

    maybe, I guess maybe not.

    I *guess* there are use cases for inheritance, WHATEVER.
    And we can always block and clear them if we want. BLAH
    Re-establishing could be hard, UGH.

    There should be a SIG_RESETONEXEC or something to reset IGNs, blah.
*** blah
    Just clear all signal handlers at start of rsyscall thread.
    Except, we do want to ignore SIGPIPE and SIGFILSIZE or whatever.

    MMMMMMMMMMMMMMM
    And there's still a race condition,
    where we can get a signal,
    and segfault.

    Wait we don't want to remove the sigigns.

    So... if we did set TLS, it would have the effect we want?!?!?!?!?!?!?

    IT WOULD WORK?!??!?!

    yuck.

    maybe I should figure out how to do that, blah.

    h m M m M m M m M m

    ok I can just store tls and set it, hum

    If I do this I can even, hmm, I can even call glibc functions.

    I can get the tls from, hm. The first process started.
    Which should be glibc linked I guess... maybe...?
    or it's statically linked maybe...

    Is it safe in general to set the same TLS in another thread??

    Ugh, but having to inherit the TLS everywhere is a hassle.

    Let's test if signal 32 causes a segfault.

    Hmm, it does interrupt read.
    We should probably have read be resumed.

    what! hmm!
    an actual resize causes the segfault?

    ok so I'm not sure exactly what's segfaulting

    the segfault happens *after* we sigreturn

    confusing, hmm.

    let's just get rid of the sigwinch in python and then, whatever.

    and also, retry reads/writes/whatevers in the rsyscall thread on EINTR
*** sigaction
    I guess I'll add support for it, fine
** DONE retry on eintr in rsyscall thread
** DONE organize things better
   Great I did even more organization in terms of headers

   We can always do more of this of course.
** DONE use struct pointers for paths
   That is the big remaining place we don't use pointers.
   And the main reason for variable-size things!

   Well, we can always use PATH_MAX
   No we can't use that.

   Well then what do we do when we want to read a symlink target or whatever?
   We allocate a buffer, fine, I guess.

   Guessing the size.

   When we want to send something over it's fine though.

   Well... sizeof on Struct could just be a hint, maybe?
   The size of the buffer to allocate for reading this thing?
   But not the actual size?
   That seems viable.t

   what about for dirfd-rooted paths?
   those are fine and not affected I guess
   (empty paths are still not supported, they're from something else)

   what about for the Path object which is object-oriented?
   should it hold a pointer?

   should we just regress to free functions?

   well what would a free function even look like?
   we'd need something that bundles together a pointer and a dirfd,
   since I don't want to pass them separately.

   so, having a Path object which holds a pointer, might be sound. hm.

   what would it look like? would we put it in handle?

   what if we had other structs like this?
   which contain a reference to an fd?

   or other structs which contain a reference to a function pointer?? we do!

   how do we handle them?
   one way would be to inherit from pointer and stick our additional things on the inheriting class.
   that would at least preserve the references...

   uhhh does that make sense though? how would we do that?
   I mean...
   we could boil ourselves down to some bytes and misc other stuff,
   and also a class for the pointer...

   well what if the pointer gets overwritten??
   how do we determine if the references are leaked or what

   this is the problem of garbage collection when we pass in references to the kernel.
   or indeed, passing in references to anywhere we can't see.

   ok i'm not too concerned about that, I'm just concerned about,
   some kind of object, um.

   which um. I can pass as a path,
   instead of this hassle of separate fd argument.
*** are paths real
    okay so maybe I could just have fd-free paths?

    but I do want people to be able to pass in a single thing?

    it seems like it would be inconvenient to have to manage both a pointer,
    and the dirfd it was relative to.

    hmmm

    if they were merged into a single structure,
    that would be more convenient.

    actually that would be perfect really, it would be so EASY

    but they aren't.

    okay so what's *real* tho?

    a relative path could be fd-relative or cwd-relative

    I *guess* I could separate them out?

    And merge them again with the wrapper object-oriented class?
*** hmmm
    so what if we were fully object-oriented?

    what if we just resolved names to fds,
    and didn't have paths?

    well then open would be something you'd do on an fd.
    open this entry in this fd to get a new fd.
    hmm.

    ok so could we do the same thing?

    basically the notion is that passing paths around is the wrong thing;

    we should pass around names,
    and have to deref them relative to fds.

    yeah that's more legacy-free

    ok and then paths can just be a list of components?

    well, maybe we'll still explicitly denote whether there's a leading empty component or not?

    AT_THIS_ROOT can still restrict absolute path lookups,
    so we don't need to prevent users from passing absolute paths to openat

    right okay so we want to be able to represent everything, so:
    we'll allow zero-length components,
    and we'll strip off a single leading zero-length component,
    and we'll just have a tag which says 

    um, prepend a zero-length component?

    but what if we have multilpe leading zero components?
    strip all?

    what if we start with no components and someone wants to append one?

    OK, let's just be a list of components then, no base.
    Comparing for equality is hard anyway

    Actually I could even use the Python pathlib PurePosixPath

    I'll inherit from that and struct?

    okay that all sounds good but:
*** how do I read a path?
    is there even any point to mallocing a path space?
*** design decision about paths

    two things:

    no "far" or "handle" equivalent for paths
    (everything could have a far/handle equivalent, but we don't do it for most structs.
    because most of the time the ownership is too tricky.
    we only do it when it's convenient, namely for fds, pointers, memory mappings;
    things with clear ownership semantics)

    no dirfds embedded in paths,
    they're just names

    which really is a consequence of them not have far/handle equivalents
    because embedding a dirfd is making something far, really.
*** start implementing functions
*** should the memory-abstracting task inherit from pure path?
    points in favor: yes it totally should because not doing that is a major drag

    points against: um then we have to clean up the struct stuff so that inheriting classes don't have from_bytes

    hmm, LSP doesn't have to apply for classes really
    from_bytes is a class method, there's no reason that it should be on the memory-full type
** TODO sandboxing
    need to actually do it for everything probs

    OK! let's do this actually

    OK, let's use bwrap for this.
    It seems nice and stable and good.

    To know what paths we need to sandbox, we need to know our packaging mechanism.

    I feel comfortable depending on Nix for these external binary scripts.

    Not the core stuff, of course, but sure, Nix for the external stuff.

    And... we can probably drop those binaries from stdtask
** TODO garbage collection
   OK. so. I need to actually do this.

   Although, in my code, I should... endeavour to not leak anything?

   Eh... that's a hassle.
   Let's just GC freely.

   Although, I can't really figure out how to check if something's leaking.
** DONE don't actually read the memory when we read signalfd in childtaskmonitor
   because we don't actually care about it, we just want to wait until there's a signal

   hmm okay so how do we clean up asyncfd to have a nice, handlish interface

   oh ok so here's a reason to have an untype pointer:
   when we get midway through a buffer pointer with a partial read.

   ok so the problem is how to pass a pointer to asyncfd.

   with nice typing
*** reworking pointer passing and returning
    what does a partial read return to us?

    yeah, that's the question:
    so far all our structs have been fully read or written,
    but now we need to engage with partial reads,
    which is a super hassle.

    we could maybe tag the type as partial.. completable with more data.
*** getpeername return a pointer
    if it's too large it tells us how much we need to read

    oh god oh blog oh harb oh blurgh
*** let's not innovate yet
    we can figure it out later
** organization
   Should I maybe use free functions instead of methods for everything?

   Methods are a hassle and lead to huge classes.

   But thiat would be churn, blah.

   Eh, methods are convenient for the user.

   Maybe I should just have both.
** DONE remove binaries from standardtask
   And therefore all associated uses from them from io.py

   We shouldn't have to depend on any binaries in the main thing.
   Those should all be extensions.

   We'll dep on binaries using Nix, that's fine.

   The core is a C library, that's fine, we don't need remote stuff.

   oh. blah. rm is useful though.
   blaaah we're using sh to 
*** DONE figure out temporary directory cleanup
    blah blah argh blah

    we're currently using sh to do it, hmm.

    I *guess* we could just use sh from /bin/sh.

    ugh no that would just be more constraints...

    better to use actual binaries

    let's just move it into an extension module along with the rest of binary-users

    Or, we could add a C function for it.

    Let's start by moving this out.

    An external mkdtemp could find the binaries through, what means?
    Well, if we're set up with Nix stuff,
    we can rely on that being valid in all stdtasks,
    roughly... might want to annotate some things there.

    If we aren't set up with Nix stuff, we can use PATH.
    We can have the executable cache be attached to the stdtask, that's fine;
    or maybe not, maybe that can be external too.

    Let's think about this.
    Because, a robust solution to this,
    is necessary for other processes to not be a hassle to run remotely.

    And we want to move all this out anyway.

    OK, ok, so.
    Let's maybe be able to use both Nix stuff and PATH?
    So, how can we represent the Nix stuff?

    If I'm in some derivation...
    or, rather, I was built in some derivation...
    I know the paths I have available.
    But, how do I know those are available remotely too?

    After all - when I spawn a subprocess -
    it only has the resources I gave it -
    and when I'm in some low-level thing,
    I spawn the subprocess with only some resources.

    So then I guess what I should obviously do is deploy the Nix paths when I spawn.

    I mean - merely spawning doesn't give me those paths.
    And it shouldn't!
    They're ancillary, not needed to the core thing.

    But I could bundle them together.
    I could maybe have some kind of Store abstraction,
    with a list of valid paths?

    Hm, very tricky.
    I mean, this breaks the whole central idea of Nix:
    correct deployment.
    When I'm on a single host, for my code to run,
    my references must all be valid.

    For me to operate on a remote host...
    well, what does it mean?

    When I'm operating on a host, my references to things on that host must all be valid.
    Nix guarantees this normally by looking at all my references,
    and not allowing me to exist on disk if my references aren't all valid.
    But, then again... in a dynamic system I could always, um...
    get a new reference from someone else?
    Right, like:
    Someone could connect to me and tell me the path of an executable to run,
    or a library to load.
    Dynamically, we can't cover everything.

    Well, does that really match what's going on here though?
    There's a certain static-ness to operating on a remote host.
    I mean, we can check all the validity up front before allowing the,
    function/method-holding object to be constructed, or something.

    Sure, I mean, that all seems fine.
    But then - what about - how do we share information about validity?

    Well how do we know things are still valid?
    Hm - I guess - Nix gives no such gurantee to us...
    unless! ah! We open temp roots for things.

    OK, so say we open temp roots for things...
    How do we know we already have temp roots for things?
    Where's that info stored?

    Could be in some per-store class thing,
    or maybe per fsinformation/mount namespace or whatever.

    I mean, nothing much wrong with that,
    except, I feel reluctant to explicitly deal with such objects.
    Although, maybe it's not so bad?

    If we explicitly deal in the Nix store deployment object stuff.
    That's kind of nice, really.
    We'll have some classmethod to construct the executable object from a Nix store.

    Then also we can support the alternative model of just,
    looking things up on the path,
    with another classmethod.
    Maybe one which takes an executable path lookup cache thing.

    that too is a separate class.

    ok so the nix store object will check mount namespace and the root dir in fsinformation,
    on the task it's working with.
    
    if they match, clearly it can be used.
    but it can also be told about new mount namespaces,
    and new root dirs,
    which are valid.
    a root dir is only valid in a specific mount namespace.
    but might be valid in multiple.

    right? it's organized like that?
    could also be like: a mount namespace is only valid in a specific root dir,
    but might be valid in multiple.

    no that doesn't make sense, we'll organize it starting with root dir.
*** DONE figure out pkglibexec things
    OK, so.

    These are all ancillary things. They aren't needed for the main stuff.

    Also, they should be static binaries.
    In which case, they can just be embedded into the library.
    In... most cases.

    move fork_persistent out
    i'm on the edge of being done
*** have only local calls in the main basic library
    The thing to make rsyscall threads,
    is on a level above just making syscalls in our current thread.

    We can pare it down quite nicely to just local calls.
*** DONE NixPath
    how should I model this?

    A NixPath is a path that I know won't become invalid (because I should have a GC reference to it),
    and which I can deploy to new places.
    (because I know the list of its references)

    I guess I'll just inherit from the taskful Path.

    no I ended up with a NixDep class
*** nix deps
    ok so basically we can have a separate instance of nix class,
    for each derivation we dep on.
    which has the path, and the closure, all that stuff

    and i'll make these with json in a special dir,
    and they'll be loaded by name at import time,
    and everything will be fine.

    and we, um, stick an instance into a dependencies class,
    to copy it over to somewhere else.
    um. to get the path on the remote host, eh?
    so it returns a path?

    and we make a temp root for them to stick around
*** dependencies class
    how do we determine if we're on local host?

    do we have one of these classes for each task???
    or many?

    we can inherit this class into child tasks, that's cool
*** source and dest?
    hm!

    we could maybe represent paths on different hosts...?

    that's what we were trying with NixPath I guess

    OK so we'll have NixDependency,
    which will encapsulate... a store, a task, and the dep information.

    Which has a property to give us the path, I guess.

    And we'll have a method (NixDependency, StdTask, Store) -> NixDep

    Hmm.
    How do we make sure we don't use non-existent paths?
    Well, who cares?
    Let's not worry about it.

    OK so, yeah, this way we have a clear chain of command

    OK, so...
    I guess we'll have some store object which we can pull from?
    We'll have our dependencies, and we'll check for existence on the store object?

    And, if the thing is not present...
    We'll...
    Deploy to that store object?

    I guess we always have things locally,
    and we always deploy them to something one hop away...

    So maybe we can just deal with storepaths after all?
    And deploy from localhost?

    Yeah actually that seems fine, let's just do that.
** DONE readlink
   ok how do we handle readlink, dang it
   turns out we actually do need it

   ok, the simple way is...
   using a serializer struct which just passes through bytes, I guess

   then we'll malloc_type(Bytes, bufsiz)

   Easy, easy.

   Oh, hmm.

   We can... just do malloc_type(Path, bufsize)

   That would be cool.

   Hmmmmmm

   Then, I guess, we know how much to read back?

   The return value tells me how much of the buffer is valid.
   So why not just return a valid pointer?
   And, perhaps, also, an invalid pointer?
*** partial reads
    How does rust do this?

    It would be good to show validity vs non-validity.
    
    Ugh, rust just returns an "amount of data read" number. Gross.

are there any crates/examples of a more safe API for readers than
std::io::Read? returning the number of bytes read is certainly very
traditional and familiar, but one could mess up the usage of that
number and read too many or too few bytes out of the buffer one passed
to read();

hmmmm
*** just do it
    ok so we can't make an empty path with pathlib

    annoying, but fine

    we need some other empty-path thing. hm.

    how can we do that?

    I guess we can just maybe have an EmptyPath object?

    ummmm okaaaay

    so we only want to read valid memory out of this readlink

    I *guess* a model where we return the valid pointer could be good.
*** ask osdev
    anyone know a nicer user-level API for reading into a buffer than just returning the number of bytes read?

    did any operating systems/languages ever have a nicer API for reading into a buffer
    than just returning the number of bytes read?

    like, say, returning a bounds-checked pointer to the 

did any operating systems/languages/architectures ever have a nicer API 
for reading into a buffer than just returning the number of bytes read?
returning the number of bytes read seems easy to get wrong and not fully generic;
if you were, say, an architecture with bounds-checked pointers, you might want to return a pointer instead,
with bounds so it can only read the part of the buffer that was written to
*** look at existing operating systems
genode, nothing
capsicum, returns bytes
sel4, couldn't find it/nothing
*** thinking
    okay so returning the amount read is very old and standard.

    can we somehow augment it?

    say, just, return a pointer plus amount read?
    hm.

    blah let's do it

    well hm m m m

    when we pass in a buffer that's... null-terminated?
    when do we ever have null-terminated buffers tho

    pointer arithmetiiiic.

    blah okay let's stick with returning size for now
    we can revisit later
** DONE use a union for repl results, not inheritance
** use conventional syscalls instead of *at when not doing *at things
** serializable for path
   ok so the to_bytes thing can work for the taskful path

   but from_bytes doesn't make any sense cuz you can't pull the task from bytes

   I wonder if we can somehow make this work, hm.

   wait why don't we just, um..

   ok so when we create a pointer, it should be a handle.Path.
   naturally...

   then that would all work.

   hmm this is awkward though

   I'd want to use the Path exactly as a handle.Path

   It's awkward to have two types that need to be worked with

   Hmmm

   The issue is that it has a constructor that I need to work with.

   Well... from_bytes...

   Hmm.
*** aaaaa
aargh
okay so having additional data on the path is a no-go.

unfortunate.

I guess we will just not inherit from the path?

yeah.
there's too many methods here to override
** DONE port ssh to not use executables from stdtask
   it's in progress! mostly done!
*** passing around executables?
    No, I'll pass around the store, and create the executable classes from them on the fly.

    The store is cached.
    Likewise with ExecutablePathLookup thing.

    We'll pass them around;
    the executables classes are just interfaces so the underlying functionality,
    doesn't have to be aware of PATH vs Nix store vs whatever.
*** DONE add interfaces for making ssh_hosts and things directly from store
    in the case of sshexecutables, it's fine I guess, to just pull from store...

    to maybe have... make_ssh_host(store, to_host)

    and make_local_ssh_host(stdtask, store)?
** DONE support split in allocation
*** make allocation interface and reference from handle
    okaaaay

    hmm I feel like I can clean up the allocator
** fix ssh tests/don't call unshare_net in main task
   the ssh tests are breaking because we're calling unshare_net in some earlier tests
** DONE nixdeps
*** how to extend command
**** build_ext
***** pros
      doesn't print error message because of append to py_modules

      has working develop mode
***** cons
      have to override various methods in build_ext which try and build as a C library
**** build_py
***** pros
      less needed to override
***** cons
      prints error message

      have to override develop
** keeping nix store reference around
   we don't want to GC the Store

   we want to unite the two GCs, Nix's and our own.

   well how do we do that then?

   blaaaaaaaaaargh
** delete file stuff
   Yeah, having a File for my file descriptors is a hassle.
** maybe have validate in handle.FD be a contextmanager?
   I think we probably need to be guaranteeing that no-one calls invalidate on me while I'm in a syscall.

   Admittedly that's a misuse, but, whatever.
** epoll
   why am I doing this epoll stuff

   oh, it's to support cancellation. argh.

   I forgot that whole episode.

   HMMMMMMMMMMMMMMM

   bl a h urgh

   ok let's just do a basic conversion
** cancellation
   remember that????
** DONE merge pointers

   And I can support radd!

   So I can do None + ptr

   Which is useful cuz then I can do 
   valid = None
   valid += ptr

   Very cool.
*** but
    what about non-adjacent pointers?

    like, what if I actually have to allocate two buffers to hold a single object?
** DONE pointers with data attached
so I could have a WrittenPointer
which inherits from Pointer
and holds a value

and a WrittenTypedPointer which has the like relationship with TypedPointer

the value holds references to pointers?

not only do we have this with pointers with references to pointers,
we also have it with fds

what if I pass in a WrittenPointer[IovecList],
and um.

how do I determine the return type in the IovecList?

I guess my type will be... um...

WrittenPointer[T_iovec_list] -> T_iovec_list

Yeah that makes sense.
IovecList can take a pointer argument too?

OK. so...

If we do that then,

I guess everything is fine.

That is frighteningly special-cased though.

wait how does writtentypedpointer work

does it inherit from two things?

yes I guess we'll just do that
** DONE deal with sockbufs
*** thoughts
   oh, argh, I have to write the length beforehand.

   My idea of passing them as two pointers and assembling them after, doesn't work.

   Hm.

   So, a type that's passed in, once again sounds like a good idea.

(not to mention how annoying it is to abstract over in a garbage-free environment...)

OK wait, what if we just passed them as one pointer?

That's what we would do if we were really just wanting to be garbage free.

And we'd return...

Oh uh.

Argh

No, a single pointer is hard;
writing the length to it is annoying.

hm. on linux, sockaddrs have a maximum size of 128.

can we exploit this fact?

I don't think so.

Well, it's useful as.

Oh wait can I do sockaddr_storage

Ah yes there's a sockaddr_storage struct which I can use, instead of hardcoding 128.
OK seems fine.

OK so we need a single unit to pass both of them. Hm. hm. hm.

Wait wait wait WAIT wait wait.

How am I going to deal with readv?

OK so...

the iovec parameter isn't mutated on either readv or writev. OK, that's good.

Instead the size written/read is returned. Good, that's good.

But, we still need to be able to create iovecs. Tricky.

So we'll have some iovec list thing...
which we'll write to...

Hm.
The difficult is the same as with sockaddr.

Well, different.

For iovecs, we pass in a pointer,
but we need to keep the references that it has alive.

We still need to do a write in advance.

I guess... we frequently need to do writes in advance.
SO that's fine.

The issue is we need to write valid data;
but again that's fine, we always need to do that.

Well, usually we can't violate memory safety if we write invalid data...
*** efficiency
Oh hm. We could actually do this read in a single blow.
Read the entire buffer, and read the pointer saying how much of it is valid;
then pare it down locally.

Ugghhhhh
*** plan

OK my type will be (Pointer[Address], WrittenPointer[Int32]),
and I'll validate that the latter matches the size of the former,
and then I'll return... what?

A function from memory-reading to an Address?

That would allow me to optimize internally,
but I want to expose reality to the user,
hmm.

Well, reality is that the pointers that are passed in,
can be read to get stuff out.

But, you could easily read invalid data from the address buf,
and think it's part of the address! (well okay not really but maybe!)

Actually.

We don't even need the out pointer.

We can just read the address, and the extra bytes can be discarded?

That... could work.
Saves us a read...

And since we know the max address size, it's not actually incorrect...

We just need to be able to dispatch on address type.

Or, at least, assert that it's right.

Yeah okay this seems good and fine.

We'll return the entire Pointer[Address] as valid.    

Essentially, it's a fixed-size buffer.

We'll just always write 128 (or a more specific size) to WrittenPointer.

Oh, hey, yeah, we don't even need that to know the size.
We can get the size from the Address.

We'll need to check that the thing is null-terminated in sockaddr_un...
either null-terminated, or exactly 108 bytes.

Ok this seems fine and good.

Basically, we ignore the socklen as an out-parameter.
We only use it as an in-parameter.

uggghh but I'm hurting my users here :(
they can't pass around opaque sockaddrs.

well, they can, they just have to use struct sockaddr_storage;
which they would have to do anyway...

but they can't truncate their storage usage to the amount of memory that is *actually* being used.
because they don't know that.

well, they could, if they really wanted to.
yeah okay.
*** neat writtenpointer thing
    If i'm actually moving validity into the writtenpointer each time I write,
    I get robust tracking of what I last wrote - I can't save an old writtenpointer and use it.

    Oh but we can just invalidate the last pointer each time.

    the multiple inheritance thing seems good though

    mmmmmmmmmmmmmmmmmmmmmm
** DONE make "rsyscall.tasks.local" which has all the local things in it
   It's a local task!

   Just like all the other tasks!

   so uniform!!!
** remove casts from rsyscall.tasks.local direct_syscall
   These seem pointless
** DONE make far.Process into a handle
hmm so far.Process really should be made a handle, innit?
because when we unshare pid namespace, it's only for our children...
hmm...
essentially, we do change pid namespace,
but the pid of our children (and everything else, true) stays valid.
TODO maybe change it to a handle
well, obviously we can signal ourselves...

oh yeah and if we make it a handle, then we can gc processes nicely, that's cool

including entire tasks - when we drop the task, we drop the process, and it dies, and everything is cool
** TODO add pointer cache/const pointers
** DONE add nice memoryabstractor API
   make SocketMemoryTransport use the direct_transport to convert bytes to pointer
   as well as allocating memory

   We should implement a local allocator that uses the Python heap.

   That would be nicely efficient.

   Or, rather - it would be faster than our current allocator.

   And allow us to not worry about the fact that direct_transport will call into the local allocator,
   which currently is a bit worrying to me in its performance overhead.

   Maybe we should have a to_pointer interface.
   Which can just byte-cast it when it's local,
   but can also be backed by a transport and allocator.

   Yes that would be even more nice and efficient.

   For that matter, can't we have that right now, then?

   No, hmm, this won't work for bytearray, right?

   For that, we need, instead...

   allocate, then read?

   Well... we can do something that allocates a pointer, which is just a bytearray,
   then reads that pointer to bytes - cheating again, by just returning the underlying bytearray,
   which it pulls out of the allocation?
   MemoryAbstractor??
*** DONE the bes
    the best way is to have thsi interface all as one

    and i'll stay with my current bytes to pointer (preallocated) thing 
    since that allows rearranging

    oh I guess if we controlled allocation too we could even do that nicely
*** to_pointer can have pointer cache in it
    like a page cache kinda
*** can do epoll on remotte side?
*** DONE preparation
    before doing this, let's get rid of the remaining calls in memory_abstracted_syscalls.

    preparation is FINALLY DONE!!! woo!!!
*** finally start

    hmm okay what if I just have a small cap,
    for to_pointer?

    yeah that would be a smaller change and more expandable later,
    let's do it.

    or maybe not.
    the nice thing is, all we need is an access,
    and a pipe,
    and a task,
    and we can make a new access on the other side!

    how do we do the fallback tho

    like, the issue is, we want to have the pipe already existing,
    so we can unshare later.

    wait a second though, we already have the ability to make pipes on the fly when we need them.
    so do we really need the pipe from the start?

    hm mebbe not

    actually maybe I can just go ahead and pull out fork and execing tasks.

    let's pull out exec at first at least

    yeah instead of trying to inherit I can just always, um..
    moving the pipe to local and create the access from there.
*** ok re-eval
    Let's determine whether we can just create the connection on-demand when unsharing memory,
    instead of up-front.

    ummmm okaaaaay soooo

    i just deleted it and it works fine

    I guess we won't need to pre-create the socket memory transport lol.

    ok so that's good, that's good.
*** rethink
    wait, uh...
    if I'm going to be doing this, allocating and then writing in it,
    why don't I just use read/write?
    just have two separate interfaces, as planned originally...

    yeah and we can just use the old Task interface, whatever.
    and, we can call the new memory operator, RAM. or something.
    cool.
    heck for that matter let's just have the io.Task inherit from RAM

    ok seems good
*** hm
    for RAM to create pointers, it requires a task to create them for,
    because the serializer-getter needs a task.

    HmMmMmMMMMMMMMM

    that's basically only for reading, sure...

    when I read and get FDs, I need the task to make 'em real.

    ok fine fine we'll pass in the task too


    ok hmmmmmmmm now how do I get tha RAM from da FAM

    liiiiiiiiiike

    I want the ram from local stdtask...

    not the RAM from my parent stdtask...

    hm. maybe I just reference local???

    hm. hm. hm. hm.
    no that is gross.
** turn SignalMask/SignalBlock into a handle
    I guess? Or at least re-evaluate this design
** DONE have handle.exec return a child process, move child management into handle
   what if I just have handle.exec return a childprocess?

   and y'know, it just throws if it's not possible to do that.

   it can return the process it embeds

   let's also incorporate all the pointers into Process again

   I just like the notion of getting rid of rsyscallthread. hm.

   mebbe should just do that

   ok so we'll have process, childprocess (with valid flag), and threadprocess.

   and yeah we'll just um, make thread just directly contain the child process and stuff

   hmm.

   should I store the death event too?
   I guess...

   and store the buffers and stuff...*
** DONE should we get rid of childprocessmonitor? we can't
   the childprocessmonitor stuff... is actually a pretty low-level detail.

   maybe we shouldn't do this inheritance stuff?
   instead just explicitly clone_parent when necessary?

   we can decide to clone_parent if the parent_task is set on our task

   oh arghy blarghy

   we can't clone_parent if we're pid 1, in a process namespace.

   how are we gonna know that???

   ugh it looks like i'm not properly setting the self-view of process;
   i'm seeing the pid in the outside namespace, instead of inside.

   ok ok let's pass it down, fine fine
** unshare CLONE_NEWPID to make children die on death
   We just start an init process, then all our other children will die when that one does.

   And we can use PDEATHSIG to kill the init process.

   We can even put them in several groups.
** DONE convert socketpair and pipe to handle style
   OK so what will we return?

   I guess, we know the type of the memory in a loose sense;
   but...

   I guess it would be cool if we could, like,
   return a pointer which, when you read from it,
   gives you an fdpair of handles.

   So that would be optimal, yes.

   So, for writing handles to memory,
   we have WrittenPointer.
   So we store the handles along with the pointer;
   seems fine and good.

   For reading them out, what do we do?
   Well, we need to know what task owns them,
   and that's about it.

   Well... we do have that information on the pointer.
   Maybe struct deserialization can just use that information?

   That would be interesting... so essentially every struct would now be able to be handle-aware?
   We'd just pass the task in along with the bytes?

   That actually makes a lot of sense, hm.

   Hmm, it could enhance a lot of things.
   Also, this makes clear the distinction between the task, and the mem_task;
   the latter is just what we use to read and write the memory.

   We have the invariant that everything in a block of memory is owned by a single task...
   that's certainly true since Linux doesn't have any cross-task operations
   (well okay ptrace might be bad but whatever)

   And so, yes, we'll just pass the task in. Hm.

   Huh. Huh. I guess we could even...
   Have handle.FD etc be actual structs.

   Well, not really necessary, but yeah, we could.

   So we could have Task at a low level,
   and then handle.FD above it,
   in specific files,
   just like any other struct. Hmm.

   But that doesn't work because of management stuff. Hm.

   Also, Task still uses types from classes.

   I... *guess* we could use far.Task.

   We... could have mixins for different handle features. Or something.

   It certainly would be interesting to be able to attach Task at deserialization time, hm.

   hMmMmMmM
*** haskell
    so I guess the nicely typed way to do it would be to,

    pass in the additional data thiny at pointer construction time


    ok yeah i just need serializer objects, it's obviou
*** serializer objects
    boom, got 'em
** DONE upgrade to cffi 1.12
   Then I can use the fancy new features!

   ffi.from_buffer("type", object) is a major advance!

   and direct pkgconfig support!
** writtenpointer
   properly deal with split

   what should happen when we split one of these?

   what currently happens is, both of them become writtenpointers,
   which is, rather wacky

   oh wait no, it drops the writtenpointerness. so this is fine.

   no wait no it doesn't

   ok yeah it now drops the writtenness.

   but I'm still confuzzled about what should happen.

   meh it just degrades
** Major TODOs
   implement garbage collection

   implement a memory model that handles the futex sub-reference problem
   (and possibly the problem of multiple references through multiple mappings?)
   (and allows a holistic way for me to use .split even on pointers I don't own, but am merely borrowing...)

   find a good model for typed extensibility of handle.Task (for new syscalls), and implement it
** DONE convert serializer to tagless final style
   we'll encapsulate the code-to-optimize in a function,
   and we'll run it twice:
   first time returning null pointers,
   second time returning real things.

   and the semantic functions will be normal, not async;
   indicating the lack of IO happening

   ok sure fine this is a good idea but still, what about Path?
*** doing it
    hmm we want to be able to return memory instead of use this ugly contextmanager

    also we want to not have to use base pointers I guess

    hmm what if batch frees something
    it could be reused while we are doing the writes
*** hacking it
    okay we're just going to hackily use this new batch thing,
    in a way compatible with old-style allocation,
    so we can delete the serializer
*** hmm alignment
    okaaaaaay

    let's do it in the allocator

    we'll finish the rest of the conversions first

    lol how do I do this when I do a python allocator

    oh well
*** hmm self-reference
    Well, we don't really need to support this... or do we?

    Mmmmmm, possibly.
** DONE execveat to handle style
   The big kahuna.

   We want to figure out a clean way to handle Serializer first.

   Will we convert it to tagless final style?

   What can we do with our nice Abstractor API?

   ok so we obviously use to_pointer and malloc, nice, nice.

   How can we use our Serializable API here?

   I guess we can return handle pointers or something?
   but for what task?
   we don't want them to be actually usable.

   I guess the handle execveat API will take a bunch of WrittenPointers.

   like, one for path

   so, like:
   we just got the basic batch serializer stuff working.

   now I'll do exec without batching,
   so that I can see how it looks with the handle memory nice API,
   and then figure out how to improve the batch API to work with that.
*** DONE how to structure execveat pointers
    So the ArgList is the big tricky one.

    We can have its serializer throw if you call from_bytes, that's fine.

    We want to do that because - how do you find what allocation a pointer is tied to?
    Tricky tricky.

    in general the concept of these serializable classes,
    containing handles for data,
    is... tricky.

    not least because it's not clear what module to put them in.

    well, if I can have them just contain some base class,
    which the real handles inherit from...

    or, an interface...

    oh, or be a generic, taking the pointer to use as an argument

    we can't have an env class because it doesn't make sense,
    we can't serialize the pointers within it separately.

    we could have a list of pairs though
*** DONE okay, structured, now to batch
    I have the structure I want, now let's switch to batchpointers.
*** DONE hmm, problem with handle pointers
    Leaking this pointer is causing exceptions.
    Why??? merely leaking shouldn't cause problems.

    ugh it was an alignment bug, of course
** DONE sendmsg/recvmsg in handle style
   OK so I need to implement structs for all these things.
   Yes...
*** memory-abstracted structs
    OK so I could have a basepointer,
    which takes two generic parameters:

    Its type, and the capability type for operating on the pointer.

    And it can have methods which just use that capability type.

    And then, my structs can be generic in the latter one.

    And we'll bound it to some minimal cap base type.

    eh...

    does this let us break the circular dep?

    ...

    kind of?

    no not really

    only for pointers

    for FileDescriptors...

    well, we'd have this base class, then...

    lol it would be nice to do curiously-recurring template pattern

    we could pass in the type we'll use for pointers,
    and then inherit from it?

    mmm no...

    ok so sure that's an option

    also Pipes could take a generic parameter for their fd type

    for now though, let's just depend on handle.py,
    that's fine.
*** concrete
    So, sticking bytes on disk, and then being able to read a pointer back from them,
    is gross.

    On the other hand, we don't have any casting interface.
    (lol no but we can, like, just read from an fd)
    We can just, like...

    You know, if we guarantee we're reading from something of the right type,
    then it's valid to read the pointer back.

    Well...
    Here's the thing, we don't need the pointers back, just the counts.
    Which then let us split the pointers so they can be read.

    I guess there could be a pool of valid pointers,
    specific to the serializer,
    (which incidentally in this case is exactly what we wrote)
    and we're only allowed to read back pointers in that pool.

    ok so and we have a helper which takes ints and turns them to realized pointers

    hm.

    essentially int to allocation, I guess? if allocation is the cap...

    or maybe

    I could just turn it into a pointer with the appropriate specialization

    you give me a type as well and I do the rest

    uhhh okayyyy yeah

    and I'll just have my Pointer[T] be alias for BasePointer[T, Writable]

    ummm is that possible?

    shrug, even if we have to always write two args to pointer, that's not a big deal

    ok and we can't always make a serializer,
    so our args will be: (int, T) -> something, Serializer[T]
    the valid pointer store will contain within it the serializers for the pointers it knows.

    (and it'll do a typecheck I guess)

    Oh hey, simple solution:
    Just let whether pointers are writable or not be a dynamic property.

    Always have the methods,
    but use them with an interface.

    Can do the same with Task for that matter.

    Hmm we can merge this interface with the allocation interface.
    (instead of having a cap for writing to arbitrary memory)

    essentially pointer as a whole is an interface then;
    could we do that?

    seems big, better not

    yeah so being dynamic is actually good because it's less typing overhead

    because most of the time, even all the time, we'll have memory access available
*** type transformation!
    we'll just do that.

    have WrittenPointer[Msghdr],
    then return Pointer[MsghdrOut]
*** okay so
    the plan is to add read/write methods to Pointer,
    on top of some interface,
    so that it is a dynamic attribute whether a Pointer can be read/written.

    Then, we can have these dataclasses containing pointers,
    which are useful also as out-parameters.

    Once we have that, it's relatively smooth sailing
*** add read/write to Pointer
    OK so are we going to do this by making the whole thing an interface,
    or by just adding those methods?

    I think we should make the whole thing an interface.

    OK, making the whole thing an interface requires figuring out something to replace task.

    that's tricky so let's skip it for now.
    we do want to do that though, as part of making pointer lower-level than the header modules,
    so they can use pointer,
    and therefore we can move the dataclasses using pointer into header modules.

    ok so we can just stick a transport into handle.Pointer

    the only usage of TypedPointer at the moment is, well, unfortunate to lose,
    but oh well...
*** stick transport in Pointer
*** time to deserialize cmsgs
    OK so this is non-trivial right?

    Wait actually it's easy,
    we already have the task.

    OK so...

    I guess we peek ahead precisely one headersworth;

    or, rather, we deserialize out a Cmsghdr.
    And call parse on it so that it actually gets parsed down?

    Uhhh
*** convert the rest
    done!
** DONE clone in handle style
   I guess we can have a Serializable for the stack???

   ok lol I don't need this bufferedstack thing,
   it's just a super primitive batch serializer.
*** first step, do_cloexec_except
    it's done!

    next is all the other usages.
*** rest of the usage of clone
    childtaskmonitor.
    a tricky one.
*** are we interfering with the child monitor
    uhhhh I guess it's fine to create tasks without SIGCHLD and then directly wait on them
*** remove usage of monitor in unshare_files
    boom it's done
*** rework monitor in launch_futex_monitor
    OK hmmmmmmmmmmmm

    Soooooooo

    OK. So.

    monitor.

    What's the API we want here?

    I guess the problem is, we want to be able to, know when, to wait?

    What if we just waitid on all the individual processes?
    Instead of one central thing?
    That's O(n) in the number of processes we manage, which is a drag.

    Maybe we could... do something clever?

    So, with the futex monitor, what are we trying to do?
    We just want to wait on epoll thing,
    because we can't wait on multiple thing.

    We could use clone_thread.

    I mean, indeed, why not just have a CLONE_THREAD which is dedicated to watching our children?

    It could, I guess, um. wait for sigchld? or block in waitid?

    then wait on all the individual processes.

    I mean... couldn't we do our current thing in a thread?

    if we just block sigchld? then the thread can wait for sigchld?

    and when it gets a sigchld, it does a waitid on all the individual tasks that are registered?

    and just, sends us all the signal informations?

    well, that's the same as what we currently have.

    the issue is that clone can start returning events before we have a chance to look at the pid.

    so I mean... what if we just, like, have the pid be written to an address?

    and have the thread look at that address?

    then we further send the pid to the thing?

    and we lock the address I guess.
    or can have a pool of them.

    what does that realy give me?

    well it sends process wait edges to me, I guess.
*** uggghhhh
    the process API is so bad :(

    ok so if we suppose that we use ctid in this way, what can we do?

    well I guess the API would be,
    we allocate a ctid,

    oh hm.
    if we have this monitoring thread,
    it would also save us making futex threads each time.

    argh, no it wouldn't, because of this incredibly stupid horrible shit,
    where ctid futex wakeups don't happen if we are alone in our address space.

    SIGH.
    also anyway we can't wait on multiple futexes at once.

    ok so anyway, the API is,
    we allocate a ctid.
    possibly sending a message to the thread about it?
    or possibly just taking it from our pool

    and we use that ctid in clone
    and clone returns and we get the pid
    and we send that pid and ctid to the thread.

    can we somehow avoid the need for a return handshake?

    so, the thread sees a new pid in a ctid.
    it looks for these pids when it gets a sigchld

    it caches that pid, and remembers to spin-waitid on it.

    hmm. how do we deregister a pid? argh.
*** design
    ok so suppose we had pidfd return values from clone

    ugghhhhghhghgh

    ok so. what about storing pids into bits of memory, then sending them to the thread?

    well. hmm.

    so, okay, we currently can non-concurrently wait for a single child just fine.
    that's easy.

    so then the thing we want to do is,
    we want to...
    wait on multiple. but only some multiple!

    hmm.

    does cleartid happen even in different processes?
    not the futex wakeup, but the clear?

    damn it, no it doesn't

    and anyone, that's child_cleartid, not parent_ptid, which is what we want.

    le infinite sigh
*** thread benefits
    OK so with a thread,
    we can get a subset of tasks monitored.

    For each one, we'll have a ptid,
    and we'll have the pid in that ptid,
    all good.

    And the thread will wait on sigchld,
    and when it gets sigchld,
    it will... do a wait NOWAIT on every child,
    and send that edge to main, I guess.

    And then main can waitid and collect the child at its leisure.

    And, I guess we don't have to deregister, right?
    Because... suppose we waitid on something that's not there...

    Then we can just remove it.
    And if we pid loop?

    And when the thread gets a,
    this child doesn't exist,
    it just forgets about it. I guess.

    Hm. Wait a second, can't we just do this by,
    just,
    waitid(NOWAIT) on every pid we know about,
    send edges for pids,
    (only if there's been a down-edge in between)
    and just forget about children we don't know?

    And we can just send pids to this thread?
    Which will be blocked waiting for sigchld?

    And it will start waiting on that pid?
    I guess that, it just...
    Makes sure to do a wait when it gets a child?

    And, when we get a NOCHILD on a pid,
    we just deregister it,
    no big deal.

    Oh argh, no this won't work.
    We might catch children started by others, right?

    Well, can't we solve that with a handshake?
    We say, ok stop looking at this process,
    and it returns to us,
    okay done, I stopped.

    so I mean, we could achive this on the main thread too.
    it would just require, iterating through every child,
    whenever we get a sigchld.

    that's too costly :(
    but, wouldn't it be correct though?

    What if we just had a thread that we could ask to waitid for us?

    We send it, "waitid on this list of pids",
    and it does, and it sends us the results.

    Wait wait wait wait wAIT WAIT WAIT.

    Could I just batch the syscalls?!

    Then I could do this without an extra thread!
    That would be... great!
** DONE batch waitids
    OK ok, this seems interesting and cool.
    (the idea was obvious as soon as I posed it as, request a batch waitid)

    Will this allow this nice approach?

    Heck... aren't syscalls already batched sufficiently for this?
    I can just call waitid a bunch on handle...

    So, okay.
    When I have some active caller trying to collect events for a tid,
    I issue waitids for that tid,
    when I get some child event.
    That's a minor, but real, efficiency improvement.

    So, 
    where M is number of process events,
    and N is the number of children we're waiting on,
    this takes my waits from O(M),
    to O(MN).

    That's distasteful, but... it's the only non-racy way to do it.

    And, practically, it's still O(M), since we'll batch the syscalls.
** DONE mysteriousness
   Seems I'm sending syscalls but never actually writing them.

   Oh well maybe that's fine actually, could be due to cancellation or whatever

   But actually confusing is that I read three syscall responses worth of bytes (24 bytes)
   but only process 2 response!
** DONE check if there's something in the syscall connection buffer before running waitid
   We should prioritize reading things out of the syscall connection buffer,
   to running waitids for the futex and child task.
   After all if there's something in the buffer, we should look at it
** DONE futex in handle style
*** need to allocate shared memory
    Hmm

    OK so I could allow a page?

    OK so. I have this memfd. I can make this memfd, map it, then make an allocator from it.

    Yeah okay I'll just do that. I'll make an allocator which...

    Well hm.
    Couldn't I expand the mmap space if necessary?

    Hmm that's an interesting idea.

    We can treat it as a heap, I guess. Just a standard, linearly growable heap

    break-style heap

    GrowableMapping

    Actually for that matter couldn't we just use such an allocator by default?

    Instead of having several mappings?

    Well, I guess it would be misleading to the kernel...
    we want to give it more freedom about establishing mappings.

    Meh okay we call it SingleMappingAllocator.

    Ah and I can reserve space by just creating another mapping... cool.

    I guess I could, uh. Store both the mapping I'm using, and the reserve mapping I'm growing in to.

    The thing we would want to do is move to a new pointer.
    Hmm! Which we can actually do, using this allocation interface! That's cool!

    So I guess what we would do is,
    we'd check that each allocation is not pinned,
    then try to move to a new address.

    If some allocation is pinned, then we try grow-remapping without moving.
    If that then fails, then we throw.

    Hm. hm hm hm hm.

    I mean really I just want to have a bunch of allocations ahead of time.
    Hmm.

    Yeah so I guess the creation of additional mappings
    is how we avoid issues with growing a mapping coliding with another.

    But, we do want to be able to grow mappings, hmm.

    We should be able to move around allocations, really.
    We should express when some address is pinned by some means.

    Like... when we store the address of something, we should know that we've done that.
    Currently we do that by borrowing; but what about when we do it persistently?
    We want to attach the borrow to a pointer... so that when that pointer is freed...
    then we're freed...

    Or also attach it to a task I guess, when we tell the kernel about some address,
    and then we need to store that address forever.
*** SingleMappingAllocator
    And we'll just not support resizing.

    Resizing is for later. When we have a robust way to relocate mappings.
*** DONE need to handle self-reference in robust list
    I started on this but I'm still not through,
    since doing a reinterpret and passing the serializer doesn't actually work,
    since it invalidates the old pointer.

    Hah, problem solved! We'll just store a null instead!
*** DONE need to pass around pointer to futex embedded in other structure
    HmMmMm

    Hah, problem solved! We'll just pass around a Futex structure,
    that contain both the futex,
    and the robust list!
*** sharing a mappingallocator between address spaces
    We need to return the pointer to the futex,
    but it'll be at a different address in a different address space,
    but in the same mapping.

    OK so I guess the notion would be,
    map the same thing in multiple address spaces,
    somehow verify it's the same mapping,
    and ask the mapping allocator to translate.

    And we'll have one address space be the owner, and manage allocation?
    Well, we could do fallback. Like we're planning with fd tables.

    I could have allocation.near() take the address space,
    and get back a pointer for that address space.

    But no I think it's better to bake in the address space/mapping a pointer is for.

    The allocator doesn't actually allocate in a mapping;
    it allocates in a file.

    And so I can pass a pointer to the allocator and get it translated to another mapping?

    Well, okay, so ultimately the allocator needs to know what mappings it is valid in.
    We can call "add_mapping" to store another mapping that it should regard as valid...

    OK so the tricky.. thing...
    oh wait we'll never reallocate anyway...
    And we'll never unmap it...
    so it would be fine to just use a single arena

    hmmmm so. how do we determine two mappings are the same. hm.

    that is, maybe we could do it outside the allocator?

    like... suppose the pointer had a reference to the mapping it's within?

    well, if we did that, we'd have trouble using the regular python allocator for to_pointer.
    but, we don't strictly need to do that actually.

    well, so, we can do it inside the local allocator.
    we can copy bytes() into a region we control.

    that reduces our constraints anyway, means we don't depend on the weak Python allocator.
    which can't even do alignment.
    so that seems better anyway.
**** hm
     so if the pointer contained the mapping it's within,
     then we could, ourselves, outside the allocator,
     handle mapping equality.

     that is interesting, yes.

     what if we attached an allocator to a mapping that's already being allocated?
     well what if we started, um, closing fds of, um, an fd space that's already managed?

     the thing is, the allocator subdivides a space.
     it's our own multiplexer. um. is that right?

     Yes whatever. OK so how do we get a pointer relative to another space?

     I guess the notion is, I, a pointer, have a mapping, and I resolve that mapping to an allocator,
     and then that allocator performs the operations necessary to change my allocation.
**** ok
     practically speaking, what do I do?

     I want a pointer which is pointing to a different memory mapping, but the same address space.

     *maybe* I could have an AllocationInterface which just returns the offset in a memory mapping???

     actually for that matter, the allocator doesn't even need to hold any memory mapping.
     hm! that's interesting.

     ok so I like this idea, my allocator just returns an allocationinterface,
     which gives me an offset in a memory mapping.

     hmm I guess then maybe it should return the memory mapping it used so it can synth new ones?

     or maybe that's a higher-level concept?

     i guess one thing I want is to be able to map the mapping elsewhere um hm hm

     so I like the notion of, an allocator,
     which can fail...?

     ok so i feel like I missed a thought because I didn't write it down.
     if malloc returns a mapping, then I lose, um, some abstraction power, don't I?
     or wait no I don't

     ok so, why don't I just,
     have the allocator be a struct I can instantiate?
     which handles allocation?

     and so I can have the pointer reference the memory mapping,
     and ask the allocationinterface for split/join.

     ok so if I explicitly know the mapping at all times, that's pretty cool isn't it?
     I can move things between tasks, all that stuff.
     h m m.

     well the allocationinferface could just tell me what the mapping is I suppose.

     that way it could still move pointers between mappings if necessary.
     or change the offset or whatever.

     like, relocate things.
     but wait, it can still change the offset, that's fine.

     it just can't change the mapping.

     hmm also we can't grow mappings.

     ok so I don't see why we would need to move pointers between mappings?
     well, for compaction.

     well if we're talking about this like files on disk, allocating out of there...

     then, y'know, it's not clear that it's necessary to compact between files?

     I guess that could be an offline step?
     how would we do this offline?

     we'd need to collect all the pointers and repoint them.
**** so I like this notion of exposing the mapping
     but, I'm concerned about the loss of the ability to compact across mappings.

     Should I be concerned?
     I think no.
     This is the kind of de-abstraction that I typically like.

     And anyway C works just fine without compaction.

     Compaction is only necessary if we want to return memory.
     Well, we can still return memory, we just have to move out of it first.
     We have to, stop having pointers which map it.

     Hey wow can we even just forward all responsibility for keeping mappings alive to the user?
     Maybe not.

     OK so yeah we just have to get rid of all the pointers in a mapping.
     Which, because we can't move between mappings, is hard.
     But... whatever, I think it's fine.
     It makes pointers more awesomely portable.

     Shared memory as a native functionality! Very cool.
**** moving the pointer validity
     So I guess we need multiple valid pointers for the same allocation now.

     Since they can be in different mappings. Hmm.

     What if validity is just.

     Well validity determines if we free on del.

     We want to release the reference to the allocation when we become invalid I guess.

     We also don't want to allow an explicit free, right?

     Since multiple things might hold references, and we don't know when we can delete it.

     I guess this is the same as the fd problem

     Like right now I'm doing linearity...

     but how does that factor into a shared memory system???

     The issue is, there's multiple owner tasks, not just one owner task, right?

     If one crashes we still want to keep it alive.

     Yeah okay this is really just the same thing as the fds.

     Fds are: fd:fd table (shared between task through inheritance)

     Pointers are: allocation:file (shared between task through memory mapping)

     So I have like... a list of handles, for each allocation, for each file?

     I guess I need a notion of file then.
     
     OK so since this means tracking files anyway,
     that can simplify a lot of the memory mapping convertability closure thing.

     If we're the same file, the pointer can be converted. Easy.

     So I guess we'll have Files in MemoryMappings.

     Let's skip um, putting the Files in FDs.

     I... guess we could just... save the File() used in mmap.

     Yeah huh, that's straightforward and simple.
*** need to store robust list
    We could attach that to the task.
    That might be good, sure.

    Yes we can just attach it to the task, easy!
*** garbage collection of intrinsic list
    So really the issue is that I need to figure out how to GC something containing an intrinsic list.
    Namely the robust list.

    Say I enclosed it in a struct;
    well then the list would be pointing to invalid memory...

    Well that's just another way to view it.
** DONE stick MemoryMapping directly into all pointers
*** then start managing pointers in file_to_allocation_to_handles
*** then we can explicitly pass a file into our mmap and make it clear the mappings are the same
*** thoughts
    What about split and merge in the presence of multiple handles?

    So...
    if we've got a bunch of handles to an allocation, hm hm hm.

    well, splitting or merging will invalidate the old allocation,
    so the old pointer will break. hm.

    oh glob this is all a lot
    ok ok we can still do it, it's okay.

    Hmm what if I put this handle tracking behind the AllocatorInterface?

    Instead of having an explicit map...

    I just track which allocator is being used for which file?

    That could allow for more flexibility for allocations, like splitting/merging allocations.

    Well that's gross obviously because instead I should just contain the file and...
    use it...

    Well then I can't work with multiple mappings.

    HMM. This is too advanced.
    maybe.

    OK so I'd need some kind of allocation range system, which can overlap.
    Which is what we were talking about earlier. Hm.

    Argh it's really just too complex.
*** blah
    OK ok okay.

    What about byte-level allocation?
    Basically we somehow know when some allocation has a byte active,
    and then we don't allocate into that byte?

    Maybe our pointers (spans) are like, lists of bytes or something?
    Lists of contiguous bytes.

    Which can be summarized into just (start, length).

    HmMmm

    I mean okay, so I could have like, file to offset to handle.

    And it's super huge, because there's one offset for each, y'know, offset.

    And... what exactly is the principle to follow?
    I mean, split and merge don't require this.
    Do they?
    I mean... oh yeah they do require it.

    The thing is that you'll have multiple references to a given allocation,
    so that means you need some kind of handle list.

    And you want to be able to split/merge allocations,
    which means your handle tracking needs to be at the level of bytes.

    Or, virtually at that level, anyway.
    I guess we should provide an interface then. For the things we want to do.

    What... exactly do we want to do?
    With fds I guess we... want to know when we can close the file descriptor.
    
    And also this thing with disable_cloexec, for inheritance...
    But I guess that's not an issue...

    So, I mean, I guess all we want to do is know when we can free the pointer.

    Which is ultimately the allocator's responsibility?

    I mean... why don't we just support, like...
    Split and stuff...
    On the allocator...
    Without having to worry?

    Well, the question is again, how do we know when we can call free?
    Once there are no valid pointers for some byte sub-range,
    then we can call free.

    Well, why can't we just track that with the allocator? Like, one valid pointer per "allocation"?

    That would just be a matter of being able to make multiple "allocations" for the same range.

    OK, that seems good, seems good. Then, okay...

    I guess we don't need the file to allocator mapping because... why?
    Well we already have our allocation I guess.

    The allocation is already specific to each valid file handle.

    We could extend this technique to fds by having fds be, uh, linear, I guess.

    We'd have some kind of, uh, fdallocation.
    Which there's one of, per, um, moving-around.
*** hm
    okay so yeah, if we have multiple valid pointers to the same allocation,
    that works.

    but if we introduce split/merge,
    the valid pointers are to different allocations and the old allocations are invalidated,
    and it all just doesn't work.

    that's why we need to natively support multiple allocations for the same space!

    that kinda violates my nice linearity guarantees
    if i can have multiple allocations for same space

    but I was already going to violate those,
    by having multiple valid pointers for multiple mappings.

    so, whatever
*** implementation ideas
    also, we can just have the memory mapping, instead of mmap and task.
*** batch allocator
    So the batch allocator, it's not clear what we should do.

    A memory mapping.

    So okay, ideally we would have some kind of... maybe an interface?

    I guess, okay, so.
    We need to support the various to_pointer operations,
    then also split, and write, and such.

    I suppose I could abuse duck-typing.

    So okay, the alternatives are:
    - abuse duck-typing
    - make a pointer interface
    - make a memory mapping that the pointers are within

    The last seems compelling...
    Oh hey can't we just make a fake memory mapping like that?
    Like, something at null? Offset 0 always?

    Hmm. But what if we go about unmapping it?
    We wouldn't want to do that.
    How to prevent that?

    OK, so...

    I guess if we just allocated a real mapping and used that for everything?
*** first class typing
    Well, I guess higher-order-kinds are required to do, um.

    The semantics are specialized with a type, which is then  passed in to a function.

    ummm so...

    actually doesn't this just work?

    or, wait, how do I write down the type? of the... implementer...

    heck for that matter how does someone write down the type in haskell

    ok wait so... the return type of one of these semantics[T] taking functions is...
    some expression involving T.

    then... ok yeah it's a higher-order-kind.

    Hmm I think abusing duck-typing might actually be my best bet.

    A pointer interface is way less expressive.
    Actually making a mapping, or having a valid pointer at all, is too unsafe!

    OK so what if we extend the semantics with all the operations we can do,
    and we require using those instead of methods?

    Well we still need near...

    And splitting and writing as a method is convenient.

    OK sure let's duck-type it.
** TODO make allocator support duplicating allocations
   what if we sort the list of allocations by start pointer?

   okay that seems to work.
   allocation continues to work the same with a minor tweak

   when we split we'll need to insert the second half carefully to maintain sorted order

   OK seems fine.

   We should have a test that does some splitting and stuff.
*** datastructure
    it would be nice to have a clean datastructure that takes an initial "size", then supports:

    maybe_alloc(size: int) -> Optional[interval]
    which returns an interval of the given size that doesn't overlap with any other intervals,
    or an error if none can be found

    remove(interval) -> Foo
    removes the interval.

    split(interval)
    merge(interval, interval)

    hm.
    what if we stored lists of allocations, maybe?
    that would solve split, sure...

    what about merge?

    oh, merge would just be storing lists of allocations as well;
    we'd just store two allocations.

    ok so if we store lists of allocations then...

    and those lists can overlap then...

    the question is, how do we know when to free something?

    hm curious. well. we could continue to maintain lists of handles to allocations?
*** two alternatives
    directly have multiple overlapping intervals in our datastructure,
    and be able to remove them

    have only a single interval at each point,
    and store lists of intervals outside.
*** storing lists of handles
    So how can we do this performantly?

    I guess the API is, um.

    I have some list of handles that I own.

    And when I delete myself, I tell that to the handle-list-monitor.

    And once no-one owns a handle anymore...

    Then we free it.

    I mean it's just a refcount really.

    So okay, the choices are refcounts, or directly representing the intervals in the datastructure.

    SO saying it again:
*** two alternatives
    directly have multiple overlapping intervals in our datastructure,
    and be able to remove them.

    have only a single interval at each point,
    and store lists of intervals outside, and refcounts for each interval
*** hm
    so directly storing overlapping intervals seems like it will be much more performant.
    since repeated splits/merges won't cause gross fragmentation.

    hmm
    but what exactly is the API then?

    well basically, when we have a range,
    we can dup it.

    so. we can insert ranges, basically.

    yeah so how about the generic API is just,

    find_space(size) -> (begin, end)
    mark(begin, end) -> None
    unmark(begin, end) -> None

    oh but that doesn't really work since we want to be able to support multiple marks on a given byte.
    hmm but if we support that, that's basically refcounting for each byte, heh.

    the real API is, I guess, more like:

    find_space(size) -> allocation
    split(allocation) -> hm
    free(allocation) -> None

    is what we're doing just, recursive allocation?
    in some sense? splitting like this?
    so what if we had a split for the entire range?
    and what if allocations allowed alloc?

    so basically, when I alloc...
    I could get, like...

    well I can alloc within that alloc in a sense with split,
    but that's not super useful - I want it to be visible to the top level!

    so say I alloc something,
    then I sub-alloc (aka, split) something in it.

    well no I do want to make sure that I'm starting from the start of the range.

    well I guess that can be a guarantee - after all the top-level API gives me that.

    so like. if I alloc something, what if it gave me the rest of the allocator?

    like, allocation without mutation? basically?

    so I alloc, which is essentially splitting;

    and then i get the rest of the range;

    and i go on to alloc more things out of that range;

    and then, I free the first thing. how do I handle that?

    I guess I can put it back into the list of free space.

    which starts out with one big block.

    so okay, sure, I store my free space explicitly, let's say I do that.

    (oh hm is storing free space explicitly the key? since that's not multi-reference like the other thing is)

    ok so yeah I store my free space,
    I split off some memory,
    I use it,
    I split it further and return part of it.

    Now I can just immediately merge, I suppose, to reduce fragmentation.

    So the API here is essentially just, splitting and merging ranges.

    And I can perform allocation by just looking through a list that I designate "free".

    So sure, suppose I do this. Then how do I model, um,
    the multiple references to a single range, thing?
*** splitting
    *Is* splitting good?

    I think it's good.

    It clearly indicates "this range is good, and this range is bad".

    Which is nice.

    like, I consume some range.

    And I return back a buffer that's ready for reading and parsing and all that.

    Plus also some letover chaff.

    Which I want to free separately.

    I mean, yes, I think it's good. It's a nice API.
*** hmm
    how does this map to what i get from mmap hmmmmmm

    well obviously with mmap I can, at any time, free any individual page in a memory mapping!

    well hey if that's possible, then how do I store overlapping ranges?

    well... if mmap had some kind of notion of,
    here is the mapping for this range...

    and they're, um, layered? or... maybe just refcounted...

    and you can say, hey unmap this range,
    but it stays mapped because there's still something underneath providing a mapping...

    i mean... refcounting at the layer of individual page mappings works because it's not that inefficient.

    hummm

    blah hm blargh.

hey #osdev, do you know of any systems where you can allocate some memory, as normal, and then free part of it?

ok so that's not critically the thing I want to do.

like. okay. so.

the issue is what I do if I have multiple valid pointers for a single allocation,
and I split one of them.

maybe I just shouldn't support that.
what if I just ummmm only have a single pointer for an allocation,

and just have multiple views of it?

like, when I have some allocation in some file...

i could store that as primary
and just reify it as being in some memory mapping.
at... what point?

ok so um what about realloc? just support that?
well no I like the generalization here...

ok what about truly just nesting allocation?
like when it gives me a buffer,
I then allocate n out of that memory,
without invalidating the original pointer;
instead, I keep a reference to the entire original allocation.

so, that's a more arbitrarily-nestable solution,
instead of trying to flatten the whole thing. hm.

does that in some sense mirror how we relate to the system allocator?

we say, give us some memory mmap,
then we allocate within it.

then we say, give us some memory malloc,
then we allocate (by calling syscalls) within it.

so, being flat over the entire system would be cool.
but how does it compare to mmap?
well, mmap lets me free whatever pages I want in whatever order I want.
so it's kind of the same level of power, hm...

so, one place where you can kind of see an API like this is in mmap.
mmap allows you to allocate a range of pages,
and then unmap any individual page in that range, in whatever order and combination you want.

so, unlike what I'm looking for,
the mmap API is completely "raw";
there's no way to say, "I have "

um well actually with mmap you can kind of view it as linear.

right so if I own a memory mapping, um.
I can just um, split it and um, free any sub-range of it.

file systems
hmm

maybe should look at them

hmm mmap has hugepages

with mmap, when I allocate something, I might start with a hugepage.
and then I unmap something in the middle and it's no problem,
because it falls back to smaller pages.

which are slower, sure...

okay so what if I just go with a list of allocations,
and I make sure to merge them? 

and then I just refcount allocations?
well, hmm.

what about the scenario where I have a fragmented allocation from the start?

and it stays fragmented.

like, um.

well yeah what the heck about that.
how do I, if I have some allocation,
split that allocation?

I guess I need to, like... go and...
find everyone who has this allocation and...

split it?

So yeah basically, I'd change the original allocation to... hold a list.

And when it gets freed it, decrements the refcount on the allocations in that list.

So basically every allocation interface holds a list,
and decrements a refcount on the allocations in that list on __del__.

Then those primitive allocations will just...
um...

well...

so, we've got the list,
and so when we call split, we,
find the split point in the list,
then split that primitive allocation,
then return two listallocs containing those allocations.

ok so that's split, and that's fine.

what about merge?

well, we can do merge in the same way right?

hmm...

if we have two allocations,
and... we have the only references to those allocations...

then we can merge them.

well ok sure but how do we merge them even when we don't have the only references?

like say we have two pointers...

to the same byte range...

and then we split one of them...

and then we merge it again...

then we have two allocations of the same byte range,
but which are different.

so how do we deal with that?

we need, I guess, to identify allocations with the byte range that they represent.

also wait a second, how do we even go and split an allocation?
so that it, um, splits?

I guess we have some allocation, pointing to a primalloc, which we split, ok, sure.

but yeah, suppose we merge those guys again.

and then we.. split again?

um anyway, right so how do we find an allocation to split it?

what if it's arbitrary nesting?
like, I have my allocation, which might contain either a direct allocation, or a pair of allocations.

when I split, I switch that allocation to containing a pair of allocations,
and I return each of those allocations.

when I *merge*...

well, why do I ever even need to merge?

couldn't I, shouldn't I, just keep around the original merged allocation?

that's a better certificate of adjacency anyway.

ok so I haven't even used the merge API...

so let's assume merge isn't present.

instead, we need to merge implicitly when we drop the split allocation references.

so, we have our old allocation, which references each of the leaves below it.

when it becomes the only reference to those leaves,
then we can merge them back into a single leaf.

oh, that sounds really familiar.

ok yeah so we just maintain a tree of allocations,
which... maybe each allocation has a reference to its parent?

and when an allocation gets freed,
then...

we look at our parent, and if the other allocation is also freed,
we merge?

hm something like that.

i'm thinking it would be really cool if the allocation API is just the same as the split API,
and this tree is what we allocate from.

ok so, so so.

we have allocations. and allocations can be referenced, or freed.

(we can just maintain a reference count to have multiple references to an allocation?)

when we split, we split an allocation into two sub-allocations.

those are just a split in the tree. those can also be referenced or freed.

when we free one, then we check if our sibling is also freed,
and then we merge them into the parent.

uhhh well okay, before we do that I guess we check that we are a single atomic node.

er, a leaf.

if we're a leaf,
then when we're freed we check if our sibling is also freed,
then we delete ourselves and our sibling and reduce our parent to a leaf.

if we're an internal node,
then when we're freed we do nothing.

if we're freed, then when we become a leaf we do the merge-up.

and so, basically...

I guess with this API, when we're not linearly consuming the buffer passed in,
we can go ahead and only return the valid portion of the buffer. which is much nicer.

yeah so and, we can walk the tree looking for freed nodes to allocate from?

a node is only allocatable when it is a leaf and it's freed.

(if it's not a leaf, and it's freed, then it means some of its children are still active)
*** wording
I think I've figured out an implementation that will give me the split semantics I want.
it's strangely familiar, but I can't quite place it, maybe someone else recognizes it.

We structure memory as a binary tree, with internal nodes and leaves, 
with every node having a used bit U, a starting address A, a size Z,
and a pointer to its parent.
We maintain the invariant that the used bit U is true iff the user has a reference to this node.
We start out with the single root node as a leaf.

The split operation takes a leaf node L(U, A, Z), a number of bytes N, and new used bits U1 and U2.
split(L, N, U1, U2) turns the leaf node L into an internal node,
creating two leaf children, L1(U1, A, N) and L2(U2, A+N, Z-N).

So then our allocation process is:
To allocate N bytes, we search the tree for leaves with the used bit U==false and size Z>=N.
When we find such a leaf node L,
we perform split(L, N, true, false) to turn it into an internal node,
and return the address of the resulting first child L1.
We don't change the used bit on L.
(If we don't find an appropriately sized node, allocation fails, and we allocate a new arena or grow memory or something).

The free operation takes an arbitrary node D, not necessarily a leaf, necessarily with used bit U==true.
Free sets the used bit U to false.
Then we perform the housekeeping operation on D.

The housekeeping operation takes a node D, which necessarily has used bit U==false.
If D is a leaf node,
we check the used bit of D's sibling.
If D's sibling is also unused,
then we delete D and its sibling,
turn the parent node back into a leaf,
and perform the housekeeping operation on the parent node.

Then the user split operation for D simply performs:
split(D, N, true, true)
and returns the two created child nodes.
If D is already an internal node, split just sets its children to used.

lol no that requires linearity.
*** think about eliminating free bit
Hmm one nice thing about this API though,
I don't know if we even need the free bit.

We could just maintain a list of references or something, hm.
So if, hmm.

If the user free,
is just giving us back that node...

Well, okay, suppose it's just giving us back that node.

Then we can just want to be able to perform the merge operation.

I guess we can just maintain a free list, and try to merge it with things next to it.
Easy-peasy.

Ugh no there's also fragmentation from this tree structure.
Gosh dang it!!!

Blargh! Argh! Fragmentation from this tree structure! Blargh!

ok this is just a buddy allocator.
right.

people aren't concerned about its fragmentation, so no big deal.
*** argh
Oh wait. Argh.
This doesn't work because of,
what if we split an allocation in one way,
then split it again in another way?
Dang it.

Well, okay, one notion is to require the splits to be merged again.
Linearity, y'know.

But that doesn't help because then we lose the ability to,
have one allocation split and still have a reference to the container.

Ok so...

I guess we could maybe have like, some kind of alternate tree?

Basically, to split D which is an internal node,
we need to find the leaf node where we need to split it.

Then we split that leaf node.

Then we group together the rest?

Maybe, like, a direct list?

Well, no, so, we have the leaf node that is split,
and then we have the rest of the stuff.

Um, right, no, we do need a list up, uh...

We need a node which skips the split leaf node.

It points to the new left child and parent of the leaf;
and the new right child and the other child of the toplevel node D.

So then we have two nodes which are equivalent.

I guess the node can be a list of nodes or something?

Like, it has N pairs of children.

Hm.

OK so suppose this is the case, does this let us avoid fragmentation perfectly?

Hmm.

Seems kind of like we need three children, not two.

This is some kind of fancy tree.

I guess it's a dag.

Hmm so what if we always maintained a tree which covers exactly the contiguous free space?

Or... rather...

A tree which has, um, a node which is all free, and a node which is all allocated?
No opportunities for merging left un-done? Um.

Ok sure so we need three children.

Umm what's the definition?

Fug

Fug!!!!

This is too hard and too advanced.

maybe I should just have shortening buffers?

no, that doesn't support, um...

a nice API for, um...

returning the number of bytes read.
*** ughhhhh
UGGGHHH

people on IRC have supremely LIMITED VISION

ok, so. I guess we need to focus seriously on memory allocation to solve this problem.

Can we hack around it instead, and delay this problem to later?

Yes of course we can.

We can just... hmm...

Fabricate two valid pointers I guess.

Oh yeah actually since we'll never reuse the mapping used for the robust futexes,

we actually don't have a problem.

OK! yeah! Let's just skip it!

Oh also we can always take the alternative approach of making a linear pointer API
that can be viewed through multiple mappings.

That might be more fruitful.

oh. I don't need this anyway. I can just linearly move the pointer.
That keeps the allocation alive. It's completely principled and fine.
*** think about free list implementation
    OK so!!!

    if I store free spaces.

    Then a split doesn't require looking at the allocator at all.

    Only when I then free do I need to go back into the allocator.

    Which is fine and good.
** maybe integrate span/split/merge APIs directly into allocator
   Not sure this is actually a good idea.

   Having span be on top of the other thing is nice...

   Though it's not workable for syscalls, which do want to split so we can do separate freeing...

   Although maybe separate freeing is totally unnecessary, and we should just focus on spans which hold refs...
** DONE child waiting bugs
ok so I seem to get ECHILD when waitiding on a specific child.

and sometimes I deadlock

both of these point to dropped events.
which could happen if we complete a waitid but drop the siginfo buf.

oh hmm. this waitid implementation requires me to do excessive reads.

oh but I guess that's not a problem

ok so the echild seems to be gone,
but the deadlock is still there

also, do I need a lock around this siginfo buf?

oh the deadlock could be the sigfd wait...

let's see!

argh dang it no, it still deadlocked.

though maybe with less frequency?

hmm

i don't think we should deadlock here...

oh! no! this is exactly right, we're waiting for the monitor,
when we shouldn't be.

ok maybe let's employ my redesign that I thought about?

we, I guess...

just always optimistically wait?

then sigchld wait if we don't get anything?

still happening.

OK hmm.

next let's do the redesign.

ok so we're going to first strace it and see how exactly it is hanging, because it's not clear...

lol okay so strace I guess slows it down too much for it to hit the race, sigh.

so, redesign.

ok I did the redesign. now let's see how it shapes up.

argh! it didn't fix it

argh argh.

so what's causing this?

it *seems* like it's getting stuck in memory reads??
which doesn't make sense though

argh I wish I could trigger it reliably

maybe I should attach strace to it?

sure.

whaaat the heeckkk

so the subprocess is stuck in read, and the main process is... in "x32 mode"????
** DONE memory batching issue
   Lol okay so my batching is completely broken

   Ummm yeah

   So let's maybe just put this off until we revamp memory

   woo it's fixed
** DONE strace with local ssh
   ok soooo strace with local ssh blocks forever, because the.... remote forwarding process stays alive.

   why are we doing this sleep again?

   ugh to keep the tunnel alive.

   hmm.
   the tunnel will stay alive as long as there are connections through it.

   could I just establish a connection through it?

   hmm.

   basically.

   ok let's think about it, how can I do this...

   oh I'll just sleep 1 instead of inf, that's fine.
** DONE getting rid of processresources
   We can have it basically be a dynamic linking API,
   so that you can load a library into your process,
   even on a remote host.

   We don't actually need to dynlink though, we can initialize it with the symbols from bootstrap.
   Boom! problem solved.

   And we can break down stdtask so it has nixstore, and dynamiclinker, um...

   remaining things:
   stdstreams, environment, child monitor, ability to make a connection back to access task.

   and, mkdtemp can just use sh.

   and these remaining things, we can just put them all in one task together.
   they aren't really different from memory access!

   and then we can have fork() be a free function....

   oh and we should encapsulate this back-conn stuff!

   (and maybe I can make this extensible somehow?)

   (I guess one could just inherit from the omnibus)

   ok so what do we call this?

   native? code? linking? dynlibs? loader?

   loader sounds good actually.

   ok I moved the stuff to loader
** DONE move all the memory stuff into a memory subdir
*** memory rework
hmm this doesn't make any sense
how do I merge these pointers hm mhm hmmhmhm
mMhmhmhmmm
also I can just um, allocate a buffer and write to it.
ok hm i'll just do iovecs and no batching
what about merging within a page though? that could still be useful

yeah I don't know how to merge pointers.
but, if I want to fall back to just using near,
then I should first revamp the epoll stuff so I can async with into it.

oh I need that anyway to write/read the iovecs
*** memory stuff
    ok moving the memory stuff out into another file requires,
    as its last dep,
    asyncfd.

    let's, I guess, break asyncfd's dep on memory...?

    so, right, we want to be able to use asyncfd from transport. hm.

    this will require... making epoll work just in terms of handles.
*** merging
    merging is obvious in retrospect!

    i'll just make a new AggregateAllocation thing,
    which holds and pins arbitrary allocations below it.

    then I just have a pointer on top of that.
*** handling
    ah, cool!

    so now I can make the transport work in terms of handle pointers.

    so um...

    I guess reads don't need sizes associated with them.

    They can just be, um, directly based on, um, the bytesize.

    OK first I need to rearrange things so that I'm imported from the right place;

    I will just go ahead and make the transport interface use handles

    cool I did that.

    ok so there's a mysterious hang in ssh test_exec

    Hmm I think this might be unrelated.
*** weird hang in wait_readable
    ok so, weirdly, the waitid...

    umm, it seems likely that this waitid did return,
    but for some reason we aren't using the data returned by this read.

    hm m m

    oh hm this is in the, rsyscallconnection, not the child.

    hm, what response is this for actually?

    aha! i'm dropping responses again.

    I got back 16-bytes-worth of responses, and am not properly using both.

    very interesting.

    thought, it's especially interesting that, I don't seem to be um,
    returning the response for the epoll_wait at all.

    It appears the cancellation *is* working, but I'm just processing wrong.

    Hmmm...

    HMMM interesting,
    Ok, so the rate of buffer being longer than a single message is, relatively high.

    Ok, what if the issue is getting cancelled during an mmap? Uhh... I don't think I saw any mmaps, hm.

    Oh also mmap is local, not cancellable.

    OK well... not sure.

    aha okay so I was getting cancelled and dropping syscall responses
** move syscalls into individual header modules and register them on handle.FD
can't use mixins.
using a mixin requires defining a new class inheriting from it,
I don't want to do that because this class is ubiquitously passed around

although.
to what extent could I actually use mixins?

well, so the class needs to have all the things attached to it.

we need to know the class that we should instantiate in all the sub-classes.

well heck we could just use mixins couldn't we?
if we have some kind of base class,
then mixin all the things to it.

then handle just inherits a bunch of mixins.
hm.

then if the user wants to extend further, what do they do?
I guess they just inherit more, adding more mixins?

and override the original class?

yeah I kind of like the mixins idea, that's nice and type safe and all that.

so let's do that.

oh, hm. we need mixins for FD too don't we?

oh... this doesn't help us.
we want the structs to be able to say the type of FD,
but have that type of FD be able to mention the structs in its method types

hm! tricky.

oh hey, can I just use if TYPE_CHECKING?
and import handle to use the handle.FD?

actually I think that solves it nicely.

then I can use mixins for my runtime semantics?

ok sure.
** begin the separation of stdtask into inheriting classes
beginning with getting rid of io.FD!
** DONE memory rework
   Hm I guess the reason we don't have any nicer APIs for memory is,
   all the languages with nicer APIs abstract over memory.

   You only care about memory if you care about language performance;
   I care about memory, but not language performance.

   Hmm that seems lame.

   I like the notion of a scripting language which can set up DMAs,
   etc, all kinds of transfers like that.

   Basically just an orchestrator for the rest of the system.

   Anyway, how do we handle splitting these iovecs?
   We want to, I guess, split them at a single point?

   Well okay, here's the thing:
   We have a WrittenPointer[IovecList],
   and we want to pass it a number which is the amount we read,
   and we want to get back, this other stuff.

   I guess that's the abstraction, sure.
   We can copy that out as a helper so we can use it for other iovec things.

   boom! iovec done!!
** DONE lazily registered epoll
   Let's do it!

   ok so two tricky ones

   connect and wait_for_rdhup

   hmMmMMmMm

   wait_for_rdhup isn't an issue. . .

   we'll just start waiting on readhup up front,
   so that way we will get notified nicely, all fine, works good.
*** issues
 so we can do the epoll lazy register thing,
 we just need to not deregister when done.
 we can rely on weak refs, or maybe just the autoclose on close of open-file-description

 supporting both "async" and non-async modes of operation

 supporting cancellation so we don't drop events, but are still able to be cancelled.
*** connect issue
   we want something that will error nicely if it's not connected.

   getpeername works...

   mmmm I'm curious whether SO_ERROR will work.

   mmm maybe won't

   I think I can just use read/write then.
   or, recv/send?
   either way, connect is fine
*** overall design
    so I guess I'm very much moving towards a design where,
    you have to do everything yourself at the top-level.

    like, there are three things you might want:
    1. retry on partial write/read
    2. async operation
    3. abstraction of memory

    Our basic interface provides none of those;
    instead you can somewhat-straightforwardly do them yourself.

    It's very compositional.
*** EEXIST
    this is actually very cool - when we add an fd a second time we get eexist

    but, oh, wait, we don't learn what fd it's registered as, dang.

    hmm. so, handle is probably not the right granularity, but what is, then?

    well, let's try far fd I guess. or...

    maybe it's just a matter of registering the handles on both mechanisms.

    hm. i don't see 
*** design thoughts
hmm. ok so with our new API we can't register ahead of time...
ah but we do still get centralization of waiting.
it's just that it's not possible for a task to register on an epoller,
then not inherit it.

oh, but that makes sense: if that was possible then we could get in a situation where,
we can't delete an fd manually.

though, we could solve that by keeping a ref to each fd in the waiter;
but that doesn't scale as well as passing the epfd down.
*** misc problems
    I think this problem with the repl is just because I 
    add before adding waiters

    Nice indeed it was
*** problem
    So it seems to be that the problem is cancellation support on epoll.

    We are epoll_waiting forever, 
    when we should be returning from the epoll_wait due to activity on the activity_fd

    Hm.

    But, uh, that is not what is actually happening.

    We do seem to be returning from epoll_wait just fine.
    So maybe it's not cancellation support that's the problem?

    Hm. So why aren't we reading the response?

    Are we...

    Argh! Circular dep!

    To register ourselves on the epollfd, *in* the child task,
    requires that. um.
    requires that we...

    We have to make a syscall in the task to register ourselves.

    Um... Wait a second though, that's not right.

    Yeah no that doesn't make any sense, the connection handles are only in the parent.

    And we're not even adding um... the right fd?
    actually this does look a lot like the right fd...

    ok no it's not the right fd.

    ok so what's going on here then?

    we get an eagain from our read.

    hmm. the question is, why aren't we reading from 10 again, once we got this eagain?

    hmm, is the epoll_wait hanging???
*** hmm
    do i really even like this approach

    ugggghhhhhhhhhhhhhhh

    ok so things i like, partial list:
    1. the orthogonality approach where you don't have to use some asyncfd structure to read/write/operate,
       instead you just use that structure to do waiting
    2. explicitly having a thing that says you are registered on the epollfd.
       it would be nice if... we could... delete something via data number,
       but we can't, we need the handle.

    hmm argh.

    maybe this waiter approach is not good,
    maybe instead I should register, once, explicitly.

    instead of this super-dynamic approach.

    yeah and, and...

    and the asyncfd way is fine anyway.
    we can call it asyncfd,
    and it has the waiter methods.
    ugh. yes. this is fine.

    and it's far more explicit,
    and it doesn't have the concerns about deletion,
    or about mapping keys to waiters...

    blah blah BLAH okay.
    ok ok ok

    ok so how will I detect the negedges?

    well if I just assume at the start that it's positive,
    then I don't need to detect negedges centrally,
    which is fine.

    ok so i'll still have this waiter API,
    it'll just not add on enter.

    and, I guess, I can still have this wait_on_eagain thing,
    which I think is kinda cute, kinda fine.

    blaaaah okay so I need to revert these migration changes, and switch to afd as the center for waiting.
*** lol
    ok so I deleted the asyncfd stuff lol

    the question now is....

    if asyncfd has a handle, and a ram, and is async.

    then what doesn't it do???

    ok so things are still orthogonal,
    but AFD is my tuple which holds the orthogonal privs.

    ummm and how do we handle the whole, register on epoll thing?

    yeah having afd be my mem-abstracted fd is actually really sensible

    hmm and I can have it only optionally have the epoller?
    so that I can use the same interface for regular files and non-regular?
    if it doesn't have the epoller, it can just fall back to blocking?

    heck we can even support buffering.

    lol what happened to orthogonality, friend?

    ok, so maybe we won't support buffering.
    but, we can support the rest of this stuff!

    and, having it be my omnibus makes sense.

    ok, so let's recap.
    we want to explicitly register things on the epollcenter.

    so...

    we have some representation of that I guess.

    ok so, ok, we need to do this without, um.
    depending on things in io so we can use it from transport.

    yes we can use it in transport now, finally
** DONE fix memory allocator to save arenas!!
   I was not saving my arenas before, but now i'm getting random segfaults :<

   I probably have a bug :<

   ugghhh

   and it's hanging even without these allocator changes

   aha, it's just a problem with the persistent task!
   let's fix it.
** DONE remove all the helpers on io.Task
   and then delete io.Task

   hm to remove socket_inet/socket_unix I should first remove asyncfd,
   so that I don't need io.FDs

   ok I did that. now I can start removing them.

   Hmm, removing memfd_create, and directly manipulating memory... seems a bit awkward.
   Meh, I think it's fine.
   Abstraction bad! Direct access good!
*** mount
   hmm, hmm. mount is really annoying to do directly.
   maybe... maybe I shouldn't force memory-doing this...

   the thing is, it's going to be string constants most of the time.

   so, y'know.

   also, the target is usually a path, which we should probably use instead of passing bytes.
*** mmm
    what if we did just inherit straight through, the whole thing in one class?

    we decided not to do that before because of naming conflicts.

    but what if we just disregard that?

    hmm there are still tricky name conflicts, like execve.

    like, we definitely do want to memabstract at some levels.
    ehh ok, we'll still have two classes.
** DONE split out environment thing
   This will be our path cache to look up things in the environment.
** DONE move out command
   oh, command references rsyscallthread. annoying.

   we can just invert it though
** DONE resolve conflict between root/cwd on io.Task and handle.Task
   This conflicts with handle.Task hmm

   Hah, instead I just removed them from handle.Task
** DONE make io.Task just inherit from RAM and handle.Task
   This way we can incrementally migrate!

   Hm. This is tricky because we can't just suck out the innards of the handle.Task,
   and paste them into the io.Task.

   Oh, we could add a .base to handle.Task to get itself, if we are able to remove all the helpers from io.Task.

   OK let's stick with that plan, remove all the helpers.

   It is migrated!
** DONE remove socket_unix
   Hopefully we can replace some of the uses with AFD.

   Ugh, this usage is a list of fds. Hmm.
   meh let's stick to the current approach
** DONE add RAMThread
   We can use this instead of the current io.Task in places, I guess.
** DONE move some of these tests to unittests in the actual in6 module
   like this test_ipv6_encode test in test_io

   I "moved" it by deleting it, it was already there.
** DONE decide what to do with our two Paths
   can I really bear to get rid of our current memory-abstracting Path?

   I guess getting rid of the mem-abstracting Path would promote usage of at syscalls. To some degree anyway.

   And maybe I could have Command take a Union[Path, WrittenPointer[Path], FileDescriptor].

   hmm, we don't really need the path pointer cache, because if we're going to do a bunch of things to a path,
   we should just open it.

   Hm! Maybe that should be our replacement for mempath?
   Just open the path and use the at syscalls?

   That does appeal to me.
   Although, annoyingly, it's one extra syscall.

   Well, I guess we can always just pass around WP[Path]s, too.

   We deleted Mempath
** could have a contextvar thread
   then doing "with thread" would set that contextvar.

   that would be kinda cool.
** DONE clean up make_epoll_center
   We should have some kind of thing in local to create the one special EpollCenter that uses wait_readable,
   and for the rest, just... use activity_fd or something.

   Hm hm.

   We sure do call it a lot.

   We want to change its model to something more like Connection.

   OK, I see, we'll just assume there's an activity fd; and make the root task slightly different, see?

   Hmm but what if we want to make more epollers locally?

   OK, how about maybe... we have some kind of... interface for this? Which we... stick on the thread?

   Like, the task alone doesn't need this ability,
   so why is it on the sysif?

   Well, more specifically, usually we can't do this, right?
   Mmm....
   Usually we can't like, wake up whenever um, some code wants to run.

   Well we could, if the code gave us an epollfd to monitor.

   actually, why do we even want to make more epollcenters?

   oho. if we want to make more epollcenters, we need to do them by registering on our parent.

   right, right, otherwise it's not safe.

   for that matter, we could give our parent our epfd,
   and it could just stick the activity fd on us.

   then, y'know. um. wait_readable does nothing, and we freely, um.

   well no, we do want to differently block forever or not based on whether we have wait_readable.

   if we have wait_readable, then we don't block ourselves.

   well, theoretically we could rely on our parent to make the blocking call on epoll.

   right so we could just call something on our parent with the pointer, count, etc.

   and the sysif too I guess.

   ok, so how do we handle the possibility of being cancelled then?

   and how do we make it not quite so abstracted?

   also, we can still register on our parent.

   also also there's some interface, which EpollWaiter itself implements,
   and which we can also implement with the trio stuff,
   to do the epoll blocking.

   ok so currently all we do is, based on wait_readable we choose our timeout,
   and we use wait_readable.
   and also we add the activity fd to our epoller.

   so I think, if we just, um, register ourselves on our parent,
   then, use our parent like an AFD to tell us when to read...

   hmm we could even speculatively first do a timeout 0 one,
   then block using our parent's stuff.

   um, well, the only way to block is to call epoll itself.
   (notwithstanding silly select silliness)

   hm.
   well, regardless, we should definitely be able to register an epfd on an epfd.

   maybe we register, and that gives us a timeout and a wait_readable?

   note that we really don't need an interface, since we'll always have an epoller in our thread.

   we just need to be able to make an epoller on an epoller.

   so, maybe that's the interface? the epoller has some kind of thing which contains its timeout and wait_readable?

   or those just get passed in, I guess.

   and then we can create new epollers from epoller. ok. seems fine.

   let's start by changing that interface, and adding a new make_epoller on thread.

   uh, do we really want to move the activity fd out of our field of view inside epoller?
   isn't it, y'know, important?

   maybe, but the timeout approach is simple, so let's just do that

   hmm wait_readable is not directly that great for making an epoller registered on epoller.

   let's optimize our interface for that, and only then adapt it for trio badness
*** adding epoller on epoller
    ok so what exactly would this look like?

    I guess we'd use timeout 0 always of course. hm.

    We can have the new fancy epoller style thing. So we don't have to add support to asyncfd.
*** thoughts
    okay so let's just always have wait_readable,
    leave the activity fd on the sysif,
    make the special trio-integrated epoller in local,
    decide our timeout based on whether we have an activity fd,
    boom boom boom boom

    and make sure to comment that these contortions are,
    partially due to the need for signalfds to be monitored by an epoll in their thread.

    mostly due to that, really.
    if not for that, I could just only make the activityfd having epollers in,
    the task creation functions.

    although, maybe it's cool to be sending activityfd stuff up?
    we could have multiple epollers in a thread if the later ones add the upper ones.

    instead of registering our epoller on the parent,
    we register the parent on us.

    what about siblings then, though?
    well, they don't get monitored when we're monitoring a child,
    but the top-level activityfd means that we'll be able to interrupt the child monitor,
    to monitor the sibling.

    yeah. and ideally we can get trio to give us an activityfd,
    and then we can just exclusively uses activityfds.

    this is good but could we really get asyncio and things to give us this?
    because, it doesn't really give us pluggable event loops, since siblings aren't monitored.
    hmm. or are they? or why aren't they?

    well, they could be. it's just a matter of registering our event loop on them.
    ah yes, so there's the master loop,
    and we register on that;
    but there's also our loop,
    and we register the master loop on that.

    um hmm that's a cycle though, huh.
*** cycles
    We can't have cycles,
    we have to have one or the other,
    of an activity_fd to monitor the rest of the world,
    or a wait_readable for the world to monitor us.

    Having both means a loop;
    which would be fine, we could delegate to the kernel the responsibility of,
    cutting the loop and building a tree at each epoll_wait call.

    But, that would slow things down relative to having a static tree.

    So, maybe we just need to support either one.

    OK so... where would we hold the reference to the activity_fd registration?

    Obviously in the enclosing event loop. Er, in the child event loop.

    Oh, why do I even have to put the activity fd in the epollcenter?

    If I can find a place to store the acfd, then I don't need to tell the epollcenter about it.
    Which is clean - the epollcenter is just the root.

    Maybe we could put it on the sysif???

    That would be clean I guess.
    We already have AFDs there, so it's not an extra entanglement.

    We could/should probably switch from SendChannel to just a function we call;
    so we can drop the events.

    we should make sure to note in the docs that the reason for this split between waiter and registrar,
    is so that we can centralize our event loop across multiple threads.

    ok so, putting the epolledfd on the sysif won't work,
    because we can't do that when creating an epoller later on, lazily.

    what are we even doing if we do that?

    well. when we create an epoller,
    we're creating something that can block forever,
    and to do that we need to register our parent?
    or register on our parent?

    what if we put all the activityfds on one epoller?
    no, we have to wait on the individual thread, argh.

    well what if we put all the activityfds on one epoller?? what if????
    that would be ... useless.

    what if I just have an interface for making epollers?

    like, is there any reason I'd want to block in an individual thread instead of elsewhere,
    other than this signal stuff?

    well.

    i mean we're talking about authority to make blocking calls here.
    the sysif is that authority, innit.

    er hm we probably also shouldn't have a handle here for the activity fd. . .

    since we can close it easily then.

    could we have a handle for the activity fd on the sysif itself?

    well, how else are we going to handle closing the fd when we die?

    or exec for that matter!
    when we exec, we need to close the fd we left behind.
    well, when we unshare_files, that gets taken care of, cuz we're alone in our space
    but if we don't unshare_files, it won't!

    wait, for that matter, how does our left-behind fd in the old space,
    get closed when we unshare files?

    we clearly need a handle!
    in our own task!

    And... indeed we have one.
** DONE move stack out to sched, leave T_borrowable in handle
** DONE make make_connection object into an interface
   Then I can remove this annoying conditionality from the implementation.

   Instead I just have a single class for each possible configuration.

   And furthermore, when it's an interface, I could use TCP to do the connections too.

   Yes that seems like a good idea.

   OK so it's kind of like, we have this reference to our "Access" task,
   which is essentially locally.

   Oh, it can just be, um, like, AccessTask!
   That can be the name.

   And we have the ability to make a connection back to it.

   Or, no, we just need handle FDs, and the ability to write to them.

   We don't need to know that there's a task there.

   It's just an efficiently-accessible FD space.

   Basically it gives us an efficient-connection.

   A closer connection to our runtime.

   Better locality.

   Also it can return AFDs I guess on both sides?

   Er wait.

   We're already using an AFD in socket transport!

   Hmm

   And obviously it can't return an AFD on the remote side.

   OK sure so the question is, how do we handle returning a non-AFD that is RAM-accessible?

   We could just return a RAM with it?

   Ummmm.

   Oh hey also what the heck's this all about?
   We're using it to make things we can pass down to local stdtasks?

   Hmm that's distasteful, since that's not purely for locality.

   In fact that's what all these uses are,
   they're passing it down to other tasks.

   Hmmmmm.

   I suppose the question is, how do we know it's right task?
   Well, I guess it doesn't matter?

   Oh wait, hey, if we're passing it down,
   then we don't need a RAM anyway!

   OK so this is fine then! We'll keep the current interface - not returning a RAM if you just ask for a handle.

   If you want that handle you must know the access task some other way, I gues.

   AccessNetwork

   what's a good word for,
   "a thing which allows me to make a connection between two places",
   which isn't "Network" because I don't want to privilege networking as a way to do that.

   it's actually, um.

   something that allows me to get two fds,
   one in some arbitrary task, one in my task.

   and also give me the necessary RAM and stuff to use the one in the arbitrary task.

   and the arbitrary task is close to me, so access is efficient.

   I dunno, can we just make this more implicit?
   I already have close-by things, in the form of the SocketMemoryTransport.
** DONE make borrowing not async
   We can't run async code on exiting.
   Because we might drop a syscall result.

   And we don't need to on entering.
** DONE make batch interface use duck-typing
   actually no I won't do this after all

   Since, a proper duck-typed pointer can be passed to syscalls just fine.

   We can just assume they don't munmap, that's fine
** DONE use WrittenPointer[Sockbuf[Address]] to make accept/getname/etc calls nicer
   Basically I pass in a WPointer[Sockbuf[Address]],
   and a Sockbuf[Address] contains a Pointer[Address].

   And I return back the same thing too.

   let's go ahead and do this, yuck.

   cool cool i did it
** DONE remove FileDescriptor from asyncfd
   ok so we have a bunch of users

   ah! we can have a method on stdtask to create asyncfd from handle.
** notes while waiting for kai
Okay so....... So....

So the primary difficulty with tracking the stack is that, we can drop the ref to the stack without returning.

Well, that isn't even important! Suppose that that couldn't happen - suppose that the only way to drop it was to exit - what would we do?

Well we could call notification methods on the process when it changes state or something, like exits or mmreleases.

Or just drops the stack.

I mean, sure, we can just stick the stack on the process.

And... Well can we just always have the stack there?

It is some kind of child process from which we can free the stack... Or free the TLS... Or whatever.

And we can mark it as dead... And internally it knows whether it is in use or not...

That all seems fine.

I mean, that is better than a thread.

Oh so hmm. We could have three levels: process, child process (which allows waitid and tracks usage),
and threadprocess (which further holds stack and TLS notifications, and can be released to child process).

What about arguments to the process?

Well I guess the stack is alive. Hmm.

Yeah why don't we just save the stack in the process then we can manually free things out of it? That sounds good and fine.

The same would apply for TLS or whatever.

Hmm, free on writtenpointer should drop the value?

Or maybe not? Or something, meh. It isn't that big a problem.

Ok so I like this approach.

How do I deal with futex task then?

Well I guess I can store that along side... Things...

Right so we can have a single child process monitor thing, which allows launching things with both with mm release futex, and without.

And in both scenarios returns a threadprocess.

Seems good.

Er no, in one scenario we return, uh, something containing a threadprocess and a monitor ref,

And in the other scenario we return something containing two of the above.

Yeah I guess a better way is to have some cap thing which is, a multiplexed waiter for sigchild.

Then we can, uh. Use that to spawn children from free functions. Which have a reference to that cap, and to the child.

Hmm. Would be nicer to have some kind of cleaner way. Hm.

I mean, we need to know if a sigchild has been detected by someone else.

Because then we might have an event.

What would be great is if I could just wait on sigchild without worrying if I had an event.
Hmm. Couldn't I do that by optimistically waiting, then sigchild waiting if I don't get anything? Hm.

Huh, couldn't I do the same thing for epoll? If I get an eagain, I know all the things that would let me read are not set. Hmm.

Okay so anyway I can have a function like concurrent_waitid, which takes the process and the sigchild waiting cap.

It's kind of nice that I don't have to register processes. Could we achieve the same flexibility with epoll? Hmm yes kind of.

We just need to register the FD with epoll at some point. And then we can dynamically start receiving events from it.

Hmm do we even need to register it for closing?

I mean obviously we can just keep incrementing data numbers. Although.

How do we handle mapping an FD arg passed in, to something registered on epoll? How do we handle that mapping?

We could use object identity maybe...

We could have an underlying file object...

Ok so how would I do check?
It requires three args:

Process memgate and sigchildfd

Hmm I guess I don't need to track the futex task anywhere outside the actual syscall interface.

An rsyscallthread as I currently have it, can just be a threadprocess plus a task. (Plus memory access and signalfd stuff ofc)

And we can have an exec which is not memory-abstracting, I guess.

** investigate possible race/block forever in malloc
   We call .mmap in malloc, under a lock;
   but if that .mmap call goes out to allocate memory,
   then we'll block forever.

   But I thought I fixed this.
** DONE split out both spawn_exec
   There are two task creation things still in io.py!
** DONE split out fork task creation
   okaaaay soooo

   the issue is that I dep on things from stdtask.

   oh let's just split out the thread thing.

   we need, let's see.

   CPM, ProcessResources, we can replace Task with RAM.

   Oh, we return a Task though.

   So... yeah we need to, I guess... clean this up.

   We should probably return a handle.Task instead.

   And let's go ahead and move sigmask into handle.Task.

   ok so now is CPM and PR.

   wat do

   ok so we need to separate PR out into another module.

   and, I guess the same with CPM . . .

   since we use CPM to monitor the children.

   ok! both are split out to different files!

   wooo!!!

   I did it!

   next is more mixinification of Task I guess?

   handle is now my largest file! awesome!
** DONE remove circular dep between near and signal
** more mixin-ification of handle.Task
** DONE move out sigmask
   oh, hey, I can do this with a mixin, too, for nice modularity!
** DONE figure out good name for thing containing lots of resources, below thread
   context?
   process?

   It's kind of like all the resources needed for an event loop.

   loop?
   vat?

   It also has this network thing though.

   HmMmMmMm.

   oh hey can we call the network thing, AccessConnection?

   after all, it it basically an already-established connection.

   Yeah I like that.

   Then we can say channels maybe.

   tls?

   like, how is this related to thread-local-storage?

   well tls points to functions and things I guess,
   and could point to other resources.

   it's indeed TLS, I guess.

   though the distinction is that a thread, um,
   has to use TLS or something?

   well, ok, so, we have TLS in everything, and a "thread",
   is just something that we can monitor.

   maybe I should call it ChildTask instead of thread.

   after all we're now generic such that execs can use childtask.

   Yeah hm.

   Maybe I should come up with a new name other than task?

   Meh no, task is fine: it's something I control.

   So instead of RsyscallThread, I could call it ChildTask.

   Maybe... I could call the main thing Thread then?

   ChildThread?

   Hmm. That's interesting.

   Thread, yeah, Thread is a good name.

   I like these names. ChildThread and Thread.

   Thread nicely connotes that we control it.

   I mean, maybe I should change Task to thread, for that matter.

   Hey maybe ChildThread should be moved under the rest of this stuff?

   After all, now Task can directly exec;
   so maybe we should directly inherit from it.

   Hm. That's tricky, because that's not actually the order of dependencies.

   Not all Threads are ChildThreads.

   But, ChildThread doesn't use any Thread stuff.

   Wouldn't it be nice if one could just pass a ChildTask around?

   OK, what if, instead, we have Thread be generic in the kind of Task?

   And then ChildThread just specializes it to contain a ChildTask?

   That... could work, I guess.

   I mean... that's fairly pointless, isn't it?

   Hm. Because, we can already exec directly on the task and get the childprocess and attach it.

   Yeah I mean, putting ChildTask below Thread is silly,
   because we still want ChildThread.

   So then the API will generally be,
   thread.task.pipe? etc?

   thr.task.pipe?

   Or I can just habitually do "task = thr.task; ram = thr.ram" at the start of functions.

   Hmmmmm. oh, right, so the reason I want ChildTask below thread is,
   so that I can, um,
   pass around that Task without all the rest of the stuff.

   Except, wait, I can already do that,
   the only reason I'd want ChildTask is so I can exec.

   But I am unlikely to want that.

   So it's fine as is.
*** names
    so then I have ChildThread, Thread, and Task as my main hierarchy of types.

    should we change the name of Task to Thread?

    Then what would we call Thread?

    Oh, actually, Thread should have the shortest name anyway.

    The thing with the RAM and the task, is what people will usually want and use.

    ok, I like these names!
** DONE figure out good name for application scripts
   what should I call that directory?

   rsysapps, maybe?

   that is not a good name but it will do for now
** DONE rename StandardTask/RsyscallThread to Thread/ChildThread
   Possibly with a further MemThread below Thread,
   containing just handle.Task, RAM, and EpollCenter.

   Oh, maybe I can have ChildThread actually be a generic, like Child[Thread],
   which inherits from its type argument,
   and then I can have my own custom classes which inherit from Thread and which get put in there...

   Well maybe that's not necessary, because, fork and all that isn't going to return my custom thread anyway...
*** hmm
   OK, so, this is tricky.

   How do we have things which depend on the ability to fork,
   but which we want to include in the Thread?

   Well, we can have, um, maybe.

   Some kind of Child generic?

   would that help?

   when inheriting, we could call parent fork,
   and, just tear apart the thread it gives me,
   and augment it with more stuff.

   Wait a second lol, you can't even implement the curiously recurring template pattern!
   Heh.

   OK so any kind of thing like that would be too sketchy.

   So, I guess users will just define two types, if they want to make their own Thread?

   OK and we'll just use, um. hm.

   Well if fork.py defines a ForkThread, and a ChildForkThread.

   And, we define things which inherit from those.

   And, which, just rip them apart and feast on their insides.

   Well, there's a bit of multiple inheritance awkwardness,
   in that we'll have to do ChildThread(Thread, ChildForkThread),
   but it'll be fine.
** clean up test_io more
   There's a bunch of commented out tests,
   the Nix tests are skipped,
   all needs cleaning up.
** remove nonblock arg to make_afd
   We should just manually call it I guess.
** DONE fix ssh test ordering
   nice, we can do it in any order now
** remove syscall eof message
   Hmm, what's the clean way to shut down a thread?
   Exit it?
** switch ssh to use multiplexing
   ssh is annoyingly slow. 100 milliseconds! And I run it three times for each ssh!

   The obvious optimization is multiplexing.

   I was reluctant before, but, really, always multiplexing simplifies so many things.
   The current level of customizability is abnormal, let's abandon it.
   And anyone we don't lose that much, I don't think, by forcing multiplexing.

   Note that we'll still need the separate SSHHost thing,
   so that we can track when namespaces are shared across ssh connections.

   Though we don't really use those namespaces at the moment anyway...
** DONE remove need for careful use of unshare_files with persistent thread
   In fact there's a ton of crazy unshare_files stuff going on here.

   OK the issue is really just the same thing as with childsyscallinterface.
   We need a hard rule that if a task is in our fd space, it's our child,
   and when we perform operations on its fds,
   we monitor the child task/futex task for exiting.

   Otherwise we don't have the nice hangup guarantees!

   This applies both in syscall interface, and in connecting to the persistent socket.
** implement real dynamic loader so we can get rid of fake mappings for local and remote functions
   If we do a real loader then, I guess, uh, we can know the mappings, both remotely and locally.
** DONE use spawn_child_task in persistent?
   the issue is maintaining a reference to the stack stuff

   hmm, if we purge the stack, maybe we're good?

   Maybe I hold the reference to the ThreadProcess in the Sysif??

   hmm, when it's not a threadprocess, how do we keep its stack alive?

   when we're the ones allocating our own stack, hmm.

   I guess it stays a threadprocess, it's just not our child anymore.

   hm.

   because the task it was a child of is dead.
*** what should happen when a ChildSyscallInterface's parent dies?
    Well... shouldn't that kill off the child too? In fact, why isn't that happening?

    Oh! Wait, it is.

    OK so... how do we make the reconnect fail nicely?

    So we're in a CSI, and our parent is dead.
    We... can't necessarily tell, I guess?
    We try to do a waitid, and we notice our parent is dead.
    What do we do?
    Well, we throw an exception, natch, because we can't reasonably call syscalls in this task anymore.
    
    And here that exception works just fine.

    Well... if we successfully do an await anyway then... I think we might as well return responses.
*** unshare files issue
    wait lol, how do we learn exactly what fds are in the new fd table so we can close them?

    uhhh. we aren't copying the fd table right.

    great!!! fixed!!
*** async testing doesn't work after reconnect
    sigh, not a surprise, we don't actually work.

    fufufufufu there's some kind of truly severe issue, maybe.

    oH!!! oh oh!!!

    we forgot to register the new fds on epoll!!!

    hum!!
*** crazy waitid
   also what was that crazy waitid loop issue?

   i was spinning on waitid, that seems bad..

   oh I think I got rid of it by removing my hack to do eager reading in epoller.
   if it's something I have to worry about, I'll see it again when I next refactor epoller in that direction.
*** epoll activity fd registration
    ok so since my fds will change over time,
    we could/should use a single epfd to manage them.

    recovering my old thoughts, I think I got to the point where,
    we can manage that epfd from python.

    but, wait, if we're managing it from python,
    then we could just use the raw epfd.

    let's just manage it from python, I don't care about overhead.

    woo it works!!
** TODO still getting EINTR or something from resizing terminal
   HmMmMmMm ugh
** fix fd leaks in tests
   We close some fds, but we still gradually increase in number of fds.
** pass in single flags: CLONE argument to spawn_child_task
   rather than a bunch of different keyword arguments.

   This will be equivalent and more idiomatic.
** TODO clean up lifecycle for fd tables
   Namely this manipulating_fd_table thing. What does it mean?
** DONE fix mysterious __del__ exceptions
   need to setup fd tables on exit
** DONE fix broken test_socket tests
   There's a weird exception happening in the destructor.

   Oh hmm I think I see, are these coming from, after we unshare,
   these are the handles in that space,
   which are closed by the exec? Hm.

   But it's confusing because the fd table should still be in this map. Hm.

   Oh I see, the issue is with execing without first calling unshare_files. Hmmm.

   I guess, um... we need to set the handles up,
   but.... not put ourselves in the fd_table_to_task list, as that's a list of tasks that can be used.

   Ummm that would have the right effect, but does it make sense?
   Well what about exit? I mean right now when we exit, we leave our handles around...
** TODO abbreviate to_pointer?
   to_ptr?

   ptr?

   it is definitely the longest thing in usage.

   and maybe abbreviate malloc_struct/malloc_type as well?

   to just malloc?
   or, perhaps, new? nah malloc.
   since we don't run constructors

   maybe to_pointer should be "new"???
** TODO make an overload for to_pointer which takes regular Python "bytes" directly
** cloexec
   we do actually need to cloexec when in a shared fd space, I guess,
   otherwise our fds will get inherited if some other task calls exec without first calling unshare_files.

   hmm this is kind of security-sensitive too because the fds might be inherited by a network-exposed process.
** DONE get nix deploy stuff working again
   ok so...

   I guess the initial thing we want to get working is, bootstrapping the Nix binaries.

   We could start from a Store?

   Some store on some host,
   a thread on that host,
   and a thread on another host?

   We don't really need a full store, just the Nix binaries.

   Or, I guess, we create a store?

   If we assume write-access to /nix in the thread, then.

   I think that's fine, we'll just make a Store.
   Then, heck, we can even exec into the daemon maybe?
*** thoughts
    We need to be able to run when our binaries come from /nix,
    for example in a nix build.

    OK so it's two stages:
    One to copy the binaries to a subdir,
    and two to enter the container.

    And three, I guess, to init the database? Awkward.
    Can we init the database without entering the container?
    No, we need the tools themselves to do that.

    Ok let's just do this by hand.

    Well ok I guess we could just, maybe...

    Stick a thread in the /nix namespace, and just immediately exec the daemon?
    And talk to it to configure things?

    I mean, using the daemon is definitely better.

    OK so we just want to test Nix deploy stuff.

    That's our goal.

    So we need a working Nix daemon where we're a trusted user, I guess.
    
    also we want to be able to use this to deploy Nix on the secure servers, mmm
** implement GC for asyncfds
   We need to do this I guess.

   Currently we can drop them and it's not a big deal because eventually epoll stops getting events, hm.

   Relying on that would be interesting.

   Hmm, we could get the output of dump-db...

   We could even get the database pre-created at build time.

   That would be nice.

   Indeed, very cool.

   Then we don't need to enter the namespace at all.

   Hmm wait a second. If we want to test deploying with Nix, we can just deploy to a prefixed store,
   with the binaries from /nix.

   So, hm, this is a bit overkill.

   Oh, wait, but, to use that store, we'd need to enter the container,
   and in that container, our Nix binaries aren't available.

   OK so.

   I like the notion of just copying some stuff over to initialize.

   Meh but it's a hassle, let's just, um,
   make a thread with a store.
** TODO nix deploy is obscenely slow
   hmmmmmm, hmm, hmm.
** DONE memory corruption on batch writes
   memoyr corrruption?????
   in MY threads??
   it's more likely than u think...

   hmm so how is this happening...

   haha oops i didn't properly keep doing writes until I was done, when batching several together
** implement some const_pointer optimization
   So we can cached writtenpointers and return the same back.
** DONE rename processresources to nativeloader
   and the field to .loader
** DONE add more intermediate threads
   Why did I stop doing this?

   um, because, I really need all this stuff in stdtask, to exec.
   so, y'know.

   OK hm so write_user_mappings is needed

   Maybe we should have a low-level fork, which, um, dunno.

   Well we can skip setting up the user mappings.

   Can we skip setting up the child monitor?

   Oh hey we can skip most of this.

   OK hm.

   Technically all we need is environ.

   Oh let's just put this in a new file.

   Hmm what will we call it?

   UnixThread? Sure.

   Since it's got Environ and stuff.

   Or just, EnvironThread.

   And stick it in rsyscall.environ_thread??

   Meh let's call it UnixThread. That's fine.
** DONE move out memfd and mempath
   Hmmm.

   I think we could maybe get rid of mempath;
   it's relatively easy to do to_pointer(path) manually,
   and that's its main benefit.

   MemFD is harder. Hm Hm.

   Yeah let's just delete MemPath.

   They are both deleted! Wonderful!
** DONE make SockaddrUn.from_path take a ramthread
   Passing the individual task and ram is simply too much of a hassle.

   We should do this before deleting mempath;
   currently we use this method off mempath a lot.
** DONE move out mkdtemp
   First we need to delete mempath
   well, delete it from mkdtemp, which we did.
** DONE move out update_symlink
   hmm I have no tests for this lol

   oh well, i'll rework it and use it later
** DONE consider making mkdtemp a method on tmpdir
   That would be cool, very cool.

   Environ is low enough on the hierarchy that we could do it. Maybe.
   Hmm, it would be a pretty twisted dependency.

   Eh that would be too heavily nested anyway;
   thread.environ.tmpdir.mkdir is, while cool, too nested.
** DONE delete MemPath
   The goal of doing this is to be able to move out mkdtemp
** thought
   all these dependencies in my todos are tricky

   that's like dependencies in bringing up a system,
   which can be nicely solved with programs

   what about a personal organizer based on rsyscall?
   and wish.

   I can Wish for something to be done, and it gets surfaced to the user.
   what about dependencies though?
   how do I depend on a task?

   (and, it would be nice to be able to see what gets unblocked when I do something)
   (but absolutely not essential)

   well, I can depend on a task just through, I guess, calling some function?
   I could have parameterized tasks, which remember their args and uniquify calls to themselves based on that.

   That could be my primitive;
   some generalization of wish that uniquifies.

   Then I don't see larger task nodes, though.
   Just individual ones surfaced to me;
   but I guess that could be fine.

   (I can have repeated tasks by passing the time)

   maybe I can surface larger tasks nodes by explicitly doing uniquification,
   or having a function which is decorated to uniquify itself,
   and then those get surfaced as nodes.

   and, how do I add new tasks?
   well I guess that's just a matter of adding new threads.
   I do want to be able to add new dependencies to an existing task, though, I guess.

   Well I can just have a repl and type in new tasks;
   or some UI to add new tasks with parameters, etc etc.
   Standard stuff.

   Is there anything that corresponds to upgrading the system?
   I mean, mutating existing tasks is similar in that it requires mutation.
   Taking the same data and plumbing it through new code, though...
   Meh, anyway.
** DONE change AFD methods to take pointers in their short form
** DONE move rsyscallconnection to rsyscall.tasks.connection
** DONE rename make_connection/make_async_connection
** DONE split out mkdtemp
** DONE make connection interface with open_channels
   OK one difficulty is inheritance.

   How do we inherit this object into a new task without opening it up?

   Let's not make it an interface yet.

   OK, so the only thing that we use from the connection resources is,
   the connecting_connection.

   We pass it over sendmsg so that the new process can use the connection.
   Hm. How do we handle this cleanly?

   Well, we could maybe have some kind of,
   pass-this-fd-and-I'll-be-usable,
   thing.

   Or, actually, can we just say,
   we want to update the remote task?
   And so we need to give a new remote-task-side of the connection?

   We could make it generic, so the interface is just,
   you have to inherit this fd into the new task,
   and then the connection can keep working.

   Hmm is there some way to cleanly support connections like this,
   without having to have one from the start?

   So the issue is that if we're in the same fd space,
   and we try to inherit to a new task,
   then...

   We'll need to make a socketpair I guess?

   And then we can inherit that?
   So how do we handle that?

   Basically the user will ask for an fd, we'll make the socketpair, and return one end,
   and then the user will handle getting that fd into a situation where it's owned by the new task,
   and then passing it back to us.

   Obviously the natural way to do this is inside a function that we call.

   Doing things inside a closure is quite awkward though. Can't we make it less constrained somehow?

   Maybe if there's some reason to have access to this fd all the time?

   Well, the fd is totally internal to the Connection.

   I don't see why we would need to have access to it.

   Except for the purposes of migration to a new space.

   Well, ok, so.
   We can associate a task with a connection,
   and inherit it over unshare - easy, it just keeps working.

   The tricky thing is "inheriting" it over something not quite simple as an unshare. hm.

   OK let's just punt on this, yet more punting.
*** hm
    OK so, we're currently facing the issue of,
    we have a thing which is only valid while these guys are in the same address space,
    and if we unshare the address space,
    then we need to do some repair.

    Maybe we just need to call it when we run unshare_files?

    Maybe we can register callbacks on the task to be called as part of unshare?

    Oh, for that matter, we should clean up how we currently handle unshare. Hm.
** DONE clean up do_unshare implementation
   It's spread across io and handle in a not-very-clean way.

   So, okay, let's first get an understanding of what's going on.

   First first let's think a little about what we'd want to happen.

   What we'd want to happen is that, maybe, um.

   Well, I have a bunch of things attached to the task,
   and when I mutate the task, they all get updated.

   I mean, sure, it's as simple as that.

   Well, so, one thing is that to preserve the functionality of some things,
   I need to run some code to update them before and after the unshare.

   I dunno, we could just go ahead and do some hook-based thing, would that be easiest?

   Currently we have this function do_unshare,
   and we build a list of fds to close and fds to copy,
   and we delegate doing that to do_unshare.
   Which seems fine, I suppose.

   Hmmmm.

   do_unshare seems distasteful;
   what if we fully delegated it to garbage collection?

   well we don't have gc at the moment so.

   but, what if we did, would we have some API for this?
   hm mm.
*** hm
   okay anyway let's think.

   So right now we're...

   Wait a second. We don't need either of these functions.

   I am so dumb.

   We already have syscall batching! These functions are unnecessary!

   Yeah so, the batch close, we don't need... except we do need it, because we can't just start a thread.

   And the do_cloexec_except?
   Well... we want to cloexec. Ugh.

   Ugh, ok, okay. We can work around this though.

   The do_cloexec_except we can get rid of,
   by assuming we are actually tracking all fds?

   Well, what about ones in our space...
   which are marked cloexec...
   which are controlled by libraries we don't own?

   Mmmmmmmmmm. MMMMMMMMMMMMMMMMMMMMM.
*** hm
   OK, so as we were thinking long ago,
   what we'd really like is a way to call unshare_files,
   with an explicit list of fds to copy.

   Or, possibly, make an empty fd space then call a syscall to attach fds to it.

   But, as that is not what we have...

   Hum hum hum.

   Well, we could do the do_cloexec_except in userspace anyway.
   Like, just open the list of open files, read it all, and then close things?

   Argh, but no, we need to call fcntl on every FD...

   but again, we have batching, so that isn't actually an issue.

   That is very interesting.
*** do_cloexec_except in userspace
    We'd do the unshare,
    then stream dirents from /proc/self/fd,
    then start fcntling on them,
    then close the ones that are cloexec and not in our list.

    Currently we do:
    write/block,
    clone/block,
    waitid/block,
    read/block.

    Instead we'd do:
    write/block,
    open/block,
    getdents/block, (keep getdents in background)
    read/block,
    fcntl/block,
    close/block.

    It's not... *that* much bad.

    And we can save the write by storing the pointer.

    Also, it's not actually hurting our fastpath.
    Since we don't do this when going_to_exec is set.
*** stop_then_close in userspace
    Well, so, if we already had a task around, then we could use that to do the closing.

    We do have a task around, the existing one;
    maybe we should create a new task with an unshared space instead of unsharing?

    No, we shouldn't do that, that would defeat the point of this whole enterprise.
    We want to be able to use our existing tasks, and exec in them.
    So ok, we need to unshare in the current task;
    then, how do we close fds in our old namespace?

    Well, yeah, we can just use the GC system.
    If we track tasks that are in the old space,
    and we use one of them to close fds,
    then we don't need this hack to unshare files.
*** combined
    So we don't need close_in_old_space,
    because the GC system will handle that.

    We can just remove that argument to do_unshare,
    and have handle.Task handle pinging the GC system.

    What about copy_to_new_space?

    Well.
    The thing is that with exec, we auto-close everything with cloexec set.

    And we want to do the same with regular unshare, I suppose?

    Well, why not just de-abstract it?

    So, ok, with exec, we need to grab the list of fds we're going to keep,
    mark them all not cloexec,
    then exec out of the space.

    Actually... isn't this unsafe?

    Huh! rsyscall_exec is not safe, yeah.
    We are persistently keeping those old fds un-cloexec!

    OK, so. Hm.

    With exec, we need to unshare first,
    then unset cloexec on some things,
    then just call an exec,
    and reclaim the fds.
    We don't need this integration with unshare_files.

    Okay...

    So then with the real unshare_files, how do we handle closing fds in the new space?

    The problem is, and remains, that if there is some other library around,
    we'll have copied their fds into the new space.

    And so, therefore, we need to close them.
    Meh, maybe we just shouldn't care, and should only close the fds we track...
    Or, don't close any files at all.

    In any case, can't that closing be done after and outside the unshare_files in handle?
    They are merely stray references, not actual problems.

    We could do it in the background, as part of GC, maybe, even.
*** neat
    OK so I think we can get rid of the do_unshare thing.

    But, we still have another issue.
    Should allow registering callbacks to update objects before we unshare?

    What, conceptually, is happening?
    We have this thing, which is the old fd space;
    and we're moving to a new one.

    So, properly, we'd pass the things to be updated in,
    and update them all,
    and then come out.

    I'm being vague but I feel like I remember some other things like this.

    OK, what if we just special-case it for now?
    It's in the thread anyway, so we can just do that.

    Well... I'd want to be able to call unshare_files on handle.Task and not break things.
    Hm.

    oops i distracted myself.

    anyway I want to call unshare_files on handle.Task and not break things,
    how can I achieve that?

    well, so. if I have handle.FDs, then those will keep working after I unshare_files.

    If I want to do other stuff, then.

    OK, meh, let's just be runtime.
    For now anyway.
    We'll clean up the thread creates and then come back to it.
*** DONE start cleanup
    We'll start cleaning up the thread creation done by unshare.

    The first one we should start with is the fd table thing.

    I guess when we create a task, we need to add it to a list of tasks in that fd table.

    Then, at unshare time, we'll run a GC on the old fd table,
    right after unsharing:
    We'll look at the fd table;
    if there's no tasks, then we'll just delete it.
    If there's at least one task, then...

    Should we switch arbitrarily between tasks?
    Or, we should just use the first one in the list;
    that seems wise.
*** DONE cleaned part up, now for do_cloexec_except
    hmm why is it so slow?

    hopefully we aren't batching properly

    seems we have this request lock, maybe that's serializing it somewhat?

    ok so it does seem unusually slow, but the batching is also working.
    the syscalls not being buffered and written all at once might be part of the problem.

    waitids are also interrupting the writes,
    but I think that would go away if we were batching the writes together.

    yeah I think the bottleneck is the many-small-writes.
    let's batch those

    ok so I batched those and now it's working better

    I think we could also afford to, um...
    Do a GC first in the new fd space.

    Great and I'm defaulting unshare_files now to not do the cloexec
** DONE syscall connection rework for more batching
*** debugging
    um lol we're mmaping after execing? hMmmm no wonder it hangs for ever l m a o.

    ok so for some reason we're a-dying.

    oh.

    we assume the response is for the execveat. hm.
    interesting, interesting, problematic.

    I guess, um...

    We should switch to a new RsyscallConnection?

    Cancelling is silly.

    Yep, done, and fixes it.
*** overlapping memcpy
    Hmm. we're passing the same pointer twice to read.

    I guess this could be due to, perhaps, me calling waitid_nohang twice,
    and getting cancelled before the read completes?

    likewise, what's up with this ECHILD thing?

    no child processes? what?? implausible

    OK so the memory overlap is definitely due to cancelled siginfo reads.

    The transport doesn't actually cancel the read when it's cancelled. Hm.

    I guess maybe we should remove it from the pending read ops.

    Hmm.

    Wait a second though, we have a cancel guard. How are we getting cancelled?
    oh, in one-at-a-time.
*** waitid
    OK so it seems that we're submitting this waitid, but,
    I guess we're getting cancelled and dropping the response?

    How's that happening?

    hmm. it might be that we're waiting for someone else to submit it. and, we get batch-submitted;
    and then we get cancelled while waiting on the submit,
    so we forget that we got submitted. hm.

    syscall submission... shouldn't be cancellable?

    maybe it should be. but if we do that, then...
    we need to check in advance of the cance, whether we've been submitted.
    oh, no, wait, we need to mark ourselves as cancelled. hm.

    hmm but that would require us to re-write the struct list.

    also doesn't this cause concerns also for the transport?
    even if we mark something as cancelled, then...

    oh, the transport is idempotent. accidentally doing a read doesn't matter.

    accidentally doing a write sure matters though, hm.

    likewise with syscalls, we don't want to accidentally do a syscall and forget about it.
    cancellation...

    if someone is trying to send the syscall, we can't cancel it.

    so that's basically the concept, innit?

    isn't that equivalent to no cancellation though?

    well the brief idea was that we would mark a syscall as being performed as soon as we remove it from pending.

    I guess that useful thing there is,
    if we add ourselves to the pending list,
    but then we're waiting on some earlier syscalls to complete,
    we can still be cancelled.

    like: if someone ahead of us is writing out some syscalls,
    then we want to be able to cancel and not wait on them... hmm.

    hmm okay so we could just set the response immediately,
    so that cancellation would just cancel sending it.
    but that would leave us in a situation where we might never send it,
    if everyone cancels.

    what's the situation where we'd have to block until it gets sent?

    well, so, if we want to cancel ourselves,
    but we're already in the written memory,
    then I guess we can't cancel until, um, it's actually written out. um.

    but if we allowed ourselves to, um, change the memory, then what?

    well, suppose it was a single atomic call for all the list of things.

    then we'd allow a cancelled flag,
    and prune cancelled ones away before writing.

    remember also partial writes

    should only complete the requests i did...

    and should not remove the requests if I am cancelled...

    hmm supporting cancellation requires supporting partial writes:
    if the coroutine writing the stuff has done a partial write,
    and then gets cancelled,
    then we need to be aware of that.
** epoll insight
   The new approach, where you read first, is taking advantage of something I missed:

   You can query the readability level from the kernel! You just do it by, reading.

   So we start by querying the readability level, then if it's low, we wait to read.
** another insight
   flexsc! flexsc! batching syscalls! dedicated syscall thread!

   hey wait a second, couldn't we use these techniques in bump?
   separate out a syscall thread? for nice locality? hmmm... neat...
** DONE clean up handling of unshare for connection
** thoughts

   can do ramthread etc in the speciif cmodules related to them

   can add ram to stdtask and have childstdtask start inheriting,
   and begin migration slowly.

   rsysapps

   oh hey, io.Task can just inherit from handle.Task for migration
** DONE clean creation and inheritance for connection
   The connection interface is actually very nice.
   We'll be able to very flexibly use it with TCP,
   or some other tunneling thing,
   or whatever.

   It makes it clear that what we have is a point to point connection,
   not a network.

   Though the fact that we have an implicit reference to the task on one side of the connection is somewhat weird.
*** inheritance
   So! How then do we handle automatically creating the new kind of connection?

   We've cleaned up unshare and gained an understanding there.

   Now let's think about this connection!

   OK we'll just explicitly call unshare_files.

   OK, nice, we cleaned it all up.
** DONE batch creation of channels in connection
   We can do them async-like.

   Nice, done.
** make rsyscall_exec return RsyscallThread or whatever
   The thread is now generic enough that it can work for things that have called exec.

   The only novel thing about thread is that,
   we know we can monitor the child. (In an aysnc way? no, at all, I guess, it's a certificate)
** TODO properly handle inheritance across tasks with handle
*** sigmask
    ok so how do we handle inheriting this signalblock?

    or, really, in general, inheriting things??

    the issue is that so far we have only handle inheritance across an unshare.

    inheriting across a clone is harder, hmm.

    well, when we clone we get a snapshot of what we believe the initial state of the process is, right?
    could we use that? and move the inheritance into handle?

    suppose we do that; then how do we handle inheritance with extensibility?

    we could have an inherit_into method, maybe.

    seems complex, let's give up for now.
** support splitting memory mappings
** support passing an existing mapping as a target for mmap
   For the address argument, so that we split the mapping and replace part or all of it with a new mapping.
** rename UnshareFlag to UnCLONE
   Ha ha yes!!
** DONE migrate everything to use robust lists, or not
   Yeah hm, instead of using ctid.

   No, we won't do this.
** DONE maybe store reaper status on the task
   So that we know whether or not CLONE_PARENT is allowed, I guess.

   Eh, no, it should just fail with an exception. Not a big deal!
** DONE change child_task_monitor to issue waitids for each process
   instead of waitid W_ALL

   wait hm

   can we... redesign this entirely???

   can we just issue waitids???

   from the original thread? that would be very cool.

   the reason we can do that is because, lol, even with a central thing,
   we still can't get information about what triggered the event.
*** hm
    can we used waitid(W.ALL|W.NOWAIT) to speed things up?

    Say we make that call;
    and it tells us some pid...
    well regardless we still have to do a roundtrip, which is not workable.
*** verify that the syscallinterface actually makes concurrent syscalls
    This is, of course, the most important part.

    Yeah looks concurrent to me.
** batch do_cloexec_except allocations together with unshare_file
   That would be a nice optimization.

   Although... we don't actually need both of them at once.

   So that will over-allocate memory. Hmm.
** DONE remove usage of far for syscalls
   I notice immediately, set_robust_list
** DONE re-optimize do_cloexec_except
   We added an additional memory write when we removed BufferedStack.

   We could re-optimize it by making a Serializable for the stack, maybe.
   That would be kind of cool.
   Then you could batchop it all together...
** remove aligned allocation again???
   Since... it's actually useless?

   Since we can't use it...
** clean up static allocation
   StaticAllocation, NullGateway, etc
** DONE make things uniformly return the pointers they were passed in
   It's convenient for expression-oriented programming.
** DONE manage keeping stackptr alive, validity tracking for processes, handle.ChildProcess
   So, we could just stick it on the Process.
   Return an inherited class from clone...

   Well. Does that make sense conceptually?
   What does clone really do?
   It consumes the stack and bundles it up,
   but we have to monitor the process.

   A process is really a self-reference at that point.
   Maybe we want to represent it as such?

   Like, it's alive because it has a reference to itself.
   The running program refers to its own stack.

   Although note that we can drop a ref too.

   Hmm it's also interesting to think about, maybe, validity tracking for processes.
   So that a process can be marked as valid or invalid.

   We could have handle.ChildProcess,
   and also have waitid automatically update its validity information.
*** thought
    hmm we also need to keep alive any references that the thread has inside it.

    could we just keep the stack args alive? all of them?

    because, right, it's not just the pointer, it's also everything used by the thread.

    how would we free them? hm.

    I guess we could just invalidate them bit by bit.
** maybe implement borrow interface for all dataclasses? or maybe just Stack[T_borrow]?
   Then they can contain pointers which we can then borrow?

   hMMMmmmmmmmmMMMmm
   so the trampoline needs to... maintain references to these guys I guess?
   no that's too extreme, fuggedaboutit
   but how would we do it?
   I guess we get an WrittenPointer[FDList], and...
   I guess we'd maybe have WrittenPointer override borrow, so that it borrows things inside the thing?
   and... we require every dataclass implement borrow?

   Oh, we can constrain it to just things which are arguments on the stack.
   We already have this constraint for sendmsg,
   its tree implements borrow.
   So we can do the same for Stack.

   We need to do this already to borrow Trampoline. So might as well...
** add async perform batch, but run it with a null async runner
   So we can be sure it doesn't actually perform effects.

   Well... we should maybe have direct_syscall do a yield, just so it breaks when running in null async runner.
** DONE handle resumable function for epoll by just re-running repeatedly
   ICFP-style, woo!

   As long as it doesn't have side effects, we can just re-run it indefinitely,
   storing previous results.
** DONE general purpose syscall batching
   We could do a general purpose batching thing.

   We would, I guess, stick the interface in at the SyscallInterface level.

   We'd return None or 0 or something from all syscalls

   Hopefully that would be enough so that we don't have to stick the interface at the task level.

   Simply doing things in parallel is fairly powerful batching.
   I don't think we need any more.

   The notion would be, we have later syscalls whose result depends on the former;
   and so we could, I guess, pipeline them together.

   But we don't have a way to do that in our syscall threads;
   we have to round-trip for each result.
   So just doing things in parallel is sufficient to exploit all the batching we can do.
** DONE fd.near in serialization
   OK so uh, we are serializing the fds.

   But... don't we need to borrow them in the task? Hm!

   hm hm hm.

   we can't just borrow them, right, because we need to keep the reference around,
   like, stick it inside the writtenpointer.

   so the issue is,
   if... the fds that I have a reference to,
   are held by a task,
   which then unshares away,
   and those fds aren't otherwise referenced by the task that actually makes the syscall,
   then. we have an issue.

   merely borrowing doesn't help, because the borrow goes away before we actually make the syscall.
   really we need to attach the fds to the writtenpointer.

   well...

   oh!
   wait, do we even have an issue?

   because, can't we borrow the fds in sendmsg, using the writtenpointer?

   aha, problem solved.
   we indeed, have now, just borrowed the fds in sendmsg, using the cmsg interface.
** using newtls to cut down thread bootstrapping cost
   So, I think we could use newtls to avoid the need for a stack...?

   Oh, wait, no. We at least need to write a pointer to return to, on the stack.
   Hmm I guess we could have the syscall handle dispatch on newtls.
   If it sees a different newtls, do something different?

   That would only be worth it if we could remove the need for any writing.
   We could maybe stick the fd number into newtls?
   Then write the rest of the bootstrap information there?
   Eh, we need memory to save things across syscalls...
   Or do we?
   Well, we at least need memory to read and write responses!
   I don't think we can escape allocating memory, but maybe we can escape writing to memory on fork startup?
** nicer APIs for sizing buffers
   So currently it's not clear how to size a buffer for, say, some cmsgs.

   Would be nice to make that simpler.
** batch operations across more than one task??
   So that if they are in the same address space, we get cheapness...
** why aren't the pointers in the IovecList being invalidated by the sendmsg call's split operation?
   reuse buffer from in previous sendmsg call
   in subsequent call to recvmsg

   the trick is that the buffer.

   oh. huh. the buffer can be reused. it's not invalidated.
   at least that's what test_io appears to be doing.

   what? why not???

   same for cmsgs too!
   very curious
** turn pointer into an interface (which throws exceptions if not readable)
   but how do we establish persistent references to pointers across calls to fork?

   well, we can't even use task for that;
   we can't unshare_vm so it's truly useless to be able to do that with task.

   heck, it's even kind of silly to use task for it for fork.
   would be nice to establish references before forking.

   ok so, let's figure it out.

   we need to figure out how we are going to manage ownership of the pointer
** improve exceptions for path operations by using path in WrittenPointer
** throw an exception if we try to free a pointer while it's borrowed
** consider naming enum which can't be shortened to just a prefix, SpecificPREFIX
   i.e.,
   the MSG flags for sendmsg can't be named MSG since there are other such flags.

   But I could name the enum SendMSG.
** DONE make socklen functions return Pointer[Pointer[Sockaddr]]
   Essentially we'll transform the type of the socklen buffer,
   and augment the serializer,
   so that first you read from the socklen,
   and that tells you the valid pointer for the sockaddr,
   which you can then read.

   oh, and nicely, we can just not invalidate the sockaddr pointer until you read socklen.
   so you can always fall back on just reading all of that and guessing the type
** somehow enforce that the user read pipe/socketpair/recvmsg buffers
   So that they don't leak file descriptors;
   this can even happen maliciously in the recvmsg case.
** DONE better execveat interface in general
    It's pretty awkward to have to go through the thread. Hm.

    ugh what, wait_for_mm_release isn't even used :(

    ah okay, but we do get mmrelease as an exception

    cool cool

    could we get rid of thread then?

    confusing

    the principle is, we need to be able to know when the other side has hung up,
    inside the syscallinterface.

    if we know that, then yes, we don't need thread.
** DONE finish refactoring to thread
** DONE fix up rsysapps
** DONE move out repl to separate package
* DOCUMENTATION!
  Time to write it!
** wait actually no
   because I need to finish refactoring to Thread instead of stdtask first,
   and also need to fix up the rsysapps.
** strategy
   write main thing first,
   then write docstrings,
   then re-write main thing
** split
   two tutorials:
   single-threaded and multi-threaded

   take thr: Thread as arg in both

   prerequisites:
   basic understanding of "async/await" in Python;
   basic understanding of type hints in Python.

   maybe I could explain async/await?
   in rsyscall, the low-level syscall function is marked async;
   that is the only fundamental async function.
   this means if a function wants to make a syscall,
   it has to be async;
   and conversely, if a function is not marked async,
   then we know that it does not make any syscalls.
** single-threaded
   explain:
   just like conventional python
   can be used as a more low-level, more featureful interface to Linux in Python

   it has inotify, for example! also rtnetlink.
   there are plenty of interfaces that are missing,
   we are happy to add those,
   just file a feature request.

   explain ram

   explain that headers are laid out like linux;
   use example of finding "SEEK".

   maybe:
   from rsyscall.local import thread as thr
** multi-threaded
   tasks
** ooh, make a choose your own explanation
   will need to split up categories of programmers, I guess.
*** are you enthusiastic about capability security?
**** do you know about capsicum and the general usage of FDs for capsec?
     if no, then explain capsicum, notion of using file descriptors for capsec
**** then
     rsyscall is a framework for deploying distributed systems made up of capability-secure components,
     which take already-established communication channels as file descriptors.

     with existing tools, it's tricky to do something even as simple as
     "start process A here, process B there,
     and pass down one end of an already-connected bidirectional socket to both of them."

     rsyscall makes this, and much more complicated scenarios, possible without tears.

     In some ways it's complementery to CloudABI (an awesome project!);
     CloudABI makes it possible to write capability-secure components,
     and rsyscall makes it possible to run and orchestrate capability-secure components.
     (Of course, the CloudABI people have their own ideas for how to do this orchestration, I'm sure!)
*** do you like ucspi stuff
* getting better at asking questions
  when sending a question as an email:
  - include a summary of relevant discoveries and thoughts so far

** hm
   hmm, the thing is,
   previously I've been in positions where there are people with more knowledge than me,
   who have some responsibility for answering my questions and therefore tolerate low-effort questions
* misc thoughts
** lambda for storing references idea
   if I have some data which references other data,
   but which in its final form drops the references...

   or...

   it needs to be serialized...

   if I have something which references other data,
   but which needs to be serialized to be used,
   
   then, storing it as a lambda instead, is a nice trick.
   how do I think formally about that?
*** dynamic scope???
    implicit parameters?

    silently passed down?

    in the global environment?
*** captp-style exports
** gc as an effect
* tags
find -name '*.py' | etags --language python --regex='{python}/^async def \(.*\)(/' -
* signal handling
  hm. the good signal handling API would be,
  the thread that got the signal is suspended,
  and someone else is notified.

  how can we do that?

  well, hm. maybe we can sigstop ourselves when we get a signal?
  sure, sure...

  and how do we notify some other signal handling process?

  well our parent will get a notification of us stopping.

  kind of interesting to consider. hm.

  oh hey how does userfaultfd work here?
* epoll notes
# We are only guaranteed to receive events from the following line because
# we are careful in our usage of epoll.  Some background: Given that we are
# using EPOLLET, we can view epoll_wait as providing us a stream of posedges
# for readability, writability, etc. We receive negedges in the form of
# EAGAINs when we try to read, write, etc.

# We could try to optimistically read or write without first receiving a
# posedge from epoll. If the read/write succeeds, that serves as a
# posedge. Importantly, that posedge might not then be delivered through
# epoll.  The posedge is definitely not delivered through epoll if the fd is
# read to EAGAIN before the next epoll call, and may also not be delivered
# in other scenarios as well.

# This can cause deadlocks.  If a thread is blocked in epoll waiting for
# some file to become readable, and some other thread goes ahead and reads
# off the posedge from the file itself, then the first thread will never get
# woken up since epoll will never have an event for readability.

# Also, epoll will be indicated as readable to poll when a posedge is ready
# for reading; but if we consume that posedge through reading the fd
# directly instead, we won't actually get anything when we call epoll_wait.

# Therefore, we must make sure to receive all posedges exclusively through
# epoll. We can't optimistically read the file descriptor before we have
# actually received an initial posedge. (This doesn't hurt performance that
# much because we can still optimistically read the fd if we haven't yet
# seen an EAGAIN since the last epoll_wait posedge.)

# Given that we follow that behavior, we are guaranteed here to get some
# event, since wait_readable returning has informed us that there is a
# posedge to be read, and all posedges are consumed exclusively through
# epoll.
received_events = await self.wait(maxevents=32, timeout=0)
# We might not receive events even after all that! The reason is as follows:
# epoll can generate multiple posedges for a single fd before we consume the
# negedge for that fd. One place this can definitely happen is when we get a
# partial read from a pipe - we're supposed to know that that's a negedge,
# but we don't. It also can happen in an unavoidable way with signalfd:
# signalfd seems to negedge as soon as we read the last signal, but we can't
# actually tell we read the last signal. er wait. hm. ah! it's a partial
# read... since we're doing a large read on the signalfd...
# it's weird. okay. whatever.
# signalfd sure is weirdly designed.

# So, if we're woken up to read from epoll after seeing that it's readable
# due to a second posedge for the same fd, and then we consume the negedge
# for that fd, and then we actually do the epoll_wait, we won't get the
# second posedge - we won't get anything at all.

# TODO maybe we can rearrange how we do epoll, now that we know that we
# can't guarantee receiving events on each wait. We might be able to improve
# performance. Or maybe we can just start specially handling streams.
* hm

interesting question.

do-notation with no possibility of runtime overhead.

(do notation is delimited continuations, etc, whatever)

how would you do this?

basically you want the continuation operator to happen only at compile time.

you could have some kind of macro-level operator that does this...

it's also kind of like what I was thinking about like,
a module which can compile itself differently.

hmm, hmm, staged compilation

ok so the notion would be explicit manipulation of function blocks, that would be one way.

instead of closures, we just have jumps.

then we could do it all at the value-level.

hm. but we want to be able to close over function pointers.

(and have the linker resolve those)

it's interesting.

I think it boils down to having nicer approaches for staged computation;
you want to be able to manipulate function blocks and ban them from being closures,
but you want those function blocks to be able to close over other functions because it's a hassle to manage that manually.

well what if you just have an ~effect~ for this?
you manipulate these function blocks,
and you get a closure...
well hm.

and you could explicitly model your program as a script which gets run to generate an executable which does what you actually want, and optimizations are just pluggable libraries.



so you would explicitly model your program as a script which gets run to generate an executable which does what you actually want once it's passed all its arguments.
IMO, such an approach is the future anyway, it's just a matter of making it ergonomic enough...


ooh, and then essentially your program becomes  you would essentially model your program as,
it's 

to me it seems to have a clear correspondence to the vision I described earlier - 
a staged language where using runtime values and doing computation on them is an effect (more probably, a range of effects) -
the effects correspond to the extensible "symantics" that oleg describes

in some sense the dynamic linker is the symantics for things...
* hmm
  not sure how you'd do type inference for this,
  but if the k was an implicit parameter,
  then you could win.

  if you were doing

  fn k { unify(k, l, r) }

  then you could maybe just have k be an implicit parameter/dynamic variable

  so, unify(l, r) would pull k from what's available in the current scope by type,
  or pull it from the current value of a dynamically scoped variable


  that requires that "unify" on its own identify what the function you want is.

  but when calling it as a method that doesn't work

  how does this lambda work?

  does it get passed to another function?

  you could maybe have $.unify(l, r) where $ is dynamically bound,
  as a syntax for doing that

  the point is, where do I get the unify function from?
  answer: I need to get it from "self".

  self could be dynamically bound?

  ok, yeah, that would make sense, you could have a dynvar self, that you can bind whenever,
  and then the function lookup happens on "self" first

  and then... well, the question is, how do you keep the method call/message send from happening immediately?

  well, do you need some syntax for that?

  well, if the function doesn't apply, then you could just wait to apply it.

  try to apply it, but if you fail, wait.

  that's not very type-safe though.

  like, if we just wait to apply it,
  then we just never apply it, if it's broken!

  we would want to not resolve the function based on that.

  maybe if we knew the type in advance?

  oh, some clever helper class could take a message call and return a function making that call

  that's a dynamic way to handle it.

  but it would be nicer if we were typed?

  right so if we knew the type in advance...

  then we would know whether we can apply the function or not.

  oh, you could dynbind one of these function maker things as self,
  so that all method calls become delayed.
* main
  august 5th to 15th
  indianapolis
  early july
