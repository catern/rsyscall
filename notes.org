* stuff
so do we really actually just one a single thread,
per connection,
which just blocks on calls,
can only support one call at a time,
easy, simple to implement.

doable in C even.
with a C interface

so okay
suppose we had this.
then the question is thrown into sharp relief:
how do we multiplex over multiple processes/hosts???

duh
we have a file descriptor interface

which is readable
when the thread
is runnable/has returned a value

also, I *could* pipeline them:
I could send multiple at once,
and get them back one by one.
(In the same order? Yes)
But let's not focus on that, that is not that important and also maybe bad.

this is nice and simple

so this is kinda translating a completion interface into a readiness interface


anyway so I have some file descriptor,
which allows me to run any system call over it,
and it's readable when it's done
(vn one at a time ofc?)

okay is it just as simple as
I have a C library

with void read_call(stuff)
and stuff read_response(void)

then I don't have to expose any serialization stuff.
i can just use structs internally

actually more like

error read_call(int procfd, int fd, size_t count);
int read_response(int procfd, char* buf, size_t bufsize);

oh and conveniently I can just completely avoid defining my own headers and enums and structs and such,
since i'll just pass things through blindly

ugh although pointers are difficult, scatter-gather IO is tricky (though actually probably I should just not support that)

look for documented linux syscall interface
think I heard about that for fuzzing

writing this by hand really doesn't seem that bad

consider the simplest scenario which is where we don't care about being nonblocking across host
we just have the interface be,
exactly the syscall interface but with an extra sargument for the procfd
the sysfd

also there are multiple ways to achieve the goal of asynchronicitiy

we could have ujst epoll_wait be asynchronous and monitorabel by readiness for completion
we cuold have a generic way to await on readability of any fd on the remote side
bridging them, that is

what about cancellation?

we might call an epoll_wait,
and then readiness tells us completion,
but what if we want to make another call in the meantime on this system?

we need to be able to do that
so having syscalls be cancellable seems not very generic

also calling epoll_wait is not right, we should call that in "userspace",
and only see readiness in the main interface

okay, if I wasn't asynchronous.
i'd just,

the calls would all look like normal calls,
with an extra argument.
and we'd just have some internal serialization,
very normal.

maybe to monitor readability I need another connection?
that way I don't have cancellability?

but what if I want to monitor readability of a bunch of different fds? i'll allocate and never free connections.

so the ability to make a call then monitor for readability to see when it's done, seems good and easy.
just splitting up epoll wait into two calls achieves it automatically.
but, it's not cancellable!

what would this be in a protocol level?
if i had a stream dedicated for readability notifications...

having all calls block, means I nicely have to explicitly allocate new threads,
through my previously-thought-of mechanism of clone passing an fd. nice.

it's like an interrupt?
I get an interrupt for readability?

oh and what if I want to get readability notifications while in the middle of another syscall?
well, impossible.

resume me when this guy is readable
seems reasonable.
extensible to,
send me a message when this guy is readable,
which is,
be readable when this guy is readable.

readability really is easiest for the kernel to implement.
the kernel just translates interrupts into wakeups of blocked threads.
easy peasy
all we need is some way to indicate what interrupts we're interested in receiving

when we enter an epoll_wait on some set of fds
we say we want interrupts for all of them.

to do the same for my syscall fd,
we need to explicitly request interrupts before waiting.
then un-request interrupts after we're done waiting?

but in practice the way we get an interrupt is by getting completion notification for a syscall.
so isn't it best to have that be the central conceit?

maybe, maybe.

what other ways could we be notified?

(really the notification/interrupt is the resumption of the thread on the other side.)

still wonder what this is at the protocol level

okay but, if we get a completion notification for a syscall,
we need to be able to cancel that syscall.

i mean, maybe we just have two threads.
one for readiness and the other for actual syscalls 

oh, i really can't have an all-blocking interface, because, the network hop is so expensive. it will be weird...

so
i could have a separate connection/thread for readability notification
and just, i open/close those threads as I want

ugh having a separate thread is inconvenient

maybe i can just have a separate mode

okay so I guess readiness wakeup/readiness notification is just,
a bridge between the interrupts of the remote side and the local side.
you'd want the same with futexes or whatever

also you should return the thing to await on
(or maybe not)

so the ideal is then:
an asynchronous syscall interface,
taking an opaque thing and returning an opaque thing to wait on,
(maybe)
augmented by a cancellable readiness notification registration thing.
* notes
C library interface with a bespoke protocol underneath
two issues: defining custom logic for each syscall, and defining a custom serialization/protocol for the stuff.


It would be nice to instead have a protocol interface
no, no, there's no need for that.
I can just poll on the fd and do the operations.

no.
a protocol is better because then the user can handle IO,
*and* handle retrying on partial reads, and all that stuff.

so all I want to get in my library, is a stream of data in,
which eventually leads to a fully parsed response.

i could just use capnproto serialization.

audit and seccomp-bpf aren't suitable because they don't actually look at the pointers.


nice rqusetest b 

* HAHAHA
vfork!
implementing the cancellable readiness thing with select!
write memory, read memory!

RETURNING FROM EXEC/EXIT LMAO YES BEAUTIFUL

wonder if the registers change


now what I need to do is, call syscalls *really* directly so glibc doesn't get in the way with this errno stuff.


exec makes a new stack for you that the process will run on, so you don't have to make your own stack.
but clone for threads, you have to manually creattoeac


to ask on irc:
why SHOULDN'T i vfork all the time?
and, how CAN i make syscalls directly without errno, without assembly


also can I get rid of the read/write by just,
reading and writing from the connection file descriptor?

MSG_WAITALL does the write part, as an argument to recv...
and send just alwys blocks, for the read part!

then i'm down to a single function call in a loop

recv MSG_WAITALL on stdin, cast to syscall struct, make syscall, write response.

does splice write all the stuff? without blocking?

maybe I don't even need to go through local memory?

and I guess I'll just use the exact interface of seccomp trap to userspace?

you could implement filesystems with seccomp trap to userspace.
the BPF filter could even make it efficient, kinda!

conceptually, it would be nice if things were just modeled as,
we have a pipe full of systemcalls from the user,
and we have a BPF program attached to that which filters it,
and we process the ones it sends us...
and sometimes it just kicks them back to the user program to be run there.
but I guess that's close enough to what we have.

 
* syscaller musings
Hmm.
I'm tempted to have the syscaller be a separate structure,
and have separate notions of a pure ProcessContext with associated FileDescriptors
(which don't by themselves let me do operations in that process)
and some means of accessing a ProcessContext, which any individual IO structure will use.

No, that is absurd. We wouldn't be able to actually own and close the fds!
To properly do this we need ownership. Which means a given FD must be tied to a ProcessConnection.
So an FD holds a reference to a... Syscaller? or a LowLevelIOInterface? Hmm...
to close we only actually need the IOInterface.
but to do further operations we need a Syscaller.
so it seems like the basic thing should just have the IOInterface,
while more stuff has the syscaller.
Maybe we should just state that every connection has an associated page of memory.
Therefore it is already fully-functioning.
But isn't there more stateful stuff?
Like pipes, say!
To do a splice, I need a pipe!
And pipelining demands still more sophistication.
Although I can only pipeline through one process connection at a time, since the pipelining will cause problems otherwise..
I guess pipelining is controlled at the ProcessContext level.
Remember what I originally wanted, when I was planning on using serialization: A simple way to do remote syscalls
Now I have to resort to pipelining for good performance, I guess.
Or do I? Let's just stick to something simple:
Each fd is associated with a SyscallConnection or something,
which contains all the resources needed to make any syscall.
Likewise with each piece of memory? But then what of the syscall buffer and the remote sides of the syscall and data pipes?
We'll just require the syscall buffer to exist, and the remote sides of the syscall and data pipes are internal details.
Wait, I think that circular reference is fine.
It will require use of with to preform orderly destruction, but that's OK
OK, so all memory and fds can have a reference to this Connection thing, including the internal ones.

Maybe if my abstract interface is just, perform any syscall?
Should I abstract over memory?
Obviously yes.
Should I abstract over fds, and only provide owned ones?
can't really do that in general.
also, what if I wanted to use memory in the remote place?
I guess I should only abstract over *memory involved in the system call*.
but er no sometimes I might want to pass a specific pointer to read into *remotely*. for the side effect.
well maybe read() can take an optional buffer argument that is either a local buffer or a remote buffer.
guess that could be good.
nah forget it I don't think I'd ever actually want to deal in memory in the near term.
so let's just abstract over memory completely.
and that'll be my interface!
a buncha methods returning ints!
with memory completely handled.
and then i'll build my common fd abstractions on top of that.

What about concurrency?
What about nonblockingness?
Above or below?

well ideally when we call this thing it would only block the current task.
and other tasks would be left free.
but, note that the object is still "globally" blocked.
we can only have one person calling one of these syscalls at a time.
so there's all the same deadlock issues within a single "process"
so maybe it's therefore not so bad that a native Syscaller would hard-block when we call it?
it's a protection against anyone else interfering with its process, y'know...
could we make it truly async? not without some effort.
to make it truly async would require using a native IO library.
ultimately we'll have to make a blocking call in our thread.
I think it is fine that they are marked async. It demonstrates that they may block.
And shows the silliness of an explicit marking for async? though it's a nice effect system...

man. a nice serializable thing which is automatically nonblocking would be exactly what I want,
since it would be a cross-language IO library.
bah! oh well! we have no cross-language type system, so we can't
computer science is not yet advanced to perform such feats.
also it's dubious because we can't really re-expose exactly the syscall interface.

so, anyway, handling concurrency above the interface is the right thing, I think.
even though it leaves this weirdness of tasks marked async, hard-blocking.
but ultimately we need to block, so...

ah, and how do we bridge this into the native trio thing?
the remote syscalls work fine, I can just do a select and it's good.
good-ish...
I'd need a nice queue underneath the syscall interface...
to link up call/response...
since multiple people need to call at once actually...

note: try to perform IO first, and only select if it returns EAGAIN

so anyway, briding into the native trio thing works fine with remote syscalls,
since the object is truly async,
and doesn't block other tasks.

but the native calls! aie!
I guess I'd have some thing where I, uh.
I forward epoll_wait to trio so that it really does not block other tasks?
then I rely on the EAGAINing of everything else?
I want to interrupt the blocker guy if I need to block on something.
I really like the "make a syscall to wake it up" thing, it should Just Work.

actually okay, maybe I do need to just expose a wait_readable call.
because I can't do the select hack in userspace...
and it returns either "readable" or "EAGAIN" though I guess the fd could be closed under me, eh.
no wait it can't because of blocking, eh.

so okay sure I'll expose the wait_readable call, whatever.
since I can't expose the file descriptor number of the remote thingy fd in userspace.
but wait, I need to do that to avoid it when dup2ing...

well, even with local syscall I need to cleanly handle dup2
ok, I will avoid dup2 problems by making the remote syscall and data fds be high numbers.
local dup2s don't have any serious problems, if I overwrite something, oh well.

hey neat, when I vfork the exceptions in the subprocess propagate back to the main process.

oh anyway so when I call wait_readable with remote calls,
I can also make a syscall at the same time (like with native),
and it will interrupt the wait_readable (not like with native)
but I can make it like that for native. just cancel the wait under the hood?

when I make another syscall, it interrupts wait_readable automatically.
then the async layer will not wait_readable again until it's finished performing other IO.

I guess that's a nice invariant.

okay, we'll just have wait_readable only return when the underlying FD really is readable.
And the conceptual reason to call wait_readable: It doesn't take the lock on the object.
Other syscalls can go through while we're calling it.
That will be nice and uniform with native.
And we need to have such a wait_readable call as a primitive to avoid deadlocks.

That is, a deadlock where we call some blocking syscall, and it blocks, but
then we realize we need to call some other syscall to make it
unblock.

Right, the reason we need the primitive isn't because we want to get asyncness in there,
it's because we don't want to take the lock on the object.
which is conceptually the same as not globally blocking

* naming vfork thing
So the real system call wouldn't be clone, it would be... unshare?

Well, vfork is the best way to view it I think. It's just a vfork that doesn't return twice.

Meh, I'll call it sfork.

* 
if syscalls were RPCs

you could set up the new process/do the exec stuff in a remote process

it would be nice to have an exec that creates a thread in the same way as clone

but really clone doesn't need to take a stack argument, does it?
why can't you just conditionally return?

yeah and I mean, why can't I just say, start thread running with these registers

exec and clone are kind of the same thing


anyway if I could RPC to another process space, I could set up its memory with mmap just fine.
then kick it off and let it run.

adding RPC to another process space...
targeted, surgical interventions that massively increase power

anyway if syscalls were RPCs,
and you could just start with an empty address space and map a bunch of things inside it,
then start a thread inside it,
then that would be real cool y'know

unshare
* accessing the environment and args over rsyscall
okay so args and env are at the base of the stack. hm.
so we could access them that way.
that's a bit silly though, let's just get it externally
* handling entering the child process
  I guess it seems fine.
  Maybe I'll wrap the SyscallInterface in something more structured and nice.
* what does the FD hold
  Currently it holds a SyscallInterface.

  And since we mutate the SyscallInterface when we change process...

  There's nothing to update/convert in the child process.

  but wait so, this also means that if I open things in the child process,
  then when the SI reverts back,
  I won't be able to see that they are bad.

  I think I should not be mutating the SyscallInterface, but rather creating a new wrapper thing.

  Yeah, and then I should mutate the ProcessContext to indicate that its SyscallInterface has stopped working.

  Technically the processcontext should support multiple syscallinterfaces.
  argh. so it's desirable that, uh...

  OK, so maybe I should have a single TaskContext,
  which references the Process it runs within,
  and has a single fixed syscallinterface.

  why can't FDs just have both a syscaller and a process?

  well what happens when the syscaller switches what it's referencing.
  or rather what process it operates within.

  I guess one concept is that, um.
  our task moves to another process.
  so er.

  but argh, how do we then prevent other things using that syscaller while in the child process?
  when it will be wrong?

  okay so maybe I have this notion Task,
  which holds the syscall immutably,
  and mutates what context it's in.

  and then each FD holds a reference to both a Task and a FileDescriptorTable.
  and before doing anything,
  checks that the Task and FDTable match.

  That makes sense.
  Task is, then, essentially the same thing as SI,
  just a slightly friendlier presentation.
  well, a nice wrapper anyway.
* epoll/asynchronicity
  Now I need to decide how to handle this.

  Maybe have another wrapper for fds?

  I guess the epoll guy has to own the fd since it has to remove it

  Don't forget to read/write first and only on EAGAIN start blocking.

  Should the epoller be tied to the task?

  I don't see why.

  Well, of course an epoller holds a task, of course,
  because it needs to be able to syscall.

  FDs can be switched between tasks easily though. Or should be able to anyway.

  Maybe an epoller should inherit from fd?
  Can you actually read from an epollfd?

  Oh, I certainly should have an epoll file descriptor in any case.

  And then I have a thing which owns that...

  Maybe release() should return a copy of the FD object instead of an int?
  It's just a way to mutate the object into giving up the fd?

  OK so the question of whether I have an epoll fd that is just known on the other side,
  or whether I have a wait_readable call.
  
  Hm.
  I guess I can just epoll_wait anyway.
  As long as no-one else is using the thread/epollfd...
  Then can't I just make blocking calls?
  If there's no other tasks waiting on things other than in my world?
  Which is probably the case?

  Of course, the epoll_wait needs to be interruptable.
  So that other tasks can cause a call.
  For example, some task has an FD become readable, I read it,
  and then I want to write that data to another task.

  Tricky, tricky.

  Except it's not really the case that I'm the only thing in this world.
  Both native and through rsyscall.
  Others are waiting and might cause me to want to do something in a task.

  Tasks, tasks, tasks

  Single language runtime, multiple tasks,
  freely scheduling on any task,
  tasks tasks tasks

  Tricky, tricky.

  tasks.

  Essentially we're providing a way to work with multiple Linux tasks,
  as objects inside a single-threaded language runtime.

  But when we call a function on one of them, it locks the object.

  We want to be able to interrupt those functions.

  Ideally this would be implemented by,
  when we perform the call to an epoll_wait,
  there's an additional thing that can wake us up:

  More people calling into the object.

  Ho hum he...

  I guess this is an issue of multiplexing?

  We could write a nice API where we run each system call on a different task.
  The ole, allocate thread pool and run syscalls on them, approach.

  But instead we want to multiplex a bunch of system calls on a single task.
  And in some sense, we want to get notified when they return successfully,
  but we still want to be able to send new system calls...

  wait_readable does seem fairly good.

  OKay so I'll stick to wait_readable being abstracted by the interface,
  I'll code using trio-only libraries,
  and, yeah!
* file descriptor .release()
  release() allows some kind of linear movement of types through the system,
  so we ensure they get closed.

  man a linear typed language would be such an improvement for resource tracking.
  blargh
* important design notes
  dup2 takes a file descriptor object and never ever takes a number
  subproc has a method to convert objects from the parent process to the child process
  cloexec is on by default, and unset manually when creating a new process

  wait_readable only returns when the underlying FD really is readable - it doesn't take the lock on the object
* concerns
  okay so...
  what the heck

  if I pass an fd into a subprocess, does it then get the ability to change my own nonblock status of that file description?

  fml

  how to test, hm...

  guess I can just test fcntl

  okay so the internet suggests it really does work that way
  even for stdin/out/err

  fml fml fml

  is there a nice clean Linux-specific solution?

  so I just realized that since struct file is shared between file descriptors,
  O_NONBLOCK can't be really set on file descriptors that might point to shared (non-owned) struct files...

  argh okay so let's think about how we could model this if we have to.

  also terrifying is the prospect that it's not just shared across dup'd fds,
  but also across separate opens of a fifo, as that one guy said. but that seems dubious

  so we need some way to represent whether it has
  extra references that are outside my set of fds

  and also when my set of fds passes outside my control??
  no I guess I don't need that, that will just be a stray files/task thing

  okay so
* file object management
  we'll have FDs point to a FileObject

  And we'll have FOs have a list of FDs?

  And we'll use this to implement something like Rust's cell system?
  A runtime borrow checker?

  If we have an exclusive FD to something, we can perform FO mutations.
  Otherwise we can't.

  And we'll also have a flag on FO, "leaked".

  Or wait...
  I guess when we pass something to a subprocess, a file descriptor will be left behind that is open.

  Oh god
  When we create a subprocess we create a bunch of new references to a FO.
  So we add them when we translate, right?

  But, ones that are marked cloexec are closed once we leave, right?
  So, we shouldn't make references to them.

  I guess we would want to manually... implement... cloexec...
  Track whether each FD is cloexec...

  We'd want to have the FD table maintained for each FilesNamespace
  Christ almighty
  Why can't we just use the underlying data for this, again?

  I wonder if CRIU people do any of this

  So okay let's talk again about what we'd want to do.
  We want to detect when file descriptors are closed by cloexec.
  It's a nice automatic feature but our tracking needs to know about it making it useless.

  Maybe I should just not use cloexec?

  Why do I need to know when fds are closed again?

  Well, to know when the FileObjects are not shared.
  Oh, god...

  In a linear type system,
  I'd hand out a reference to a FileObject,
  and the subprocess would not return.
  Er hm. Even if it returns it's still leakedL forever I guess.

  Ok so I don't need much big stuff, I just need a FileObject with a shared flag, and unsetting cloexec can set the shared flag.
  And it'll be initialized as either shared or not.

  But the issue then is how I actually handle changing flags on the FileObject

  For example, what if, like... I want to leak do some aio on it, then leak it down?

  I guess I move it into the epoller, which sets nonblock, move it out again, which unsets nonblock, and leak it down.

  But what if I want to leak it while using it?
  well obviously that's bad.

  But what if I want to set a file object flag through one FD,
  while another FD is using the O_NONBLOCKness of the FileObject?

  ok, so there's also open file description locks, which also operate on the file description level.
  oh and there's also file offsets but who cares about those lol.

  it's kind of beginning to feel like maybe there should be some kind of syscall to duplicate an open file description.

  Ok so that would be stupid and couldn't work in general,
  because the struct file for sockets, for example, can't be duplicated.

  probably can't, anyway.
  nor for pipes.
  probably.

  okay, so is there *any* reason that I would *ever* want to share FileObjects between FDs?
** benefits
   Hey I don't think I need to pass in ownership of the FD to the Epoller anymore

   Because it's IMPOSSIBLE FUG

   oh wait i still need to do that actually

   uh, hm.
   let's think about it actually.

   so we still clearly need to take ownership, right? so that we can close it...

   but we can't "take ownership" of the FileObject, eh

   well

   okay, so technically we could not pass in ownership of the FD to the epoller

   just register the FileObject, *HEAVILY CONDITIONAL* on the FileObject being exclusively owned by us forever.
   otherwise it'll be leaked hardcore in that epollfd.

   then when we close the last fd, we'll be good.

   I guess in this kind of scenario we'd only really need one epollfd per...
   FileObjectNamespace or whatever...
   aka kernel...
   
   and we can unregister the FileObject using any other FD to the FO, I guess.

   jeese.

   okay...

   so file descriptors actually suck??? :(
   not really, they're still an excellent dependency injection thing
   but they aren't really that cap-secure since they are actually keys to an underlying mutable object.

   so maybe I should take file objects as primary?

   And have a way to manage, file object vs shared file object differences?

   I can even have, like...
   shared file object reference thing
   and real file object

   and have file descriptor numbers exist only under the hood

   aaaaaaaaaaaaaaaaa

   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa

   okay okay

   I guess this may be the right way to go

   let's, seriously, defer this, though
** file object design
   So really file descriptors are references to file objects

   I'm tempted towards a design which looks like:

   UniqueFD<FileObject>
   SharedFD<FileObject>

   With a one-way conversion allowed from unique to shared.

   Then on top of that you can have references to the uniqueFD.

   You convert to SharedFD whenever you want to pass an FD to someone else.
   It doesn't necessarily set CLOEXEC, because you might just be passing it over a socket, say.

   UniqueFD is for resources that we have exclusive ownership of.

   We can't implement FD proxying (it would be too high-overhead anyway),
   so the only option is SharedFD to share something.
   But I think we'd need SharedFD anyway.
   We'd want to represent whether the proxy is exclusive or shared before sending it out, of course.

   O... kay.

   And also, FileObjects are also just references,
   in some cases to things which can be opened a second time and mutably messed with more,
   but that's just application logic, whatever

   And, I guess the FileObject would be the one carrying all the capability flags.
   Actually it's interesting that this kind of makes capsicum support hard,
   if there's ever fd-level attenuation of caps.

   but I think this FileObject-primary approach is good

   And I guess we can make references to FDs and pass them around and such.

   And an FD also has the syscall interface, so it's the whole entirety of the reference.

   OK and also we'll be able to wrap the EpollOwnedFDs directly around the underlying FileObject.

   Should Exclusive and Shared inherit from the same FD class?

   yeah

   both for the interface, and for implementation sharing.
** sharing
   So technically stdin/stdout are exclusive for the lifetime of my process, aren't they?

   So I should be able to set them to a nonblocking mode.

   stderr is still shared though, innit

   I can have a pipeline and, each of the things,
   they all,
   write to it.

   Oh but stdin/stdout aren't exclusive, consider backgrounding.
   That takes away ownership.
   Maybe the shell should save and restore the flags on stdin/stdout when doing that.

   stderr tho

   Urgh, given this blocking problem,
   and given that an async read from the filesystem would require a separate thread,
   wait no it wouldn't, that's only if I want to parallelize it, which I don't really care about.

   Well, I was wondering if maybe I should support easy creation of additional threads/SyscallInterfaces.

   that would make rsyscall a hard dep, if we were using it in the core.

   hmm I guess there's no disadvantage in RWF_NOWAIT being both for pipes and things and files

   since we can't otherwise wait for files

   okay! whatever! I have to do the file object thing, practically!
   there are probably other use cases and stuff!
** wait okay
   Why am I seperating FD and FileObject again?
   I don't need to know when two FDs point to the same FileObject.
   Why don't I just have a single type that represents a file object, which contains the fd used to access it,
   and has a boolean flag about whether it's exclusive or not?

   Oh the issue is that I want the type to tell me when it's exclusive

   Well, I could have the FileObject take a type parameter which tells me whether it's exclusive or not.

   How would I do this in a proper typed system?


   Hmm.

   The file object really is just a marker to make sure you don't mess up.

   Free functions... seem fine for this?

   I just like how ergonomic methods are, since they're self-namespacing.

   hmmmmmm

   having them be methods seems fine really
** open file description
   OK, clearly passing down shared open file descriptions is bad.

   And terminals, terminals are bad too.
   So, yeah, it's fine.
** epoll wrapper approach
   I'll be led to a good design by trying to implement a multiplexer for epoll


   So let's assume that they only want to block once they have polled and failed for every reader.

   right?

   hmm there is a contention that the only right way to do it is,
   to read until eagain

   although I guess I can translate the edge into some status on my side.
   the edge is raised, we save it, we can then read until we get eagain, and then we lower the edge
   and if we want to read again after that then we have to block

   we really need to get the eagain information i guess

   also how does this relate to getting edge/level notification over the network?

   so anyway we return when the level goes high,
   and then we should call again when the level goes low.

   and i guess all we can get over the network is,
   "hey the level is high"
   "do a bunch of stuff"
   and when the level goes low maybe we need to say "ok the level's low, let's block on receiving a high-notification"

   I guess that's a much more stream-oriented approach.
   We could actually emulate that on top of pipes.
   Which makes it much better!

   I see, I see, so edge triggered is much better.
   It's like reading off notifications from a pipe.

   And the notification is, "hey this is readable now".
   And we get the "hey this isn't readable now" notification from EAGAIN.

   And, we could get multiple "hey this is readable now" events,
   as a result of efficient implementation that doesn't require storing data.
   Really we could get such events every time something becomes readable.
   And they'd get coalesced automatically I guess, for efficiency.

   Okay, okay, so this makes a lot of sense.

   Then the multiplexer is simple:
   It's just a proxy for pipes, to direct things to the correct destination.
   They register for events,
   and they get them sent to them.
   So that seems really quite simple.

   And there's no need for EPOLLONESHOT either

   So the real low level interface is this stream of readable events.
   And, we can wait for one to appear,
   but then it's not safe to keep waiting for another one to appear on that same channel,
   we need to go do IO until it's exhausted.

   It's kind of a weird dual-channel thing.
   The readability stream is like a control channel,
   and the data fd is like the data channel.
   Except the data channel also has notification of its own exhaustion, hm

   wait_readable is definitely not the right interface.

   What's a safe interface?

   Maybe some kind of, "local level"?
   We can wait for it to be high,
   but how do we make sure we mark it low?

   I guess it's not safe.

   The raw interface is certainly the edges.
   But how do we make that safe?
** general question
   I have a boolean variable.
   One source will tell me when it goes from low to high.
   Another source will tell me when it goes from high to low, among other things.

   The only actual uses for it involve messing with the latter source.
   There are two sources of things that change it?

   Obvious solution: Wrap all access to the sources

   But, the sources have a bunch of functionality that I don't want to wrap.

   Another obvious solution: Have the source know about the boolean variable and update it.

   Well... that's essentially the same as wrapping.

   I think probably we do want to just wrap.

   Oh, here's the real issue:
   We don't know how many of the sources there are.
   Many different things can possibly cause the variable to change.
   How do we ensure that the user appropriately detects the variable-changing event,
   and appropriately changes the variable?

   Well, the event throws an exception.
   Or, perhaps, can be linearly typed.

   I mean, when we get an EAGAIN...
   That's not really an error. It's a return value saying,
   "hey yo, I don't have anything more to give you right now, come back later, ya dig?"

   But still.

   If there's many sources and they all change the variable, well
   The obvious solution is still to wrap all the sources.
   Though, we don't necessarily have to wrap them to know about the variable.

   We can have the sources return a value that must be consumed by the variable.
   Though that requires a bunch of types.

   I think the best model for Python is to just have the EpollWrapper wrap every method.
   And update the readability status thing when it gets EAGAIN

   It works! Yay
** type-directed model
   We could have some operation on an fd,
   consume the fd,
   and either return the fd and some data,
   or an EAGAIN wrapping the fd, of some specific type.

   No we'd have to work with fd-operation pairs.
   Well, in any case, that's the basic model.
   Let's call the fd-operation pair just an "operation".

   So we have an operation.
   Operation -> Either (Operation,Data) (EAGAIN::Read Operation)

   We consume it and either get data and the operation back, or no data back and the operation wrapped in an EAGAIN.

   To unwrap the EAGAIN, we need to get a read posedge on the source.

   But, the read posedge has to come after the operation has been done.
   Tricky.

** no unique shared fd types
   It's too much overhead.
** TODO create epoll wrapper
   This has two advantages:
   can pass arbitrary data into epoll
   can pass weird epoll flags

   no wait the only weird epoll flags go in the event mask and I can already pass all those from python
** things which operate on file objects
*** Open file description locks
    urgh how do these work
*** File offsets
    but I can explicitly specify the offset.
*** O_NONBLOCK!!!! and other file description flags
    ugh EPOLLET really very much requires O_NONBLOCk to be set, otherwise you can't tell when to stop reading
    (though for streams (as the manpage says) you can detect partial reads)
** next up: supervise
   I could maybe do clever things to support multiple children in one supervise.
   But let's do the simple one child case first to see how it works first
** only on one thread thing
   I find myself frequently wanting to do a thing in only one thread calling into an object.

   Once the thing is done, all the callers can return.
   But, just, only one of them needs to actually do it.

   I find myself frequently wanting to do:
   "here is an async method. multiple tasks can call this async method at once. "

I find myself frequently wanting to perform some function in only one task calling a method,
and have the other tasks just wait until that function is done.

   It's kind of like picking a task to schedule some processing on,
   and then if other tasks call the method too, they just have to wait until the processing is done.
** resource issues
   many resource leakage issues are resolved by the fact that resources are local to a task
   and the task can be destructed all at once.
** fd leak on script interpreters with execveat(fd)
so thereotically we should just never cloexec it
always let it pass through
in which case,
the interpreter will get an argument of the form /dev/fd/N
well, the interpreter should really know to just use that as an fd!
and I could just send patches to bash and python to fix them...
if they get an argument like /dev/fd/N, they should just use that fd instead of reopening
especially because the permissions might be tricky on, say, memfds or whatever
though, both of them can support running scripts from stdin.
i guess the real issue is that the filename is provided as a path not stdin?
well, if it was provided on stdin then stdin would be used up, argh.
yeah so providing as an fd argument is best
I'll code around that for now,
and use it to test...
well no
I'll write a minimal C program that execs into something via open and execveat
also this works for sending the script once??? over a pipe??? not over stdin??

oh this doesn't work because normal executables will have to close the fd too
but then how does the dynamic linker work??

argh, the program is already loaded into memory by the time the dynamic linker runs
hmm so how can we handle this for ELF binaries?
for elf the fd is loaded into memory and isn't needed.

argh
should we just dispatch based on whether it's a #!?

blargh I guess we can't execveat for now
** process stuff
   pids are mostly terrible but only mostly

   as long as you only kill your children, you can work with them in a race-free manner

   I need to implement process management stuff (wait, sigchld, kill) in Python, not just rely on supervise.
   This way is less racy (can be certain I'm killing the thing I expect, while supervise can pid loop and I can kill the wrong child)
   And it reduces my dependence on weird stuff (supervise)
   and that weird stuff, I'm having trouble figuring out how to represent anyway.

   should I set subreaper? maaaybe? meh I'll figure that out later

   So, OK.

   I'll have some multiplexing on top of a sigchildfd,
   which then does a wait,
   and dispatches events,
   and sees what it sees.

   And kill on top of that.

   Essentially it's the same multiplexing as for epoll.

   so we'll have a Process object?
   which represents an existing standalone process?

   maybe it should be a task object instead?
   so we can accurately represent pseudo-threads and pseudo-processes and all that stuff?

   ChildTask, perhaps?
   Representing a standalone task that is a child of ours?

   Only direct children can be safely killed, since grandchildren can be collected.

   So ChildTask is only for direct children.

   What if we double fork?

   Well...

   Well, what if we double fork and then take ownership of the intermediate task?
   We want to support that.

   I guess we have some kind of ChildGroup thing
   Or like, some kind of TaskId thing which points ChildTask points to as its parent?

   And a task's TaskId can't really change..
   but our Task structure thing can.
   Well, actually we should really model it as a thread, not a task.
   Since the TaskId can change and thereby change what Linux task we're operating within.

   Thread makes more sense really...
   It's sfork that allows us to change our taskid.

   Fork too, kinda. But let us not speak of fork, that forbidden syscall.

   So sfork lets us change our taskid.
   But really it pushes our taskid on to a stack.
   We should make sure to explicitly model the fact that it changes our taskid.

   It would be unrealistic to pretend that it doesn't change our taskid.
   Since it's visibly different - children have a different parent.

   What happens to children when you change process namespace?

   We'll have a Thread, which contains a Task (which changes) and a SyscallInterface (which does not)

   We can consume a Task to produce a ChildTask, namely a thing thing.
   That's exec.

   A Task, I guess, has another blocked Task inside it.
   That's the Task that will be resumed when it sforks.

   I think it's best to have it look like that.
   That's realistic, though not necessarily exactly what the interface will look like when it gets into the kernel.

   So eh...

   A ChildTask contains a parent: Task

   And I guess also a Thread?
   And we can only kill it when thread.task == parent?

   soooooo
   okaaaay

   it's not really a collection of registers and an address space.

   the address space, you know, is actually an attribute of the task.
   and the registers don't even matter

   it's actually really truly just a wrapper around a syscallinterface
   and the task it corresponds to, can change.

   how would we even represent this in the kernel, sigh

   I guess we can't?
   what does it really mean to have continuity here?

   it's an illusion
   we copy our thread and we think we're the same

   but we're actually a copy

   are there any other means of having a syscallinterface other than "syscall" and "fd"?

   well we could do it over shared memory.

   and, doing it over shared memory...

   well, we'd have a task which we sfork inside, to create a new task,
   which is still listening to the same syscallinterface.

   so... we just want to track what task is currently active on the other side.

   could multiple tasks be on the other side?

   like, we send a syscall and,
   we can send another and,
   how do we determine which task gets which syscall?

   do we want the syscall interface to somehow give us some kind of token?

   which we can use to make a syscall on a specific task? ha ha.
   that's exactly the same as the current syscall interface.

   and anyway it doesn't work for a specific task,
   but rather a specific thread whatever thingy.

   cuz when we sfork inside a task
   it pushes the task on a stack and makes a new one.

   so this is kind of a "mobile thread".
   it's a thread that can move between tasks.
   and move between processes.

   who knows, maybe it could even move between hosts?
   well it can certainly move between containers

   hmm it sure would be nicer if sfork didn't create a new task

   exec after sfork I guess, as always, does three things:
   creates a new task with a new tid in the current namespaces,
   makes that tid have a new address space,
   returns us to the parent,
   blurgh

   wait a second, exec probably also creates a new file descriptor space, to cloexec in?
   blah

   well, okay, so.
   all this would be complicated.

   what if we just accept that sfork does create new tasks,
   it does create a stack of tasks,
   and exec is just the normal thing.

   then all that I'm doing is,
   having a single thread of execution move between tasks.
   the new task starts up with new registers,
   and the old task gets the new task's registers when it execs.

   so I guess this is pretty unnatural
   generally a task and a thread are identical

   so how would I do it otherwise?

   I guess

   I would just start up a task with a new SyscallInterface

   And in that way avoid the problems of "a thread that moves between tasks".

   Actually, for that matter,
   I would even be able to double-fork and start supervise?
   No no I wouldn't

   Wait, yes I would!
   Through the subreaper thing!
   Since it would inherit great-grand children.

   Huh.

   I could even do a CLONE_PARENT to have supervise directly become the parent of the great-grand-children.
   (haha, that would be so crazy)

   So okay, blah blah blah,
   I guess the best thing to do is to use clone to create new SyscallInterfaces.

   So, then, how do I...

   wait, argh.

   To use clone to do this, I would need to start using rsyscall immediately.
   Even for local, shared memory stuff.

   Although I guess I can directly write to the shared address space.

   Maybe I will indeed just do that, have a shared memory rsyscall.

   So okay, I will also need to set up the child stack right.
   
   I guess that I could probably have a C routine to do that, for now, with the one-process model.

   Oh, I guess I can have a C routine which returns a buffer or something?

   I can get the bytes that need to go on the stack,
   and then pick how I put them there.

   Yeah, yeah, sfork is weird, making tasks on the fly is better.

   Let's be sure we can do this right with remote rsyscall though.

   I would need to prepare the stack.
   But I guess I can prepare the stack just fine.
   It's just a little (a lot) weird.

   Though I wonder if I can leave child_stack NULL?
   Probably not, I do have to change the fds that I use.

   And no matter what, it would loop and then they would both read from the same fd.
   hmmmmmmmmmmmm

   OK I think building the stack is fine

   Doing this with rsyscall is just clearly better.

   So that's my next project.
   In-process rsyscall so that threads work.

   Yeah, just making more tasks is clearly better.

   Oh, also this means I no longer segfault if I don't use sfork :)

   Oh, let's just have a C function that starts an rsyscall thread in the current space.

   Er wait no that's not right, we need to control the clone args.

   hmm I guess we could use CHILD_CLEARTID and futexes as a way to get notified on child process exit :)

   oh wait no, exec will wipe that out

   okay I see, clone_child_cleartid and settid have to be there because
   uhhh, probably posix reasons
   oh, cleartid is how notification of child process exit happens.
   and I guess wait can't be used because, er, tricky posix reasons?
   whatevs

   oh also let's have the task have its own fd space, so we can get easy termination notification still
   er, probably.
   maybe.

   haha this use of CLONE_PARENT will be great
   well, maybe

   okay so it does seem pretty useful to have in-process rsyscall things I guess
   which I communicate with using file descriptors so they can be polled on

   oh, so!
   ptid, ctid, and newtls can all be ignored for our use case.

   we'll get notification of exit through fd hangup, I guess.
   possibly through __WCLONE if we must

   I don't think we need to or want to specify CLONE_THREAD
   although maybe we do

   all we need to specify is the flags, and the child_stack, and possibly the child signal

   we'll build the child_stack ourselves, hum hum...

   will we mmap it?
   instead of mallocing it?
   probably? that's the most common with a totally remote thing.

   we'll still do direct memory writing to it I guess

   oh, to kill a task, we'll, er
   close the infd,
   wait on the outfd,
   unmap the memory.

   okay so the role for C is just to build the stack

   what will the stack even look like?
   I guess we have some assembly routine which loads registers from the stack,
   then calls...

   oh wait, oh no

   clone continues from the point of the call??

   how do we fix this then...

   so I guess in a low level implementation of syscall,
   we'll call the system call,
   then ret.

   the ret will allow us to jump to an arbitrary location by manipulating the stack.
   okay, seems reasonable.

   so we'll write that component of rsyscall in assembly

   and skip the whole errno drag too

   hm.
   newtls, that's tricky.

   I'll avoid it. I'll make my syscalls directly instead, with no deps on glibc, so I'm totally freestanding.

   So, let's do all the syscall wrapping in Python.
   We'll have a class that provides a syscall interface and takes a function to do raw syscalls.
   Which uses cffi to do memory access.
   And we can wrap that class in either a syscall to thread or not.

   That's so type-unsafe it's not even funny lol.

   I guess I can write standalone Python functions that take do_syscall and are type safe.

   It's like assembly from Python.

   OK so we'll start off with just replacing io.py with cffi to async do_syscall.

   Then we can replace the async do_syscall with a thread-remote one.

   Replacing clone will have to come last.
   Since we'll need to be able to do all rsyscalls remotely first,
   and refactor the interface.
   Also can't do execveat too, because sfork clone needs it to work.

   But can do everything else
   Then add the new interface and test it
   (with a new syscall kinda thing maybe)

   Then replace the old interface

   OK so I converted things.
   Now let's add a clone2 which will do the stuff!

   hm
   should I just pass bytes for the stack

   probably not?

   okay so I need mmap now

   hmm apparently I can't do such fancy relocations in assembly.
   so I can't directly call rsyscall_server

   so probably I should instead call some kinda gadget that will call something of my choice.
   so, syscalls.
   stash all registers,
   pop all registers.
   hum hum.
   if we did that then I guess we'd be good.
   but like, most of the time our registers don't change
   so it's pointless

   we would only be doing that to support the fact that our stack can change
   
   okay, so we can have a generic gadget for turning stack args into register args,
   so we can call an arbitrary C function.

   this will be useful: it will allow us to call (in a separate thread) any C function in our address space.

   we can possibly use that to call dynamic linker functions to load libraries.

   it would be nice if status was a full int, but oh well.

   can we figure out a way to get a return value at low cost?

   maybe after we call, we move eax to someplace?
   maybe the base of the stack? :)

   that would be cute.

   so I guess clone is our "call arbitrary function remotely" entry point.
   just has to be combined with a gadget

   of course, we are relying on the "immediate ret after syscall" behavior.
   can we do better?
** child task monitoring thing
   I'll have some multiplexing on top of a sigchildfd,
   which then does a wait,
   and dispatches events,
   and sees what it sees.

   And kill on top of that.

   Essentially it's the same multiplexing as for epoll.

   This will be a Task object
   Which also implements ChildTask?

   Or maybe I just get back a ChildTask object and a Task object when launching a thread.

   Automatic cleanup is the tricky part.
   We don't get fd-based cleanup since we share an fd space.
   Could we run supervise in the middle to clean up our threads?

   That does seem possibly viable.

   Oh wait no!
   supervise won't help either because it won't get notified by the fd closing.
   Hm!

   I could resort to PDEATHSIG to notify supervise.
   That would work perhaps.

   The nice thing about thread groups is that they offer a guarantee.

   They are not nestable though

   There are too many task cleanup things in Linux, and all of them suck!

   I could also PDEATHSIG to kill the child threads.

   classic problem, classic problem

   I guess supervise + PDEATHSIG is fine.

   But I still don't like the fact that I don't have race-free-ness!

   I'm not killing my direct children...

   To have a cleanup task, my children must be theirs...

   Hmm, could I do an arrangement like...

   me -> supervise (with subreaper) -> childspawnerwaiter -> [all children]

   Then when I die, supervise is notified, and kills childspawner and all other children.

   Hmmmmmmm curious, curious.

   Alternatively, I guess I could wrap around me instead.
   Then I could spawn and wait on children directly.

   supervise -> me -> [all children]

   Like... treat me as something which can be sigkill'd?

   Essentially externalizing the cleanup?

   The fact that my children share my fd space makes this all tricky.

   I could just PDEATHSIG them too I suppose.
   
   Well... hmm. Then if someone sigkills supervise, things break.

   Oh wait, does SIGHAND just negate this whole problem?
   Does it mean that a signal to one, causes all to die?

   We should check that.

   OK yeah

   If I set PDEATHSIG for everything
   And have threads/tasks I control be direct children,
   and have uncontrolled things run under supervise to clean them up,
   everything will be good.

   This does suggest I can't exec in a direct child to something uncontrolled.
   Unless I can figure out a way to..

   Oh!
   I'll just fork off a child of my direct child
   and exec supervise in my direct child.

   So this seems... really good and clean!

   Direct children (such as supervise and rsyscall) can be trusted to terminate when I SIGTERM

   Indirect children (under supervise) get a harser approach, being SIGKILL'd by supervise.

   so it will look like

   me -> rsyscall, rsyscall, rsyscall, [supervise -> [nginx -> worker, worker, worker]]

   And PDEATHSIG will ensure cleanup.

   We don't really have to set PDEATHSIG on supervise since...
   well actually we do have to, because we could have:
   me -> rsyscall -> supervise -> rsyscall

   And fd sharing between 1,2 and 4.
   (presumably only temporarily, while execing in 4)

   Then we'd need supervise to be killed when 2 dies,
   because the fds wouldn't be closed until 4 dies.

   What would the kernel support look like then?

   ¯\_(ツ)_/¯

   It's not clear yet whether supervise will be doing child monitoring?

   I guess we can do:
**** me
***** rsyscall
***** rsyscall
***** supervise
****** rsyscall
       Here we will monitor child processes and stuff.

       It seems like it would be better for the thread to exit by raising a signal.
       That way it can be handled.

       And we could have a signal handler that does filicide and terminates.

       That would allow removing supervise from the picture
******* nginx
******** worker
******** worker
******** worker
*** continuation
    but wait, how does this interact with the remote use case?

    I guess it's pretty hard to load a library remotely

    Although actually I guess it could be pretty simple.

    So let's assume we can do a remote library load.

    Then we'd have our daemonized rsyscall process,
    which is controlled by its fds.

    When we shut down, we terminate the connection and its fds close.

    It reacts by raising a signal.

    That calls filicide if the signal handler is registered, but either way it terminates rsyscall.

    That results in PDEATHSIG terminating all immediate children.

    That all seems sensible.

    Oh wait, I can even load the library remotely through using clone-to-run-function hacks!
    What a cool hack that is.
    I just need to run it through ld.so or something, so that the dynamic linker appears in my address space.

    For now let's just put supervise in the middle.

    In this model, supervise does basically nothing at all:
    It just provides a process that calls filicide on exit.

    It's like a hack around the fact that I can't load libraries to get a function to register as a signal handler.

    So yeah, this all seems good.

    supervise then is really a simple utility.
    It does basically nothing.
    And is useful generically, and from the shell.
    well no not really, since it doesn't exec for you :)

    we're just using rsyscall or other magics to handle the child status reporting below it, if we care about that.

    seems great!
*** ways to clean up tasks
    thread groups

    process groups

    controlling ttys

    pid namespaces

    control groups

    PDEATHSIG

    I guess the ultimate way is to have a separate cleanup task.
    Which I don't SIGKILL.

    And I guess I'll use PDEATHSIG to notify it, shrug!

    We can also clear pdeathsig if we want to detach.

    So PDEATHSIG is a decent thing. I guess we'll keep it.

    filicide, then, is what we need. It needs to be in the kernel so it's robust to SIGKILL

    It's a thing which sigkills all transitive child processes when your process exits.

    I guess essentially it just SIGKILLs all children until there are no more children

    subreaper is required I guess.

    essentially we want a parent-level attribute that specifies the signal to send to all our children when we die.
    that way our children can't turn it off.

    and we want to keep sending it as we get more children reparented to us, I guess.

*** how did LinuxThreads do it??
    how did they manage to have SIGKILL to one thread, kill all threads?

    probably by pdeathsig or somefin
** implementing the child status monitoring thingy
   so I'll allocate a sigchld signalfd

   then attach it to an epoller

   then wrap it in a multiplexer

   to dispatch waits to the right associated ChildTask

   note well that subreaper means I'll get wakeups for unknown children.

   rsyscall[reap]

   so handling sigchld and manipulating child processes isn't so bad after all
   IF you're the only one in the process doing it

   actually no, just: if you don't have to stick to posix semantics

   hmm.
   aren't privilege level changes in Linux threading libraries inherently racy since one thread has to change before the others?

   Oh okay I guess first I'll do a signal multiplexer thing.
   I'll explicitly opt in to signals that I want to receive instead of terminating me?

   Then we take the SIGCHLD queue and

   We wrap it in Waiter

   Which has a "make" method which returns ChildTasks with a given tid.

   Or actually I guess a clone method?

   No, Waiter returns just ChildTasks.

   We have another thing that takes no arguments and returns a (Task, ChildTask) tuple

   Since it needs the pointer to rsyscall stuff to launch.

   We also want to have some kind of SupervisedTask which is required to fork off
   things which can
   exec random binaries

   And what about our trampoline which lets us run arbitrary C functions?
   Shouldn't we expose that?

   And that just has a ChildTask dep...

   Or rather it's a wrapper around a ChildTask.

   We don't want to share SIGHAND; not sharing SIGHAND means we can have specific tasks do filicide,
   without everything doing filicide.

   VM and FILES are both namespaces for dynamic resources,
   which we can safely share because the kernel multiplexes/allocates in a non-interfering way in those namespaces.
   (as long as we don't MAP_FIXED or dup2 to an unallocated address/number)
** rsyscall connection

   OK!

   So now I need to turn an RsyscallConnection into a syscall interface.

** runningtask

    Close this to murder the task.

    But wait what if the task execs?
    Then it is also fine to free these resources.

    But how will we know?
    HOW will we KNOW?

    So we can never exec in our base task, right, because then there will be no-one to
    filicide for us. Though, if we make a supervised task, maybe?

    So we have to figure out how to consume a task's resources after an exec.

    Also how does this relate to remote/out of process rsyscalls, ya dig?
    With those, one task may depend on another to be accessed! So we can never really exec them.
    So we do kind of have a differentiation between leaf task and lead task.
    Or rather, tasks which are depended on by other tasks,
    and tasks which aren't depended on by others.

    Right, because, only leaf tasks can exec - if a non-leaf task execs,
    we can no longer track its children!
** passing an epoll'd socket down to a subprocess
   Oh no!
   We need to be able to turn O_NONBLOCK off,
   but even if we're in a separate CLONE_FILES,
   that will still affect the same object.

   So we need to, in some sense...

   Oh, wait!
   Since we're passing it down to a subprocess,
   we can't ourselves continue to O_NONBLOCK on it.
   It needs to be consumed by the subprocess.

   So we'll just have some kind of release_from_epoll(),
   which returns the underlying,
   and unsets O_NONBLOCK,
   so it can be used by the subprocess.

   All fine.

   Connect and stuff can happen asynchronously without a problem.
** IntFlag!!!
   Woah!!! Use this thing for all the flags!!! So excite!!
** name
   it will be a family of related libraries that are similar in structure

   so, rsyscall is I guess an okay name
** binding sockets
   so I can do:
   chdir; bind(./name); chdir

   or I can do

   open(O_DIRECTORY); bind(/dev/fd/n/name)

   the former gives me a full 16 or so extra characters.

   but obviously I have to do the latter
** fifos
   interesting, fifos should always be opened for reading with O_NONBLOCK

   that way they won't block forever when opening for read
   which means deadlocks aren't possible

   maybe all opens should be done with O_NONBLOCK

   nah it has no effect for files
** path object, O_PATH fds
   There are a few common operations between Paths and O_PATH fds.

   namely: fchdir, fstat, fstatfs

   and we can probably use it with AT_EMPTY_PATH

   do we want to support having a Path which is backed only by an O_PATH?
   maybe, let's think about it future
** sockets
   So to know the address type for a socket,
   I need to know what kind of socket it is.

   There are a bunch of calls using the address type:
   bind
   connect
   sendto
   recvfrom
   getsockname
   getpeername
   accept

   Also, sockets are only readable once they're connected.
   so...

   Also, after a shutdown, they're no longer readable.
   Hm.

   Let's not try and track the readability status.

   But let's indeed track the domain in the type.
   Since it can't change

   And what about the type?
   er, that is, the sockettype.

   Maybe I should mix that also into the class type?

   Okay, so all three parameters can change the address type, actually.

   Maybe I should focus specifically on the address type for these SocketFiles.

   Maybe it can be, like,
   SocketFile[InternetAddress]
   SocketFile[UnixAddress]

   And so on.

   Hmm, we also need to think about how to handle fd passing

   I guess when we recvmsg,
   we get back a t.Tuple[t.Optional[T_addr], bytes, t.List[ControlMessage], MsgFlags]

   Then we can parse the ControlMessages to see what kinda do-hicky we got.

   But, this can all be represented low-level.
   t.Tuple[t.Optional[bytes], bytes, t.List[t.Tuple[int, int, bytes]], int]

   god the socket API is so crap
   i think, maybe

   okay, so the question is whether we have the address type above or below the FileDescriptor.

   I think below, it's nicer.

   But processing of recvmsg results is above the FileDescriptor.
   What class does that take place in?

   Well, I suppose each ControlMessage will be parsed.

   And will already contain whatever fds we pull off the socket
   But we need to specify a File type.

   I guess we can change the File it refers to, after the fact.
   Just have it refer to a plain base File at first.

   OK! Remember the central goal:
   Expose Linux features as they are!
   Do not become opinionated!
** unix sockets
   interesting, connect on a unix socket immediately returns without blocking,
   as long as it's to a listening socket

   it doesn't block until the server accepts
** child task scheme, final version for sure
   So I can clone and exec at any time without restrictions.

   And when I clone I'm relying on someone up the supervision tree,
   to have enabled filicide.

   And there's an easy helper to take a task and enable filicide on it.
   (either by signal handler, or execing, or starting up a thread waiting in sigwait then calling filicide)

   And when I clone I get a ChildTask back,
   which is usually wrapped in a larger [Something]Task,
   which holds all the resources given to that ChildTask,
   so that they can be freed when the child is killed.

   And for rsyscall tasks,
   that object is RsyscallTask,
   and it has a reference to the Task,
   and it has an exec wrapper method,
   which execs in the Task and frees the resources it used and gives me the ChildTask.

   But I don't have to go through the RsyscallTask to exec,
   I can just exec directly if I so choose,
   including from the root task on any system,
   even though that may break things.
** exec
*** DONE child tasks stay around after exec
   Hmm.
   Also, when I exec, how do I make my child threads go away?
   If I exec up in the tree, my children won't get signal'd, because I am not exiting.

   Actually I think that is fine.

   I have a neat visual of the Starcraft terran HQ, lifting off, setting up a big hierarchy of threads,
   then anchoring again and transforming into a high-efficiency C program/factory/assault fort.

   For it to be efficient, though, we'd need to free lingering memory and close fds referenced by no task.
   or something. but that's for later.
*** TODO detect when exec has completed
   Though, hmm. how do I detect when an exec has completed?
   And won't now return with an error?
   Traditional way would be to use files being closed.

   This is one place where sfork was nice.
   When I exec'd, it returned to the parent on success.

   How would I ideally do this?

   Well I could use the sfork trick I guess
   Hm! This is a puzzle.

   So after exec, the fd table is unshared.
   So maybe we could start off by unsharing the fd table?
   Then we close the rsyscall-side fds,
   and then we exec.
   Then we'll get either a hangup or a result.

   Maybe I can just close the fromfd,
   so that if it tries to respond,
   it gets a sigpipe,
   and exits?

   Yes, let's just convert an error into an exit.

   Wait, wait, wait, again I can't do that, if I want to exec into rsyscall
   Because, if I want to share the fd in the threaded rsyscall with the process rsyscall,
   I have to keep them open.

   Well if I'm doing that then I'm not freeing all the resources owned by the task.

   I'm pretty much only freeing the stack space.
   So then the question is: How can I know when the stack space is unused?

   Hmm, I guess this is an issue that is solved by running in a separate address space.
   Then the stack space is definitely freed up when the task exits.

   But if I want to use clone...
   And thereby have a thread in my address space...
   I have to manually free that.

   So what's the trick?
   How do I know when to free the thing?
   Well, after an exit or an exec.

   OK, and also keep in mind that the task might not be our child.
   It's gotta be task-only.

   Though... in that case we don't have anything to free.

   Oh dang, also we can't do it by exiting on error,
   because then we don't know when to free stuff on success.

   We need some kind of event after the exec but before exit(0)

   If we constructed a process from scratch,
   then kicked it off,
   then it did something to no longer need the resources we gave it,
   shouldn't it free those resources itself?

   So we're in a situation where it's just borrowing resources
   And we need to know that it no longer needs them.

   Well how the heck do we do that

   I guess there's an easy way to be sure: if the address space changes
   For fds we can just see when there's no more object references

   Ehhhh hmmmm
   I think separate fd tables and address spaces are good because they keep things nicely reference counted
   Everything in a space is needed by that space,
   when a space goes away all those things have their refcounts decremented,
   and maybe go away.

   Yes, yes, that is all true!
   But what about when it's not a separate address space!
   Oh. Maybe then we can't call exec?
   But, argh, then how do we even get new tasks in the first place?

   So okay, clearly a separate process (address space, fd table) is very nice and it keeps things nicely refcounted.

   But we need to also support the case where we aren't in a separate fd or vm namepsace.
   In that case we have to know when things leave ourselves.

   So how do we figure out when the reference to the stack is dropped?
   Indeed even other more rich functionality could theoretically switch off its stack.

   Well, those things would have rich signaling functions to say when they drop refs.
   (or just be explicitly controlled by the runtime)

   But if we're talking just system calls, how do we do it?

   Well how would we manage this prop'ly?

   The stack would be linear and passed to the thread
   When it goes to exit or exec,
   we'd free the stack purely in registers.

   Okay, so that is all fine and good.
   What stops us from doing that with exec?

   Well with exec,
   the thread stops at the point of exec,
   it doesn't get the chance to continue.

   What is this in terms of continuations?

   OK so it looks like I can use CLONE_CHILD_CLEARTID

   I want to get notified on mm_release,
   which notifies currently on cleartid and vfork.

   cleartid is gross though because it does a futex wakeup instead of an FD wakeup.

   a CLOEXEC pipe is nicer than CLEARTID maybe

   Oh, but we have to unshare before exec if we do that.

   urgh

   hmm so that seems like it might be okay
   we'll force the thread to exit if its exec fails?

   how does that interact with main tasks and such?

   well, if a main task execs,

   well, I can't figure out a task to monitor it from in that case!
   a parent task who can share a pipe with it, argh

   the same problem applies for any exec-monitoring thing

   the problem of responding to the notification and freeing the resources after the exec

   that requirese some kind of monitoring

   which I guess only the wrapper can do.

   some kind of on-the-fly vfork?

   like, instead of setting vfork,
   I call block_until_child_exec,
   urgh which is essentially the same as vfork
   
   You could emulate vfork with clear_child_tid, I think.
   Do a clone,
   in the parent do a futex wait on an address,
   in a child do clear_child_tid,
   then proceed to exec.

   so we'd want something that we can bake into our event loop

   I guess if there was a waitid event on exec,
   then...

   we could also emulate vfork that way?
   neat

   so OK,
   two methods suggest themselves to me.
   1. use CHILD_CLEARTID
   2. use ptrace

   ptrace would obviously be disgusting but, is it really that bad?
   it's certainly better than futexing trash...
   and I only have like,
   a tiny bit of ptrace.

   But, of course, that is a portability problem because ptrace can be turned off (through the YAMA LSM)
   But, eh, whatever.

   ok so futexes seem utterly crap, I don't see a way to wait on multiple things

   could I like, mmap an fd or something, and specify that

   ughhhghhh

   so i'll just do traceme,
   set PTRACE_O_TRACEEXEC,
   forward any non ptrace signals,
   and untrace once exec is done successfully

   look on the bright side, supporting ptrace is cool!

   oh hey and uhhh
   could I speed this up with that seccomp yield to userspace  things?
   nah, there's no way to exec in the original process with that
   it tells me before the syscall instead of the syscall

   ptrace is not an acceptable solution
   because ptrace isn't recursive

   CHILD_CLEARTID or vfork it is.
   vfork is probably just plain unsuitable.

   so we need to figure out futex integration.

   I guess that essentially means one thread per futex.
   Doing.. what?
   ugh, maybe just bridging a futex to an eventfd?
   that would be cool and generic I guess...

   OK let's do it.
   Not that specifically, but a thread dedicated to waiting on the futex.
** fd namespace   
   It should contain a list of tasks in that namespace.

   And when an FD's task goes away,
   it picks a new one from the FDNamespace list.
** file descriptor table inheritance issues
   fork is really problematic
** shared vs unshared file descriptor tables
   The benefit of unshared is that we can detect things via file descriptor hangup, which is nice.
   Also it's just a lot more nice isolation, I guess.

   What's the benefit of shared?
   Well...
   We can very easily pass file descriptors between tasks?
   But we could do that with unix sockets anyway, just slightly harder.
   We kind of don't have to deal with inheritance being tricky,
   since any fd in a space is valid in new tasks.

   But we kind of want to explicitly move fds between tasks anyway,
   so...

   On the other hand, do we really want to rely on unshared fd tables?
   Because, isn't that somewhat expensive?
   Maybe not that expensive...
   It's mainly shared memory that we want to preserve.

   So we'll have the shared memory threads,
   but each with their own fd table.

   That seems fairly good I guess.

   I doubt we'll have issues around moving between tasks being too expensive.
   And if we do, we can resurrect the trickiness of shared fd tables.

   The issue with shared memory is the same as with shared fds, though.
   When one dies, its resources aren't freed.

   So I guess we could just go all the way to separate processes.
   But spawning them requires spawning threads, anyway, so...

   well, not if we use sfork... blargh

   okay, separate fd tables it is
   that is clean, I think.
   and also, that's kind of natural for "process is a virtual machine" notion.
   actually no it's not, why would each CPU have its own fd table,
   ridiculous

   maybe because of NUMA???? NUMA fds???? NUFA????

   maybe we should exec so we trigger cloexec so only explicitly passed fds go to our internaltasks

   if we start out shared, then move to unshared,
   how do we handle this inheritance question?
   we have to deal with that either way.
   how do we handle inheritance?
   it would be nice to have a CLOUNSHARE maybe
*** the file descriptor inheritance question
    I still need to think some about how to properly handle the semantics of fd table inheritance.
    that's why I currently have all my child tasks using CLONE_FILES so they share fd tables
    because that way there's no inheritance going on

    fd table inheritance means that any libraries not controlled by me will have their fds duplicated and kept open,
    which might break those libraries

    so this is why I can't have things in their own fd table, but in the same VM space as me.
    the only way to trigger clearing of the fd table is through exec.

    if there was a clounshare...

    wait wait

    a thread that I unshare in,
    if it's running rsyscall,
    can't use any fds I don't know about.
    since I won't send those commands

    but if it's running arbitrary code

    oh wait no

    it's the other libraries, the ones left behind
    the threads that live and breathe with fds unused
    if I fork a thread off, and then unshare, and I have the fds, and the original guys close the fd,
    then that's a weird situation, one which cloexec is supposed to prevent.

    I'd really want to have CLOEXEC also affect unshare.
    Then I don't have any of these issues, right?

    well... I can implement this myself with scanning the fd table.
** argh
   okay so I have two options

   option 1: implement CLOUNSHARE so I can deal with unsharing the fd table,
   and then have each task in an address space have a different fd table
   (this doesn't provide a way to exec into a new process and keep using the same rsyscall fds, right?)

   option 2: use futexes and child wait and all that stuff.
   and pdeathsig.
   this is gross but I can have everything in the same fd table so it's fine
** argh argh option 1
   OK, so let's be realistic, right?
   Isn't this required for starting subprocesses?

   We unshare(CLONE_FILES), assign some fds to some other fds (fds which we explicitly know the number of),
   then exec, and it's all good.

   There's no way to do the stdin/stdout assignment without unsharing CLONE_FILES.
   At most we could first exec a separate rsyscall process,
   and then start running from there.

   Although.

   Doing either of those irreversible things, really seems like maybe it something I shouldn't be doing.

   Feels like maybe I should have unshare(CLONE_FILES) from the start.

   Then I start my child, not passing CLONE_FILES,
   and it inherits all my fds.

   (I can have some other mode for running a thread pool for blocking syscalls over shared fds)

   Man, doesn't this suggest that we probably shouldn't even have CLONE_VM either?
   What if I keep alive some memory that a library was usin'?

   No no, that's exactly opposite. We're all in the same address space, but that's fine.
   Each one manages his own stuff.

   And we never need to unshare(CLONE_VM) (and it's impossible anyway, lol)
   Because there's no refcounting hangup stuff that we want to trigger.
   And things are never placed at known offsets.
   And all that stuff.

   Yeah, there's no way to inherit a CLONE_VM.
   You're either in the same address space or a completely new one.
   Er...
   Well, that's not true, you can inherit CLONE_VM as you move to a new thread - fork for example.

   Well, we won't be inheriting CLONE_VM, anyway.
   We'll be doing it all in a single place, right?
   A single address space

   And we manage memory explicitly within there.
   Why do this?

   After all, if some thread in that address space dies,
   its memory resources will not be collected.
   Only its fd resources.

   Why don't we just forcibly always move to a new process, and exec the thing?

   Because it's far cheaper to write to memory in a single process.

   Well, we could provide a shared memory section.

   And isn't this also an argument in favor of always being in the same fd table?
   After all,
   it's far cheaper to pass an fd to another task when you're in the same fd table;
   you don't have to send it over a Unix socket.

   But the thing differentiating the two is the new-process-start.
   That is, what happens when we exec.

   When we exec, the address space is destroyed and recreated, not inherited - fine.
   But the fd table is inherited.

   This means we can avoid dealing with address space inheritance.
   We could just have everything in its own fresh address space,
   all fine and good.

   But we can't avoid dealing with fd table inheritance?
   Is that right?

   It would be nice if we could get a fresh address space and put things in it.
   Inheriting addresses would be weird.
   And a gross abuse of virtual memory.

   Since ideally everyone would be in the same address space...
   And exec would merely set up a new program area...

   Which, in fact, we could do in userspace...

   We could implement processes in userspace -
   but only if we had each with their own fd table and sighand and stuff?
   (and assuming that malloc et al was smart enough to not collide other things)

   I mean, if they syscall directly,
   they really do each need their own fd table and things.

   But that is fine.

   Running them in a single address space, somewhat neat, though somewhat useless.

   But! The issue is!
   CLOEXEC does not trigger!

   When we start a new thread running a program we've loaded,
   its fd table is unshared,
   but not CLOEXEC'd.

   Furthermore, if we first spin up a thread to do preparation of the fd table,
   it'll be crazy


   OK!
   classes of fd:

   explicitly passed down: not cloexec or clounshare
   private to a thread: CLOEXEC, CLOUNSHARE
   dynamically passed down: not cloexec or clounshare

   But, what about when we fork?
   Do we really want all the library-private things to go away due to clounshare?

   so when we fork off a *thread*, or when we exec a new *program*, we can't use any of the libraries we had before
   even though the thread could call those functions, because they're in its address space

   but if we fork our process, the libraries are in our address space,
   and we're in the middle of a call stack,
   and we probably are going to want to use those libraries.

   okay, triggering cloexec

   urgh I want to be able to iterate over the cloexec fds from userspace :(

   okay, okay, we'll hack it, a lot of fds to iterate over, but whatever

   so this kind of leaves it as,

   I can pass in some fds into a new thread,
   and some will be dynscoped in.

   If I want to have multiple people operate on the same fd,
   I can just pass it into all of them.

   But the fd *table* isn't shared-mutable,
   the mutations each make aren't visible to others.

   Yes, it seems good

   okay...

   I wonder if I can support both methods?

   Both "perfect shared memory eternal glory",
   and "sublime justice private fd table".

   So I will clearly want to do CLONE_DO_CLOEXEC, right?
   Always?

   Okay, so what I have is,
   two different ways to handle detection of the memory space no longer being used.

   OK, so.
   The private fd space way is cleaner, isn't it?

   That's incremental.

   Now, a true thread would support sharing the table, wouldn't it?

   Well, no.
   It's cleaner for each thread to have its own table.
   FDs are exclusively owned by one thread, after all!

   And that allows for neat process stuff.

   But, sharing the fd table...

   Hey wait a second
   The futex task also exits when it's left the fd table.

   But, it's not clear what the task has taken with it.
   So does it really allow us to close those things?

   We can clearly close the fds in our address space
   The question is whether we can close the communication fds.
   And, again, yes, we clearly can, unless we are reusing it.

   Really, each thread having its own private fd table seems better.
   But it is limiting.

   Language-level access control over who owns an fd is more flexible/better/more glorious.

   We'd need to, um...
   On thread exit, track down all the fds it owns, and close them?
   Along with memory resources too?

   Well uhhhhhhhhhhhhh

   A thread doesn't really own an fd nor does it own memory.
   We have all that managed centrally.

   I mean, why would I use my own thread system, rly
   Well, to integrate it with a nice beautiful epoll event loop.

   OK, so let's suppose I start a thread that shares my fd table.

   I need to be notified when it drops the stack.

   Then I go forward and unshare the fd table.

   I need to CLONE_DO_CLOEXEC (after unsharing) so that private fds, and the fds of other threads, are not duplicated.

   Though.
   Yes, that makes sense.
   It's not possible to unset CLOEXEC while multithreaded-in-fd-table,
   since it may be inherited.

   Everything (except for dynscoped things) is CLOEXEC.

   Then I unshare and inherit everything,
   then I perform some manipulations and unset CLOEXEC on some things,
   and then I call exec.

   So I don't need to CLONE_DO_CLOEXEC, unless I'm not going to exec.
   Unless... I regard this as racy.
   Having the duplicated fd... is there a race there?
   Might I need to do this immediately, atomically?

   Maybe what I need, then, is some form of atomic,
   unshare, unset cloexec on things, do_cloexec

   I mean, alternatively...

   I just don't inherit anything?
   Sounds gross, that's not acceptable.

   So then, maybe something like:
   int unshare_fd_table(int *inherited_fds, size_t count);

   Which unshares the fd table, closing everything in it that is marked cloexec and which is not in inherited_fds.

   Well, what if I implement that by a socketpair?
   Oh, ha ha, I can't do that, because I can't inherit the socketpair! Gross.

   Okay, so I don't think there's a race here.
   Just imagine that I had unshare_fd_table, it's instant, sure?

   But it would be equally instant for me to do the unshare and unset cloexec and call do_cloexec.

   Er.

   That is, one possible interleaving is,
   if there's some thread A that wants to close fds which another thread is waiting for,
   A doesn't run for a while.

   But wait, there's certain things that aren't possible.
   Like.

   Thread A could close fds 4,5,6,
   then signal using a futex that they closed them.
   Then other coordinating process would believe with absolute certainty that it should get hups on them.
   Immediately! Right?
   (if they're local pipes, sure)

   Well, no, some other thread could also have just forked, as things do.
   Since it could have always happened before by fork, I think we don't need to care.

   But suppose we did care!
   It would sure be tough to deal with!

   How would we ever go from a shared fd table,
   to a private one,
   inheriting some things from it,
   while closing the things we don't want,
   without racing against others?
   ...locks?

   a lock on... close?

   no, yuck, screw that

   so if a library somewhere deep below,
   performs a fork,
   what should they do to ensure they aren't keeping other fds erroneously open?

   well, how do we pass fds to the fork?

   I guess this is a good reason to have something other than "dynamic scope".
   or, I guess, globals across multiple tasks

   you really want something absolutely task-local

   well.

   is keeping something memory mapped an example of keeping something erroneously open?
   the address space may be a problem in itself if that's the case...
** what to do
   Once it reaches a certain level of maturity, I should use this inside TS to get more validation/testing.

   i.e. immediately upon finishing the basic stuff

   My goal: configuring/running nginx.
* criu
  I feel like I have a bunch in common with criu

  They are manipulating and preparing tasks as objects,
  and I'm doing the same thing
* unrelated thought
  dependency injection is autoresolving dependencies by type???
  that's... that's... that's implicit parameters :(((((
  looking up things by type :((((
  but I hate dependency injection!!!

  urgh, I've just had a terrible thought
  a typeclass can be modeled as an additional argument that is automatically looked in a table indexed by type, right?

  well it's just occurred to me that there's another practice that is like that,
  where an implementation of some functionality is looked up in a table indexed by type,
  common in a certain primitive language.

  and it's a practice that has reviled and disgusted me,
  so it's terrible to think that they could be the same

  how can you achieve coherence without global uniqueness of instances?
  just error if there's ambiguity or whatever
* multiplexing without depending on event-loop specifics
is single-prompt yield not powerful enough to do multiplexing without control over the prompt?

how about multi-prompt yield?

well, if I had a multiprompt yield.
then I could have a prompt for my specific multiplexer
and I'd want the user to be able to just call into me without themselves having to be within some handler

so they do it
but
oh even with a wrapping a function around the prompt to get the continuation, I only get the cont under the prompt, derp

a message passing perspective

yield to specific prompt (that's the cap they call on)
then it's good
so multi-prompt yield with first class prompts would be fine, assuming we could keep the handler on the stack.

maybe I could implement multi-prompt yield in python

it wouldn't be slow:
we'd have a separate stack, essentially, for each yield thing. which is fast.

and it would be in direct style.
when someone yields back up to us,
we'd see that as a response to a send into them.

and we'd be able to continue on and mess with them.

but, I feel like that must be too powerful for the simple use case of wanting to multiplex access to a single resource
(aka: taking requests in, submitting them to the resource, and sending the responses back out, possible in a different order)
I feel like there must be a simpler way to think about multiplexing access to a single resource than that..

well that sounds like message passing
which is equally powerful

so really that's just exactly what it is...
is it *exactly* what it is?

in a sense, yes.

is it REALLY what it is?

what other ways to multiplex are there

well exokernels have embedded code
i could poll at user level
i could have it wake up everything
maybe there's some relevance of passing down an fd...

if I have it wake up everything then I could handle it in user code.
but how would I wake up multiple things???
they'd all wait for one thing
and when it happens they return

ho hum hee

if wait_readable is my only primitive, can I do it?

I guess this is related to the whole question of,
what is the syscaller interface? how wide is it? etc

hmm, I just realized that I think this is essentially the same question as another question that I have previously deferred until later
(that other question being, "do I really need to expose wait_readable in my interface-to-native-system-calls? maybe there's something more naturally ~Linuxy~ I could expose")

hmm, actually it just occurred to me that this question is essentially the same as an earlier question I was pondering and deferred until later, so I guess I'll just defer this one too, and be trio-specific for now
(the earlier question was whether to expose wait_readable in the system call interface which abstracts over making system calls in the current thread vs. in another thread, or to expose some other primitive async mechanism, or do something else, or whatever)
** should we call PDEATHSIG in the trampoline, or with rsyscall?
   In favor of trampoline is that it will apply for all threads

   Against trampoline is that it will not reaaally work for everything

   If the parent dies, but there are still some threads alive in the address space,
   when we die, our stack won't be cleaned up.

   Well wait, that's an issue with rsyscall as well.

   No, no, we solve that by just making sure PDEATHSIG is set for all of our immediate children, right?
   Is that the plan?
   Yes, I think so.
   We don't want anything in our address space to survive our death, right...?

   Well, we could. But that would require careful resource management.
   So I think PDEATHSIG does work.

   Then we're back to the question of trampoline or rsyscall.

   What if we wanted to trampoline into something that should survive our death and be in our address space?
   Again, this would require careful resource management.

   Do we even want anything to be able to survive our death? I argue no!
   Or, more specifically, by default that shouldn't happen.

   Also, in the future we'll have an inheritable immutable PDEATHSIG.

   Although, we still want to daemonize things, don't we?
   How will we deal with that?

   I guess we'll have a PersistentChildTask,
   which is really a supervise (possibly watching multiple children)

   We'll have to unset PDEATHSIG for the supervise,
   as well as for the children.

   But, it will be in a separate address space anyway.

   Hmm hmm hmm.

   So yeah, how will we design PersistentChildTask?
   That is something tricky.
** PersistentChildTask
   Oh boy, this is a tricky one.

   Yep.
   Tricky.

   Okay time to start thinking about it.

   Could we just have a persistent rsyscall task which is not our child?

   That's perfect, yes, let's do that.

   That matches the form of remote tasks too, we can just reconnect to them.

   It's SLIGHTLY awkward that if we kill it, it murders everything on that host.

   But I guess we could have multiple of them on a host.

   It could be some kind of rsyscall server, where when you connect to this unix socket,
   it spins up a thread for you?
   But it persists forever while the unix socket lasts?

   hmm

   but we do want a specific thread to actually persist,
   don't we?

   We kind of need that, to preserve the child-parent relationship.

   We could become aware of SUBREAPER...

   Though that doesn't help, does it, we'd just reparent to some parent person,
   and want to preserve them,
   because if they go away, then we're ruined.

   We could use supervise to make the child-parent relationship irrelevant.

   Hmm.
   If an rsyscall task gets wedged by me sending a bad syscall,
   aren't we screwed?

   We can't recover.

   Wait wait wait, of course we can recover.
   EINTR is a thing... wait, will we not get EINTRs?

   Can we send it a signal that we've specially designated for interrupting bad syscalls?
   Yes, and we can get EINTRs.

   That seems distasteful.

   But it works.

   Okay, but, so, I'll either do persistent rsyscall tasks, or use supervise and a Unix-socket-serving thread-server.

   But in either case, don't I need to be able to start things that don't die?

   Also if I use supervise...
   Can it be a thread inside the thread-server, which I communicate with using an fd?

   And I'd probably do the same with the persistent rsyscall tasks, right?
   They'd just be threads in the server and I'd access them through a thread-server.

   Hmm.

   A thread-server, which implements the thread creation logic I've done in Python, in C.
   That seems fairly interesting as a concept.
   PROCSESES AND THREADS R FILES, DEAL W/ IT HATERS

   Okay but wait, it would be specialized to only work with rsyscall server threads.
   That's undesirable.

   Could we..
   could we just...
   send it a function address and some arguments...
   and get back a whole lotta nothing...

   Hmm yeah, like, how do we send it to the fds or memory or all that stuff that we might want to send it.
   I guess those are questions for our runtime to solve, not our kernel-interaction-thingy.

   Only dealing with rsyscall servers seems neat.

   Though! How do I get the data?

   Also, again, I guess I'm just considering something like,
   I have some threads, I get the fds to talk to them with...
   I send those fds *into the server*,
   so it persists.
   Is that how I'd make a persistent thread?
   But how would I do that?
   How would I send the fds in?
   Since I only have one side, and they have only the other side,
   how would I get the fds from one side to another.

   Also, again, the data fd.

   Oh wait, I guess the threads I create wouldn't be persistent.
   In fact I could even connect to the thread server from inside the server.
   Then it'd be okay.

   So then, data fd. How do I data fd.
   Hmm...
   So I can allocate memory without memory, using mmap.

   Then I can write into that memory one byte at a time with syscalls, though that's mega-gross.

   Or I can do the multiplexing trick, but that requires care.

   So I guess, uh

   Oh wait, screw connect, let's just use a protocol.
   We'll say, "hey gimme some fds BAE",
   and it'll send them over.

   Although that's tricky when done remotely.

   We could have an initial process that we get,
   which we then use to connect to the persistent process server.

   And we get that thread,
   and we send the fds back to our initial process over unix sockets.
   Seems good.

   Wait, wait, no, no.

   Then we have to go through two address spaces. Not three, which is nice, but still two.

   So we'd really like a direct connection to the persistent process server.

   Wait, can't we just do that?
   We pick ourselves up, send our fds over, and throw ourselves over, by our bootstraps.

   So we get our initial process. It comes with a free data fd, compliments of the house.
   It has access to the unix socket.
   We open that to get a thread.
   We send the fds for our initial process over the Unix socket.
   Including the datafd.
   We dup those fds over the one for our thread inside the thread-server.
   Now we have direct access to the thread-server.
   And we only have one data fd.

   So that seems fine and good, and at least that's possible.
   We'll probably think of something better later.
** so how do we actually run one of these
   We probably have an initial initial process, right?
   Which just runs vanilla rsyscall.
   And we bootstrap our way into a persistent thread-server from there.

   And how do we do that?
   Well...
   I guess we can exec our way in, assuming our initial process isn't forcibly terminated.

   But is that sustainable?
   What if we want to create another one?

   We won't have another task that is unparented/unfilicided.

   So, what do we do?

   Oh wait, I think it's fine, isn't it?
   Don't we just double-fork?
   Standard bidness?
   That's gross though, I don't want to double-fork.

   Hm, so.
   This is sounding like I'm in favor of doing it in rsyscall, as a first-class thing.

   And then I can choose to not do it, and instead exec something that will persist.

   Of course, the real issue is nesting persistence...
   What if one of the applications I start, wants to persist?

   But I don't want to persist?

   I guess I have to give them the ability to persist.
   I have to pass it down.

   How do I pass down the ability to persist?

   Well. I can allow a task to live forever by default,
   and remove that ability with a persistent pdeathsig thing.

   Meh, it's some kind of silent persistence, but I guess it's fine.

   Maybe I should have the inheritable PDEATHSIG thing partnered with a clonefd?
   And the inheritable PDEATHSIG doesn't kill things made with clonefd.
   That way you can make pseudo-persistent processes which live past a tree's death.

   How would that work?

   I guess maybe I can like,
   get an fd for some process,
   which unsets pdeathsig thingy for it??

   Designing this API is hard :(

   OK, so yeah, doing it in rsyscall in code is better.

   Alternatively, we could just do it by default.
   *Should* it be by default?

   How do we control this?
   How do we control persisting tasks?

   Well, the ideal way is by linear resources.
   When we go away, the resources we own (our children) go away as well.
   
   But, we can opt in to survive the death of our parent.
   
   But, if we want to guarantee our children die because they might maliciously do that.

   Hm, hm.

   So what if it was literally a capability that's passed down?

   Oh, what if it was some kind of process group fd?
   And when they're closed, everything inside is killed...
   But you can attach your process to them, and survive!

   Like normal process groups, they aren't recursive.
   You can only be in one.

   And so, if someone passes down a "survival" process group,
   you can enter it.

   Actually maybe it should be recursive,
   then you can make more fine grained divisions.
   You can spawn a pgroupfd off of another one,
   and it will be the child of that one.
   And, if the parent is closed, it will be forcibly closed?

   Well, maybe that's silly

   Okay, why would we want to be recursive?
   Why can't we just add ourselves to the survival group?

   We could, we could, we certainly could.

   Can we also solve the "library wants to run subprocess" problem this way?
   Well, we can have the library set no signal at clone time.
   Then what?

   Could we also also solve the "detect reparenting" problem?
   Say, because you stay in the process group forever?
   And maybe you can get notifications when anything in it dies, no matter the depth?
   No, that's a mistake

   But maybe there's a "top level" of the process group,
   and when your parent at the top level dies,
   you go to the top level?

   And you can get notifications for things at the top level dying
   Nah that's complex

   Oh wait this doesn't work
   Self-reference, as usual, foils it.

   So that would imply we do need recursion.

   That's interesting.
   If things have owners, self-reference is a problem.
   But if owernship is a stack, it's no problem.
   That's like linear regions etc

   Anyway.

   I think the default would be death in any of these schemes.
   So I think it makes sense to default PDEATHSIG on.
   Turning it off is a rare operation anyway.

   Yeah and let's do it in rsyscall main, eh?
   umm

   oh wait, heh, futex waker isn't gonna wake up and die when rsyscall dies...
   no wait yes it will...
   let's put it in rsyscall main for now.

   naw, in that case, let's do it at startup.
   no wait, what if we die before that?
   we have to set pdeathsig as soon as possible to avoid this race of death...

   no wait, it's fine, there's no race, we can conditionally do it

   argh gotta fix up rsyscall trampoline or something

   blah, let's just put it in rsyscall and whatever
* cool feature: context manager container
  I can start a container with a contextmanager and spin it off by returning.

  hey, and supervise is a nice pid1 to run inside the container

  neat neat neat neaaat
* things to improve
** TODO fix the race with PDEATHSIG so that a thread dies even if we die before it sets that
   I guess we can... know the pid of our parent (have it passed down),
   set pdeathsig,
   getppid,
   if ppid isn't what we expect, suicide.

* portability things
** assuming there are 64 signals so sigset_t is an 8-byte integer
   I think you can probably compile the kernel with more signals?
   But maybe not because it looks like it's hardcoded in a header somewhere.
** rsyscall assembly code
** building a stack manually to pass to rsyscall trampoline
** architecture-specific syscalls
   clone at least is arch-specific in its argument order
** using /proc for bind and connect
   Because there's no bindat and connectat.

   This is an awkward blemish
* desirable kernel features
** notification of exec (more specifically: mm_release) through wait
   This way we can detect when a child task/thread has called exec,
   and free any resources they had.
   Otherwise there's no way to know for sure that they're done execing.

   This is like CLONE_CHILD_CLEARTID,
   but that only has a futex interface,
   which is not suitable for an event loop.

   Current workaround:
   ptrace.

   This is especially necessary for threads running arbitrary code.
   We can't just ptrace such threads all the time.
** a CLONE_DO_CLOEXEC flag to pass to clone, to close CLOEXEC fds
   This would be good and useful.

   Though we can do it in userspace, it's ugly and requires creating an extra thread for full correctness.

   And it would be simple.
   Just need to call do_close_on_exec.

   oh god, clone really is out of flags
** sigkill-resilient filicide
   Maybe, like, the combination of supervise and PDEATHSIG?

   Except, we'd want like,
   CHILDDEATHSIG...
   a signal to send to your children when you die.

   If we want to combine this with a better proc interface,
   we could maybe have some kind of childgroupfd,
   which when closed, kills all the children in that group.

   We can't realistically enforce things the same way capsicum does,
   which is to ban usage of non-forkfd process creation.
   Also I don't think that's robust to loops anyway.

   Oh, the same is true for the childgroupfd then.
   Well, no, they can create childgroupfd within them, but,
   they're still contained in the childgroupfd.

   Oh! Let's just have an inheritable PDEATHSIG which can't be unset!
   That's easier conceptually.
   Though, reparenting will still happen before the actual signalling, which is awkward.

   Maybe I should have the inheritable PDEATHSIG thing partnered with a clonefd?
   And the inheritable PDEATHSIG doesn't kill things made with clonefd.
   That way you can make pseudo-persistent processes which live past a tree's death.

   There are several things we want:
   - it should work to kill legacy processes that don't use this API
   - there should be a means for processes to still daemonize if we want to allow that

   A childgroupfd does actually meet these.
   If we have a childgroupfd,
   all the children in it can be killed when we close it,
   but not if they are inside their own childgroupfd.

   Like process groups, but as a file descriptor?
   Hmm, that seems like a nice idea.

   So you can escape the process group easily, is the thing?
   Maybe we have a flag, "escapable" or not.

   Of course, escaping will just require that something has a reference to its own process group fd.
   oh, capsicum doesn't support passing process fds
** remove old confusing comment from dequeue_signal
   in signal.c

   It just wasn't removed in b8fceee17a310f189188599a8fa5e9beaff57eb0 when it should have been
** bindat, connectat
   Would be nice to be able to pass a dirfd for these. And also lift the length limit.
** some way to deal with fd inheritance?
   CLOUNSHARE?
* design decisions
** TODO should we grab the Task out of other objects, or pass it explicitly?
   Consider RsyscallTask.make as an example.
   It takes a ChildTaskMonitor and an Epoller as arguments,
   both of which contain a Task.
   Should it also explicitly take a Task,
   or just use one of theirs?

   Also we can grab the Epoller out of the ChildTaskMonitor, for that matter.
* knowledge
** tkill vs kill
The difference between kill and tkill (besides the special negative parameters to kill, which tkill
doesn't support) is that kill always delivers the signal into the shared SIGHAND table, whereas tkill
delivers the signal into the task-private (thread-private) table.  Note that pthread_kill, raise, and
other such things work like tkill (though they actually use tgkill).
* game
  non player processes

  file descriptors are inventory

  namespaces are rooms

  avoid metaphor: namespaces aren't rooms, they're just what exists in this world
  essentially we'd build a rich shell for exploring Linux things
  (the python interpreter)
  but also have a text advanture like simple translation layer
  which translates simple commands into obvious 

  sockets are telephones?
  you can pick one up and it makes a copy I guess

  I can use this to demonstrate the essential weakness of the concept of,
  just putting an internet socket out there where anyone can dial it if they have the right number,
  by having a call verb that allows you to call people

  and have some kind of task where you want to provide a service,
  and want to avoid some prank callers from calling you.
* what is this? explanation to normies
it has the intent of providing an interface to Linux tasks as first-class internal entities,
that can be manipulated by running syscalls inside of them
** amoeba
it's kind of going back to python's roots and being a distributed scripting language :)
** make low level functionality generally available
   there are things that are easy at a low level, but become hard with layers upon layers of abstraction.

   let's make an abstraction that is much lower level and much thinner than usual,
   to undo those hardnesses that are created by abstractions.

   maybe abstraction is a bad word for this.
** name
   rsc is russ cox's username :(
