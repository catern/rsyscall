* stuff
so do we really actually just one a single thread,
per connection,
which just blocks on calls,
can only support one call at a time,
easy, simple to implement.

doable in C even.
with a C interface

so okay
suppose we had this.
then the question is thrown into sharp relief:
how do we multiplex over multiple processes/hosts???

duh
we have a file descriptor interface

which is readable
when the thread
is runnable/has returned a value

also, I *could* pipeline them:
I could send multiple at once,
and get them back one by one.
(In the same order? Yes)
But let's not focus on that, that is not that important and also maybe bad.

this is nice and simple

so this is kinda translating a completion interface into a readiness interface


anyway so I have some file descriptor,
which allows me to run any system call over it,
and it's readable when it's done
(vn one at a time ofc?)

okay is it just as simple as
I have a C library

with void read_call(stuff)
and stuff read_response(void)

then I don't have to expose any serialization stuff.
i can just use structs internally

actually more like

error read_call(int procfd, int fd, size_t count);
int read_response(int procfd, char* buf, size_t bufsize);

oh and conveniently I can just completely avoid defining my own headers and enums and structs and such,
since i'll just pass things through blindly

ugh although pointers are difficult, scatter-gather IO is tricky (though actually probably I should just not support that)

look for documented linux syscall interface
think I heard about that for fuzzing

writing this by hand really doesn't seem that bad

consider the simplest scenario which is where we don't care about being nonblocking across host
we just have the interface be,
exactly the syscall interface but with an extra sargument for the procfd
the sysfd

also there are multiple ways to achieve the goal of asynchronicitiy

we could have ujst epoll_wait be asynchronous and monitorabel by readiness for completion
we cuold have a generic way to await on readability of any fd on the remote side
bridging them, that is

what about cancellation?

we might call an epoll_wait,
and then readiness tells us completion,
but what if we want to make another call in the meantime on this system?

we need to be able to do that
so having syscalls be cancellable seems not very generic

also calling epoll_wait is not right, we should call that in "userspace",
and only see readiness in the main interface

okay, if I wasn't asynchronous.
i'd just,

the calls would all look like normal calls,
with an extra argument.
and we'd just have some internal serialization,
very normal.

maybe to monitor readability I need another connection?
that way I don't have cancellability?

but what if I want to monitor readability of a bunch of different fds? i'll allocate and never free connections.

so the ability to make a call then monitor for readability to see when it's done, seems good and easy.
just splitting up epoll wait into two calls achieves it automatically.
but, it's not cancellable!

what would this be in a protocol level?
if i had a stream dedicated for readability notifications...

having all calls block, means I nicely have to explicitly allocate new threads,
through my previously-thought-of mechanism of clone passing an fd. nice.

it's like an interrupt?
I get an interrupt for readability?

oh and what if I want to get readability notifications while in the middle of another syscall?
well, impossible.

resume me when this guy is readable
seems reasonable.
extensible to,
send me a message when this guy is readable,
which is,
be readable when this guy is readable.

readability really is easiest for the kernel to implement.
the kernel just translates interrupts into wakeups of blocked threads.
easy peasy
all we need is some way to indicate what interrupts we're interested in receiving

when we enter an epoll_wait on some set of fds
we say we want interrupts for all of them.

to do the same for my syscall fd,
we need to explicitly request interrupts before waiting.
then un-request interrupts after we're done waiting?

but in practice the way we get an interrupt is by getting completion notification for a syscall.
so isn't it best to have that be the central conceit?

maybe, maybe.

what other ways could we be notified?

(really the notification/interrupt is the resumption of the thread on the other side.)

still wonder what this is at the protocol level

okay but, if we get a completion notification for a syscall,
we need to be able to cancel that syscall.

i mean, maybe we just have two threads.
one for readiness and the other for actual syscalls 

oh, i really can't have an all-blocking interface, because, the network hop is so expensive. it will be weird...

so
i could have a separate connection/thread for readability notification
and just, i open/close those threads as I want

ugh having a separate thread is inconvenient

maybe i can just have a separate mode

okay so I guess readiness wakeup/readiness notification is just,
a bridge between the interrupts of the remote side and the local side.
you'd want the same with futexes or whatever

also you should return the thing to await on
(or maybe not)

so the ideal is then:
an asynchronous syscall interface,
taking an opaque thing and returning an opaque thing to wait on,
(maybe)
augmented by a cancellable readiness notification registration thing.
* notes
C library interface with a bespoke protocol underneath
two issues: defining custom logic for each syscall, and defining a custom serialization/protocol for the stuff.


It would be nice to instead have a protocol interface
no, no, there's no need for that.
I can just poll on the fd and do the operations.

no.
a protocol is better because then the user can handle IO,
*and* handle retrying on partial reads, and all that stuff.

so all I want to get in my library, is a stream of data in,
which eventually leads to a fully parsed response.

i could just use capnproto serialization.

audit and seccomp-bpf aren't suitable because they don't actually look at the pointers.


nice rqusetest b 

* HAHAHA
vfork!
implementing the cancellable readiness thing with select!
write memory, read memory!

RETURNING FROM EXEC/EXIT LMAO YES BEAUTIFUL

wonder if the registers change


now what I need to do is, call syscalls *really* directly so glibc doesn't get in the way with this errno stuff.


exec makes a new stack for you that the process will run on, so you don't have to make your own stack.
but clone for threads, you have to manually creattoeac


to ask on irc:
why SHOULDN'T i vfork all the time?
and, how CAN i make syscalls directly without errno, without assembly


also can I get rid of the read/write by just,
reading and writing from the connection file descriptor?

MSG_WAITALL does the write part, as an argument to recv...
and send just alwys blocks, for the read part!

then i'm down to a single function call in a loop

recv MSG_WAITALL on stdin, cast to syscall struct, make syscall, write response.

does splice write all the stuff? without blocking?

maybe I don't even need to go through local memory?

and I guess I'll just use the exact interface of seccomp trap to userspace?

you could implement filesystems with seccomp trap to userspace.
the BPF filter could even make it efficient, kinda!

conceptually, it would be nice if things were just modeled as,
we have a pipe full of systemcalls from the user,
and we have a BPF program attached to that which filters it,
and we process the ones it sends us...
and sometimes it just kicks them back to the user program to be run there.
but I guess that's close enough to what we have.

 
* syscaller musings
Hmm.
I'm tempted to have the syscaller be a separate structure,
and have separate notions of a pure ProcessContext with associated FileDescriptors
(which don't by themselves let me do operations in that process)
and some means of accessing a ProcessContext, which any individual IO structure will use.

No, that is absurd. We wouldn't be able to actually own and close the fds!
To properly do this we need ownership. Which means a given FD must be tied to a ProcessConnection.
So an FD holds a reference to a... Syscaller? or a LowLevelIOInterface? Hmm...
to close we only actually need the IOInterface.
but to do further operations we need a Syscaller.
so it seems like the basic thing should just have the IOInterface,
while more stuff has the syscaller.
Maybe we should just state that every connection has an associated page of memory.
Therefore it is already fully-functioning.
But isn't there more stateful stuff?
Like pipes, say!
To do a splice, I need a pipe!
And pipelining demands still more sophistication.
Although I can only pipeline through one process connection at a time, since the pipelining will cause problems otherwise..
I guess pipelining is controlled at the ProcessContext level.
Remember what I originally wanted, when I was planning on using serialization: A simple way to do remote syscalls
Now I have to resort to pipelining for good performance, I guess.
Or do I? Let's just stick to something simple:
Each fd is associated with a SyscallConnection or something,
which contains all the resources needed to make any syscall.
Likewise with each piece of memory? But then what of the syscall buffer and the remote sides of the syscall and data pipes?
We'll just require the syscall buffer to exist, and the remote sides of the syscall and data pipes are internal details.
Wait, I think that circular reference is fine.
It will require use of with to preform orderly destruction, but that's OK
OK, so all memory and fds can have a reference to this Connection thing, including the internal ones.

Maybe if my abstract interface is just, perform any syscall?
Should I abstract over memory?
Obviously yes.
Should I abstract over fds, and only provide owned ones?
can't really do that in general.
also, what if I wanted to use memory in the remote place?
I guess I should only abstract over *memory involved in the system call*.
but er no sometimes I might want to pass a specific pointer to read into *remotely*. for the side effect.
well maybe read() can take an optional buffer argument that is either a local buffer or a remote buffer.
guess that could be good.
nah forget it I don't think I'd ever actually want to deal in memory in the near term.
so let's just abstract over memory completely.
and that'll be my interface!
a buncha methods returning ints!
with memory completely handled.
and then i'll build my common fd abstractions on top of that.

What about concurrency?
What about nonblockingness?
Above or below?

well ideally when we call this thing it would only block the current task.
and other tasks would be left free.
but, note that the object is still "globally" blocked.
we can only have one person calling one of these syscalls at a time.
so there's all the same deadlock issues within a single "process"
so maybe it's therefore not so bad that a native Syscaller would hard-block when we call it?
it's a protection against anyone else interfering with its process, y'know...
could we make it truly async? not without some effort.
to make it truly async would require using a native IO library.
ultimately we'll have to make a blocking call in our thread.
I think it is fine that they are marked async. It demonstrates that they may block.
And shows the silliness of an explicit marking for async? though it's a nice effect system...

man. a nice serializable thing which is automatically nonblocking would be exactly what I want,
since it would be a cross-language IO library.
bah! oh well! we have no cross-language type system, so we can't
computer science is not yet advanced to perform such feats.
also it's dubious because we can't really re-expose exactly the syscall interface.

so, anyway, handling concurrency above the interface is the right thing, I think.
even though it leaves this weirdness of tasks marked async, hard-blocking.
but ultimately we need to block, so...

ah, and how do we bridge this into the native trio thing?
the remote syscalls work fine, I can just do a select and it's good.
good-ish...
I'd need a nice queue underneath the syscall interface...
to link up call/response...
since multiple people need to call at once actually...

note: try to perform IO first, and only select if it returns EAGAIN

so anyway, briding into the native trio thing works fine with remote syscalls,
since the object is truly async,
and doesn't block other tasks.

but the native calls! aie!
I guess I'd have some thing where I, uh.
I forward epoll_wait to trio so that it really does not block other tasks?
then I rely on the EAGAINing of everything else?
I want to interrupt the blocker guy if I need to block on something.
I really like the "make a syscall to wake it up" thing, it should Just Work.

actually okay, maybe I do need to just expose a wait_readable call.
because I can't do the select hack in userspace...
and it returns either "readable" or "EAGAIN" though I guess the fd could be closed under me, eh.
no wait it can't because of blocking, eh.

so okay sure I'll expose the wait_readable call, whatever.
since I can't expose the file descriptor number of the remote thingy fd in userspace.
but wait, I need to do that to avoid it when dup2ing...

well, even with local syscall I need to cleanly handle dup2
ok, I will avoid dup2 problems by making the remote syscall and data fds be high numbers.
local dup2s don't have any serious problems, if I overwrite something, oh well.

hey neat, when I vfork the exceptions in the subprocess propagate back to the main process.

oh anyway so when I call wait_readable with remote calls,
I can also make a syscall at the same time (like with native),
and it will interrupt the wait_readable (not like with native)
but I can make it like that for native. just cancel the wait under the hood?

when I make another syscall, it interrupts wait_readable automatically.
then the async layer will not wait_readable again until it's finished performing other IO.

I guess that's a nice invariant.

okay, we'll just have wait_readable only return when the underlying FD really is readable.
And the conceptual reason to call wait_readable: It doesn't take the lock on the object.
Other syscalls can go through while we're calling it.
That will be nice and uniform with native.
And we need to have such a wait_readable call as a primitive to avoid deadlocks.

That is, a deadlock where we call some blocking syscall, and it blocks, but
then we realize we need to call some other syscall to make it
unblock.

Right, the reason we need the primitive isn't because we want to get asyncness in there,
it's because we don't want to take the lock on the object.
which is conceptually the same as not globally blocking

* naming vfork thing
So the real system call wouldn't be clone, it would be... unshare?

Well, vfork is the best way to view it I think. It's just a vfork that doesn't return twice.

Meh, I'll call it sfork.

* 
if syscalls were RPCs

you could set up the new process/do the exec stuff in a remote process

it would be nice to have an exec that creates a thread in the same way as clone

but really clone doesn't need to take a stack argument, does it?
why can't you just conditionally return?

yeah and I mean, why can't I just say, start thread running with these registers

exec and clone are kind of the same thing


anyway if I could RPC to another process space, I could set up its memory with mmap just fine.
then kick it off and let it run.

adding RPC to another process space...
targeted, surgical interventions that massively increase power

anyway if syscalls were RPCs,
and you could just start with an empty address space and map a bunch of things inside it,
then start a thread inside it,
then that would be real cool y'know

unshare
* accessing the environment and args over rsyscall
okay so args and env are at the base of the stack. hm.
so we could access them that way.
that's a bit silly though, let's just get it externally
* handling entering the child process
  I guess it seems fine.
  Maybe I'll wrap the SyscallInterface in something more structured and nice.
* what does the FD hold
  Currently it holds a SyscallInterface.

  And since we mutate the SyscallInterface when we change process...

  There's nothing to update/convert in the child process.

  but wait so, this also means that if I open things in the child process,
  then when the SI reverts back,
  I won't be able to see that they are bad.

  I think I should not be mutating the SyscallInterface, but rather creating a new wrapper thing.

  Yeah, and then I should mutate the ProcessContext to indicate that its SyscallInterface has stopped working.

  Technically the processcontext should support multiple syscallinterfaces.
  argh. so it's desirable that, uh...

  OK, so maybe I should have a single TaskContext,
  which references the Process it runs within,
  and has a single fixed syscallinterface.

  why can't FDs just have both a syscaller and a process?

  well what happens when the syscaller switches what it's referencing.
  or rather what process it operates within.

  I guess one concept is that, um.
  our task moves to another process.
  so er.

  but argh, how do we then prevent other things using that syscaller while in the child process?
  when it will be wrong?

  okay so maybe I have this notion Task,
  which holds the syscall immutably,
  and mutates what context it's in.

  and then each FD holds a reference to both a Task and a FileDescriptorTable.
  and before doing anything,
  checks that the Task and FDTable match.

  That makes sense.
  Task is, then, essentially the same thing as SI,
  just a slightly friendlier presentation.
  well, a nice wrapper anyway.
* epoll/asynchronicity
  Now I need to decide how to handle this.

  Maybe have another wrapper for fds?

  I guess the epoll guy has to own the fd since it has to remove it

  Don't forget to read/write first and only on EAGAIN start blocking.

  Should the epoller be tied to the task?

  I don't see why.

  Well, of course an epoller holds a task, of course,
  because it needs to be able to syscall.

  FDs can be switched between tasks easily though. Or should be able to anyway.

  Maybe an epoller should inherit from fd?
  Can you actually read from an epollfd?

  Oh, I certainly should have an epoll file descriptor in any case.

  And then I have a thing which owns that...

  Maybe release() should return a copy of the FD object instead of an int?
  It's just a way to mutate the object into giving up the fd?

  OK so the question of whether I have an epoll fd that is just known on the other side,
  or whether I have a wait_readable call.
  
  Hm.
  I guess I can just epoll_wait anyway.
  As long as no-one else is using the thread/epollfd...
  Then can't I just make blocking calls?
  If there's no other tasks waiting on things other than in my world?
  Which is probably the case?

  Of course, the epoll_wait needs to be interruptable.
  So that other tasks can cause a call.
  For example, some task has an FD become readable, I read it,
  and then I want to write that data to another task.

  Tricky, tricky.

  Except it's not really the case that I'm the only thing in this world.
  Both native and through rsyscall.
  Others are waiting and might cause me to want to do something in a task.

  Tasks, tasks, tasks

  Single language runtime, multiple tasks,
  freely scheduling on any task,
  tasks tasks tasks

  Tricky, tricky.

  tasks.

  Essentially we're providing a way to work with multiple Linux tasks,
  as objects inside a single-threaded language runtime.

  But when we call a function on one of them, it locks the object.

  We want to be able to interrupt those functions.

  Ideally this would be implemented by,
  when we perform the call to an epoll_wait,
  there's an additional thing that can wake us up:

  More people calling into the object.

  Ho hum he...

  I guess this is an issue of multiplexing?

  We could write a nice API where we run each system call on a different task.
  The ole, allocate thread pool and run syscalls on them, approach.

  But instead we want to multiplex a bunch of system calls on a single task.
  And in some sense, we want to get notified when they return successfully,
  but we still want to be able to send new system calls...

  wait_readable does seem fairly good.

  OKay so I'll stick to wait_readable being abstracted by the interface,
  I'll code using trio-only libraries,
  and, yeah!
* file descriptor .release()
  release() allows some kind of linear movement of types through the system,
  so we ensure they get closed.

  man a linear typed language would be such an improvement for resource tracking.
  blargh
* important design notes
  dup2 takes a file descriptor object and never ever takes a number
  subproc has a method to convert objects from the parent process to the child process
  cloexec is on by default, and unset manually when creating a new process

  wait_readable only returns when the underlying FD really is readable - it doesn't take the lock on the object
* concerns
  okay so...
  what the heck

  if I pass an fd into a subprocess, does it then get the ability to change my own nonblock status of that file description?

  fml

  how to test, hm...

  guess I can just test fcntl

  okay so the internet suggests it really does work that way
  even for stdin/out/err

  fml fml fml

  is there a nice clean Linux-specific solution?

  so I just realized that since struct file is shared between file descriptors,
  O_NONBLOCK can't be really set on file descriptors that might point to shared (non-owned) struct files...

  argh okay so let's think about how we could model this if we have to.

  also terrifying is the prospect that it's not just shared across dup'd fds,
  but also across separate opens of a fifo, as that one guy said. but that seems dubious

  so we need some way to represent whether it has
  extra references that are outside my set of fds

  and also when my set of fds passes outside my control??
  no I guess I don't need that, that will just be a stray files/task thing

  okay so
* file object management
  we'll have FDs point to a FileObject

  And we'll have FOs have a list of FDs?

  And we'll use this to implement something like Rust's cell system?
  A runtime borrow checker?

  If we have an exclusive FD to something, we can perform FO mutations.
  Otherwise we can't.

  And we'll also have a flag on FO, "leaked".

  Or wait...
  I guess when we pass something to a subprocess, a file descriptor will be left behind that is open.

  Oh god
  When we create a subprocess we create a bunch of new references to a FO.
  So we add them when we translate, right?

  But, ones that are marked cloexec are closed once we leave, right?
  So, we shouldn't make references to them.

  I guess we would want to manually... implement... cloexec...
  Track whether each FD is cloexec...

  We'd want to have the FD table maintained for each FilesNamespace
  Christ almighty
  Why can't we just use the underlying data for this, again?

  I wonder if CRIU people do any of this

  So okay let's talk again about what we'd want to do.
  We want to detect when file descriptors are closed by cloexec.
  It's a nice automatic feature but our tracking needs to know about it making it useless.

  Maybe I should just not use cloexec?

  Why do I need to know when fds are closed again?

  Well, to know when the FileObjects are not shared.
  Oh, god...

  In a linear type system,
  I'd hand out a reference to a FileObject,
  and the subprocess would not return.
  Er hm. Even if it returns it's still leakedL forever I guess.

  Ok so I don't need much big stuff, I just need a FileObject with a shared flag, and unsetting cloexec can set the shared flag.
  And it'll be initialized as either shared or not.

  But the issue then is how I actually handle changing flags on the FileObject

  For example, what if, like... I want to leak do some aio on it, then leak it down?

  I guess I move it into the epoller, which sets nonblock, move it out again, which unsets nonblock, and leak it down.

  But what if I want to leak it while using it?
  well obviously that's bad.

  But what if I want to set a file object flag through one FD,
  while another FD is using the O_NONBLOCKness of the FileObject?

  ok, so there's also open file description locks, which also operate on the file description level.
  oh and there's also file offsets but who cares about those lol.

  it's kind of beginning to feel like maybe there should be some kind of syscall to duplicate an open file description.

  Ok so that would be stupid and couldn't work in general,
  because the struct file for sockets, for example, can't be duplicated.

  probably can't, anyway.
  nor for pipes.
  probably.

  okay, so is there *any* reason that I would *ever* want to share FileObjects between FDs?
** benefits
   Hey I don't think I need to pass in ownership of the FD to the Epoller anymore

   Because it's IMPOSSIBLE FUG

   oh wait i still need to do that actually

   uh, hm.
   let's think about it actually.

   so we still clearly need to take ownership, right? so that we can close it...

   but we can't "take ownership" of the FileObject, eh

   well

   okay, so technically we could not pass in ownership of the FD to the epoller

   just register the FileObject, *HEAVILY CONDITIONAL* on the FileObject being exclusively owned by us forever.
   otherwise it'll be leaked hardcore in that epollfd.

   then when we close the last fd, we'll be good.

   I guess in this kind of scenario we'd only really need one epollfd per...
   FileObjectNamespace or whatever...
   aka kernel...
   
   and we can unregister the FileObject using any other FD to the FO, I guess.

   jeese.

   okay...

   so file descriptors actually suck??? :(
   not really, they're still an excellent dependency injection thing
   but they aren't really that cap-secure since they are actually keys to an underlying mutable object.

   so maybe I should take file objects as primary?

   And have a way to manage, file object vs shared file object differences?

   I can even have, like...
   shared file object reference thing
   and real file object

   and have file descriptor numbers exist only under the hood

   aaaaaaaaaaaaaaaaa

   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa

   okay okay

   I guess this may be the right way to go

   let's, seriously, defer this, though
** file object design
   So really file descriptors are references to file objects

   I'm tempted towards a design which looks like:

   UniqueFD<FileObject>
   SharedFD<FileObject>

   With a one-way conversion allowed from unique to shared.

   Then on top of that you can have references to the uniqueFD.

   You convert to SharedFD whenever you want to pass an FD to someone else.
   It doesn't necessarily set CLOEXEC, because you might just be passing it over a socket, say.

   UniqueFD is for resources that we have exclusive ownership of.

   We can't implement FD proxying (it would be too high-overhead anyway),
   so the only option is SharedFD to share something.
   But I think we'd need SharedFD anyway.
   We'd want to represent whether the proxy is exclusive or shared before sending it out, of course.

   O... kay.

   And also, FileObjects are also just references,
   in some cases to things which can be opened a second time and mutably messed with more,
   but that's just application logic, whatever

   And, I guess the FileObject would be the one carrying all the capability flags.
   Actually it's interesting that this kind of makes capsicum support hard,
   if there's ever fd-level attenuation of caps.

   but I think this FileObject-primary approach is good

   And I guess we can make references to FDs and pass them around and such.

   And an FD also has the syscall interface, so it's the whole entirety of the reference.

   OK and also we'll be able to wrap the EpollOwnedFDs directly around the underlying FileObject.

   Should Exclusive and Shared inherit from the same FD class?

   yeah

   both for the interface, and for implementation sharing.
** sharing
   So technically stdin/stdout are exclusive for the lifetime of my process, aren't they?

   So I should be able to set them to a nonblocking mode.

   stderr is still shared though, innit

   I can have a pipeline and, each of the things,
   they all,
   write to it.

   Oh but stdin/stdout aren't exclusive, consider backgrounding.
   That takes away ownership.
   Maybe the shell should save and restore the flags on stdin/stdout when doing that.

   stderr tho

   Urgh, given this blocking problem,
   and given that an async read from the filesystem would require a separate thread,
   wait no it wouldn't, that's only if I want to parallelize it, which I don't really care about.

   Well, I was wondering if maybe I should support easy creation of additional threads/SyscallInterfaces.

   that would make rsyscall a hard dep, if we were using it in the core.

   hmm I guess there's no disadvantage in RWF_NOWAIT being both for pipes and things and files

   since we can't otherwise wait for files

   okay! whatever! I have to do the file object thing, practically!
   there are probably other use cases and stuff!
** wait okay
   Why am I seperating FD and FileObject again?
   I don't need to know when two FDs point to the same FileObject.
   Why don't I just have a single type that represents a file object, which contains the fd used to access it,
   and has a boolean flag about whether it's exclusive or not?

   Oh the issue is that I want the type to tell me when it's exclusive

   Well, I could have the FileObject take a type parameter which tells me whether it's exclusive or not.

   How would I do this in a proper typed system?


   Hmm.

   The file object really is just a marker to make sure you don't mess up.

   Free functions... seem fine for this?

   I just like how ergonomic methods are, since they're self-namespacing.

   hmmmmmm

   having them be methods seems fine really
** open file description
   OK, clearly passing down shared open file descriptions is bad.

   And terminals, terminals are bad too.
   So, yeah, it's fine.
** epoll wrapper approach
   I'll be led to a good design by trying to implement a multiplexer for epoll


   So let's assume that they only want to block once they have polled and failed for every reader.

   right?

   hmm there is a contention that the only right way to do it is,
   to read until eagain

   although I guess I can translate the edge into some status on my side.
   the edge is raised, we save it, we can then read until we get eagain, and then we lower the edge
   and if we want to read again after that then we have to block

   we really need to get the eagain information i guess

   also how does this relate to getting edge/level notification over the network?

   so anyway we return when the level goes high,
   and then we should call again when the level goes low.

   and i guess all we can get over the network is,
   "hey the level is high"
   "do a bunch of stuff"
   and when the level goes low maybe we need to say "ok the level's low, let's block on receiving a high-notification"

   I guess that's a much more stream-oriented approach.
   We could actually emulate that on top of pipes.
   Which makes it much better!

   I see, I see, so edge triggered is much better.
   It's like reading off notifications from a pipe.

   And the notification is, "hey this is readable now".
   And we get the "hey this isn't readable now" notification from EAGAIN.

   And, we could get multiple "hey this is readable now" events,
   as a result of efficient implementation that doesn't require storing data.
   Really we could get such events every time something becomes readable.
   And they'd get coalesced automatically I guess, for efficiency.

   Okay, okay, so this makes a lot of sense.

   Then the multiplexer is simple:
   It's just a proxy for pipes, to direct things to the correct destination.
   They register for events,
   and they get them sent to them.
   So that seems really quite simple.

   And there's no need for EPOLLONESHOT either

   So the real low level interface is this stream of readable events.
   And, we can wait for one to appear,
   but then it's not safe to keep waiting for another one to appear on that same channel,
   we need to go do IO until it's exhausted.

   It's kind of a weird dual-channel thing.
   The readability stream is like a control channel,
   and the data fd is like the data channel.
   Except the data channel also has notification of its own exhaustion, hm

   wait_readable is definitely not the right interface.

   What's a safe interface?

   Maybe some kind of, "local level"?
   We can wait for it to be high,
   but how do we make sure we mark it low?

   I guess it's not safe.

   The raw interface is certainly the edges.
   But how do we make that safe?
** general question
   I have a boolean variable.
   One source will tell me when it goes from low to high.
   Another source will tell me when it goes from high to low, among other things.

   The only actual uses for it involve messing with the latter source.
   There are two sources of things that change it?

   Obvious solution: Wrap all access to the sources

   But, the sources have a bunch of functionality that I don't want to wrap.

   Another obvious solution: Have the source know about the boolean variable and update it.

   Well... that's essentially the same as wrapping.

   I think probably we do want to just wrap.

   Oh, here's the real issue:
   We don't know how many of the sources there are.
   Many different things can possibly cause the variable to change.
   How do we ensure that the user appropriately detects the variable-changing event,
   and appropriately changes the variable?

   Well, the event throws an exception.
   Or, perhaps, can be linearly typed.

   I mean, when we get an EAGAIN...
   That's not really an error. It's a return value saying,
   "hey yo, I don't have anything more to give you right now, come back later, ya dig?"

   But still.

   If there's many sources and they all change the variable, well
   The obvious solution is still to wrap all the sources.
   Though, we don't necessarily have to wrap them to know about the variable.

   We can have the sources return a value that must be consumed by the variable.
   Though that requires a bunch of types.

   I think the best model for Python is to just have the EpollWrapper wrap every method.
   And update the readability status thing when it gets EAGAIN

   It works! Yay
** type-directed model
   We could have some operation on an fd,
   consume the fd,
   and either return the fd and some data,
   or an EAGAIN wrapping the fd, of some specific type.

   No we'd have to work with fd-operation pairs.
   Well, in any case, that's the basic model.
   Let's call the fd-operation pair just an "operation".

   So we have an operation.
   Operation -> Either (Operation,Data) (EAGAIN::Read Operation)

   We consume it and either get data and the operation back, or no data back and the operation wrapped in an EAGAIN.

   To unwrap the EAGAIN, we need to get a read posedge on the source.

   But, the read posedge has to come after the operation has been done.
   Tricky.

** no unique shared fd types
   It's too much overhead.
** TODO create epoll wrapper
   This has two advantages:
   can pass arbitrary data into epoll
   can pass weird epoll flags

   no wait the only weird epoll flags go in the event mask and I can already pass all those from python
** things which operate on file objects
*** Open file description locks
    urgh how do these work
*** File offsets
    but I can explicitly specify the offset.
*** O_NONBLOCK!!!! and other file description flags
    ugh EPOLLET really very much requires O_NONBLOCk to be set, otherwise you can't tell when to stop reading
    (though for streams (as the manpage says) you can detect partial reads)
** next up: supervise
   I could maybe do clever things to support multiple children in one supervise.
   But let's do the simple one child case first to see how it works first
** only on one thread thing
   I find myself frequently wanting to do a thing in only one thread calling into an object.

   Once the thing is done, all the callers can return.
   But, just, only one of them needs to actually do it.

   I find myself frequently wanting to do:
   "here is an async method. multiple tasks can call this async method at once. "

I find myself frequently wanting to perform some function in only one task calling a method,
and have the other tasks just wait until that function is done.

   It's kind of like picking a task to schedule some processing on,
   and then if other tasks call the method too, they just have to wait until the processing is done.
** resource issues
   many resource leakage issues are resolved by the fact that resources are local to a task
   and the task can be destructed all at once.
** fd leak on script interpreters with execveat(fd)
so thereotically we should just never cloexec it
always let it pass through
in which case,
the interpreter will get an argument of the form /dev/fd/N
well, the interpreter should really know to just use that as an fd!
and I could just send patches to bash and python to fix them...
if they get an argument like /dev/fd/N, they should just use that fd instead of reopening
especially because the permissions might be tricky on, say, memfds or whatever
though, both of them can support running scripts from stdin.
i guess the real issue is that the filename is provided as a path not stdin?
well, if it was provided on stdin then stdin would be used up, argh.
yeah so providing as an fd argument is best
I'll code around that for now,
and use it to test...
well no
I'll write a minimal C program that execs into something via open and execveat
also this works for sending the script once??? over a pipe??? not over stdin??

oh this doesn't work because normal executables will have to close the fd too
but then how does the dynamic linker work??

argh, the program is already loaded into memory by the time the dynamic linker runs
hmm so how can we handle this for ELF binaries?
for elf the fd is loaded into memory and isn't needed.

argh
should we just dispatch based on whether it's a #!?

blargh I guess we can't execveat for now
** process stuff
   pids are mostly terrible but only mostly

   as long as you only kill your children, you can work with them in a race-free manner

   I need to implement process management stuff (wait, sigchld, kill) in Python, not just rely on supervise.
   This way is less racy (can be certain I'm killing the thing I expect, while supervise can pid loop and I can kill the wrong child)
   And it reduces my dependence on weird stuff (supervise)
   and that weird stuff, I'm having trouble figuring out how to represent anyway.

   should I set subreaper? maaaybe? meh I'll figure that out later

   So, OK.

   I'll have some multiplexing on top of a sigchildfd,
   which then does a wait,
   and dispatches events,
   and sees what it sees.

   And kill on top of that.

   Essentially it's the same multiplexing as for epoll.

   so we'll have a Process object?
   which represents an existing standalone process?

   maybe it should be a task object instead?
   so we can accurately represent pseudo-threads and pseudo-processes and all that stuff?

   ChildTask, perhaps?
   Representing a standalone task that is a child of ours?

   Only direct children can be safely killed, since grandchildren can be collected.

   So ChildTask is only for direct children.

   What if we double fork?

   Well...

   Well, what if we double fork and then take ownership of the intermediate task?
   We want to support that.

   I guess we have some kind of ChildGroup thing
   Or like, some kind of TaskId thing which points ChildTask points to as its parent?

   And a task's TaskId can't really change..
   but our Task structure thing can.
   Well, actually we should really model it as a thread, not a task.
   Since the TaskId can change and thereby change what Linux task we're operating within.

   Thread makes more sense really...
   It's sfork that allows us to change our taskid.

   Fork too, kinda. But let us not speak of fork, that forbidden syscall.

   So sfork lets us change our taskid.
   But really it pushes our taskid on to a stack.
   We should make sure to explicitly model the fact that it changes our taskid.

   It would be unrealistic to pretend that it doesn't change our taskid.
   Since it's visibly different - children have a different parent.

   What happens to children when you change process namespace?

   We'll have a Thread, which contains a Task (which changes) and a SyscallInterface (which does not)

   We can consume a Task to produce a ChildTask, namely a thing thing.
   That's exec.

   A Task, I guess, has another blocked Task inside it.
   That's the Task that will be resumed when it sforks.

   I think it's best to have it look like that.
   That's realistic, though not necessarily exactly what the interface will look like when it gets into the kernel.

   So eh...

   A ChildTask contains a parent: Task

   And I guess also a Thread?
   And we can only kill it when thread.task == parent?

   soooooo
   okaaaay

   it's not really a collection of registers and an address space.

   the address space, you know, is actually an attribute of the task.
   and the registers don't even matter

   it's actually really truly just a wrapper around a syscallinterface
   and the task it corresponds to, can change.

   how would we even represent this in the kernel, sigh

   I guess we can't?
   what does it really mean to have continuity here?

   it's an illusion
   we copy our thread and we think we're the same

   but we're actually a copy

   are there any other means of having a syscallinterface other than "syscall" and "fd"?

   well we could do it over shared memory.

   and, doing it over shared memory...

   well, we'd have a task which we sfork inside, to create a new task,
   which is still listening to the same syscallinterface.

   so... we just want to track what task is currently active on the other side.

   could multiple tasks be on the other side?

   like, we send a syscall and,
   we can send another and,
   how do we determine which task gets which syscall?

   do we want the syscall interface to somehow give us some kind of token?

   which we can use to make a syscall on a specific task? ha ha.
   that's exactly the same as the current syscall interface.

   and anyway it doesn't work for a specific task,
   but rather a specific thread whatever thingy.

   cuz when we sfork inside a task
   it pushes the task on a stack and makes a new one.

   so this is kind of a "mobile thread".
   it's a thread that can move between tasks.
   and move between processes.

   who knows, maybe it could even move between hosts?
   well it can certainly move between containers

   hmm it sure would be nicer if sfork didn't create a new task

   exec after sfork I guess, as always, does three things:
   creates a new task with a new tid in the current namespaces,
   makes that tid have a new address space,
   returns us to the parent,
   blurgh

   wait a second, exec probably also creates a new file descriptor space, to cloexec in?
   blah

   well, okay, so.
   all this would be complicated.

   what if we just accept that sfork does create new tasks,
   it does create a stack of tasks,
   and exec is just the normal thing.

   then all that I'm doing is,
   having a single thread of execution move between tasks.
   the new task starts up with new registers,
   and the old task gets the new task's registers when it execs.

   so I guess this is pretty unnatural
   generally a task and a thread are identical

   so how would I do it otherwise?

   I guess

   I would just start up a task with a new SyscallInterface

   And in that way avoid the problems of "a thread that moves between tasks".

   Actually, for that matter,
   I would even be able to double-fork and start supervise?
   No no I wouldn't

   Wait, yes I would!
   Through the subreaper thing!
   Since it would inherit great-grand children.

   Huh.

   I could even do a CLONE_PARENT to have supervise directly become the parent of the great-grand-children.
   (haha, that would be so crazy)

   So okay, blah blah blah,
   I guess the best thing to do is to use clone to create new SyscallInterfaces.

   So, then, how do I...

   wait, argh.

   To use clone to do this, I would need to start using rsyscall immediately.
   Even for local, shared memory stuff.

   Although I guess I can directly write to the shared address space.

   Maybe I will indeed just do that, have a shared memory rsyscall.

   So okay, I will also need to set up the child stack right.
   
   I guess that I could probably have a C routine to do that, for now, with the one-process model.

   Oh, I guess I can have a C routine which returns a buffer or something?

   I can get the bytes that need to go on the stack,
   and then pick how I put them there.

   Yeah, yeah, sfork is weird, making tasks on the fly is better.

   Let's be sure we can do this right with remote rsyscall though.

   I would need to prepare the stack.
   But I guess I can prepare the stack just fine.
   It's just a little (a lot) weird.

   Though I wonder if I can leave child_stack NULL?
   Probably not, I do have to change the fds that I use.

   And no matter what, it would loop and then they would both read from the same fd.
   hmmmmmmmmmmmm

   OK I think building the stack is fine

   Doing this with rsyscall is just clearly better.

   So that's my next project.
   In-process rsyscall so that threads work.

   Yeah, just making more tasks is clearly better.

   Oh, also this means I no longer segfault if I don't use sfork :)

   Oh, let's just have a C function that starts an rsyscall thread in the current space.

   Er wait no that's not right, we need to control the clone args.

   hmm I guess we could use CHILD_CLEARTID and futexes as a way to get notified on child process exit :)

   oh wait no, exec will wipe that out

   okay I see, clone_child_cleartid and settid have to be there because
   uhhh, probably posix reasons
   oh, cleartid is how notification of child process exit happens.
   and I guess wait can't be used because, er, tricky posix reasons?
   whatevs

   oh also let's have the task have its own fd space, so we can get easy termination notification still
   er, probably.
   maybe.

   haha this use of CLONE_PARENT will be great
   well, maybe

   okay so it does seem pretty useful to have in-process rsyscall things I guess
   which I communicate with using file descriptors so they can be polled on

   oh, so!
   ptid, ctid, and newtls can all be ignored for our use case.

   we'll get notification of exit through fd hangup, I guess.
   possibly through __WCLONE if we must

   I don't think we need to or want to specify CLONE_THREAD
   although maybe we do

   all we need to specify is the flags, and the child_stack, and possibly the child signal

   we'll build the child_stack ourselves, hum hum...

   will we mmap it?
   instead of mallocing it?
   probably? that's the most common with a totally remote thing.

   we'll still do direct memory writing to it I guess

   oh, to kill a task, we'll, er
   close the infd,
   wait on the outfd,
   unmap the memory.

   okay so the role for C is just to build the stack

   what will the stack even look like?
   I guess we have some assembly routine which loads registers from the stack,
   then calls...

   oh wait, oh no

   clone continues from the point of the call??

   how do we fix this then...

   so I guess in a low level implementation of syscall,
   we'll call the system call,
   then ret.

   the ret will allow us to jump to an arbitrary location by manipulating the stack.
   okay, seems reasonable.

   so we'll write that component of rsyscall in assembly

   and skip the whole errno drag too

   hm.
   newtls, that's tricky.

   I'll avoid it. I'll make my syscalls directly instead, with no deps on glibc, so I'm totally freestanding.

   So, let's do all the syscall wrapping in Python.
   We'll have a class that provides a syscall interface and takes a function to do raw syscalls.
   Which uses cffi to do memory access.
   And we can wrap that class in either a syscall to thread or not.

   That's so type-unsafe it's not even funny lol.

   I guess I can write standalone Python functions that take do_syscall and are type safe.

   It's like assembly from Python.

   OK so we'll start off with just replacing io.py with cffi to async do_syscall.

   Then we can replace the async do_syscall with a thread-remote one.

   Replacing clone will have to come last.
   Since we'll need to be able to do all rsyscalls remotely first,
   and refactor the interface.
   Also can't do execveat too, because sfork clone needs it to work.

   But can do everything else
   Then add the new interface and test it
   (with a new syscall kinda thing maybe)

   Then replace the old interface

   OK so I converted things.
   Now let's add a clone2 which will do the stuff!

   hm
   should I just pass bytes for the stack

   probably not?

   okay so I need mmap now

   hmm apparently I can't do such fancy relocations in assembly.
   so I can't directly call rsyscall_server

   so probably I should instead call some kinda gadget that will call something of my choice.
   so, syscalls.
   stash all registers,
   pop all registers.
   hum hum.
   if we did that then I guess we'd be good.
   but like, most of the time our registers don't change
   so it's pointless

   we would only be doing that to support the fact that our stack can change
   
   okay, so we can have a generic gadget for turning stack args into register args,
   so we can call an arbitrary C function.

   this will be useful: it will allow us to call (in a separate thread) any C function in our address space.

   we can possibly use that to call dynamic linker functions to load libraries.

   it would be nice if status was a full int, but oh well.

   can we figure out a way to get a return value at low cost?

   maybe after we call, we move eax to someplace?
   maybe the base of the stack? :)

   that would be cute.

   so I guess clone is our "call arbitrary function remotely" entry point.
   just has to be combined with a gadget

   of course, we are relying on the "immediate ret after syscall" behavior.
   can we do better?
** child task monitoring thing
   I'll have some multiplexing on top of a sigchildfd,
   which then does a wait,
   and dispatches events,
   and sees what it sees.

   And kill on top of that.

   Essentially it's the same multiplexing as for epoll.

   This will be a Task object
   Which also implements ChildTask?

   Or maybe I just get back a ChildTask object and a Task object when launching a thread.

   Automatic cleanup is the tricky part.
   We don't get fd-based cleanup since we share an fd space.
   Could we run supervise in the middle to clean up our threads?

   That does seem possibly viable.

   Oh wait no!
   supervise won't help either because it won't get notified by the fd closing.
   Hm!

   I could resort to PDEATHSIG to notify supervise.
   That would work perhaps.

   The nice thing about thread groups is that they offer a guarantee.

   They are not nestable though

   There are too many task cleanup things in Linux, and all of them suck!

   I could also PDEATHSIG to kill the child threads.

   classic problem, classic problem

   I guess supervise + PDEATHSIG is fine.

   But I still don't like the fact that I don't have race-free-ness!

   I'm not killing my direct children...

   To have a cleanup task, my children must be theirs...

   Hmm, could I do an arrangement like...

   me -> supervise (with subreaper) -> childspawnerwaiter -> [all children]

   Then when I die, supervise is notified, and kills childspawner and all other children.

   Hmmmmmmm curious, curious.

   Alternatively, I guess I could wrap around me instead.
   Then I could spawn and wait on children directly.

   supervise -> me -> [all children]

   Like... treat me as something which can be sigkill'd?

   Essentially externalizing the cleanup?

   The fact that my children share my fd space makes this all tricky.

   I could just PDEATHSIG them too I suppose.
   
   Well... hmm. Then if someone sigkills supervise, things break.

   Oh wait, does SIGHAND just negate this whole problem?
   Does it mean that a signal to one, causes all to die?

   We should check that.

   OK yeah

   If I set PDEATHSIG for everything
   And have threads/tasks I control be direct children,
   and have uncontrolled things run under supervise to clean them up,
   everything will be good.

   This does suggest I can't exec in a direct child to something uncontrolled.
   Unless I can figure out a way to..

   Oh!
   I'll just fork off a child of my direct child
   and exec supervise in my direct child.

   So this seems... really good and clean!

   Direct children (such as supervise and rsyscall) can be trusted to terminate when I SIGTERM

   Indirect children (under supervise) get a harser approach, being SIGKILL'd by supervise.

   so it will look like

   me -> rsyscall, rsyscall, rsyscall, [supervise -> [nginx -> worker, worker, worker]]

   And PDEATHSIG will ensure cleanup.

   We don't really have to set PDEATHSIG on supervise since...
   well actually we do have to, because we could have:
   me -> rsyscall -> supervise -> rsyscall

   And fd sharing between 1,2 and 4.
   (presumably only temporarily, while execing in 4)

   Then we'd need supervise to be killed when 2 dies,
   because the fds wouldn't be closed until 4 dies.

   What would the kernel support look like then?

   \_()_/

   It's not clear yet whether supervise will be doing child monitoring?

   I guess we can do:
**** me
***** rsyscall
***** rsyscall
***** supervise
****** rsyscall
       Here we will monitor child processes and stuff.

       It seems like it would be better for the thread to exit by raising a signal.
       That way it can be handled.

       And we could have a signal handler that does filicide and terminates.

       That would allow removing supervise from the picture
******* nginx
******** worker
******** worker
******** worker
*** continuation
    but wait, how does this interact with the remote use case?

    I guess it's pretty hard to load a library remotely

    Although actually I guess it could be pretty simple.

    So let's assume we can do a remote library load.

    Then we'd have our daemonized rsyscall process,
    which is controlled by its fds.

    When we shut down, we terminate the connection and its fds close.

    It reacts by raising a signal.

    That calls filicide if the signal handler is registered, but either way it terminates rsyscall.

    That results in PDEATHSIG terminating all immediate children.

    That all seems sensible.

    Oh wait, I can even load the library remotely through using clone-to-run-function hacks!
    What a cool hack that is.
    I just need to run it through ld.so or something, so that the dynamic linker appears in my address space.

    For now let's just put supervise in the middle.

    In this model, supervise does basically nothing at all:
    It just provides a process that calls filicide on exit.

    It's like a hack around the fact that I can't load libraries to get a function to register as a signal handler.

    So yeah, this all seems good.

    supervise then is really a simple utility.
    It does basically nothing.
    And is useful generically, and from the shell.
    well no not really, since it doesn't exec for you :)

    we're just using rsyscall or other magics to handle the child status reporting below it, if we care about that.

    seems great!
*** ways to clean up tasks
    thread groups

    process groups

    controlling ttys

    pid namespaces

    control groups

    PDEATHSIG

    I guess the ultimate way is to have a separate cleanup task.
    Which I don't SIGKILL.

    And I guess I'll use PDEATHSIG to notify it, shrug!

    We can also clear pdeathsig if we want to detach.

    So PDEATHSIG is a decent thing. I guess we'll keep it.

    filicide, then, is what we need. It needs to be in the kernel so it's robust to SIGKILL

    It's a thing which sigkills all transitive child processes when your process exits.

    I guess essentially it just SIGKILLs all children until there are no more children

    subreaper is required I guess.

    essentially we want a parent-level attribute that specifies the signal to send to all our children when we die.
    that way our children can't turn it off.

    and we want to keep sending it as we get more children reparented to us, I guess.

*** how did LinuxThreads do it??
    how did they manage to have SIGKILL to one thread, kill all threads?

    probably by pdeathsig or somefin
** implementing the child status monitoring thingy
   so I'll allocate a sigchld signalfd

   then attach it to an epoller

   then wrap it in a multiplexer

   to dispatch waits to the right associated ChildTask

   note well that subreaper means I'll get wakeups for unknown children.

   rsyscall[reap]

   so handling sigchld and manipulating child processes isn't so bad after all
   IF you're the only one in the process doing it

   actually no, just: if you don't have to stick to posix semantics

   hmm.
   aren't privilege level changes in Linux threading libraries inherently racy since one thread has to change before the others?

   Oh okay I guess first I'll do a signal multiplexer thing.
   I'll explicitly opt in to signals that I want to receive instead of terminating me?

   Then we take the SIGCHLD queue and

   We wrap it in Waiter

   Which has a "make" method which returns ChildTasks with a given tid.

   Or actually I guess a clone method?

   No, Waiter returns just ChildTasks.

   We have another thing that takes no arguments and returns a (Task, ChildTask) tuple

   Since it needs the pointer to rsyscall stuff to launch.

   We also want to have some kind of SupervisedTask which is required to fork off
   things which can
   exec random binaries

   And what about our trampoline which lets us run arbitrary C functions?
   Shouldn't we expose that?

   And that just has a ChildTask dep...

   Or rather it's a wrapper around a ChildTask.

   We don't want to share SIGHAND; not sharing SIGHAND means we can have specific tasks do filicide,
   without everything doing filicide.

   VM and FILES are both namespaces for dynamic resources,
   which we can safely share because the kernel multiplexes/allocates in a non-interfering way in those namespaces.
   (as long as we don't MAP_FIXED or dup2 to an unallocated address/number)
** rsyscall connection

   OK!

   So now I need to turn an RsyscallConnection into a syscall interface.

** runningtask

    Close this to murder the task.

    But wait what if the task execs?
    Then it is also fine to free these resources.

    But how will we know?
    HOW will we KNOW?

    So we can never exec in our base task, right, because then there will be no-one to
    filicide for us. Though, if we make a supervised task, maybe?

    So we have to figure out how to consume a task's resources after an exec.

    Also how does this relate to remote/out of process rsyscalls, ya dig?
    With those, one task may depend on another to be accessed! So we can never really exec them.
    So we do kind of have a differentiation between leaf task and lead task.
    Or rather, tasks which are depended on by other tasks,
    and tasks which aren't depended on by others.

    Right, because, only leaf tasks can exec - if a non-leaf task execs,
    we can no longer track its children!
** passing an epoll'd socket down to a subprocess
   Oh no!
   We need to be able to turn O_NONBLOCK off,
   but even if we're in a separate CLONE_FILES,
   that will still affect the same object.

   So we need to, in some sense...

   Oh, wait!
   Since we're passing it down to a subprocess,
   we can't ourselves continue to O_NONBLOCK on it.
   It needs to be consumed by the subprocess.

   So we'll just have some kind of release_from_epoll(),
   which returns the underlying,
   and unsets O_NONBLOCK,
   so it can be used by the subprocess.

   All fine.

   Connect and stuff can happen asynchronously without a problem.
** IntFlag!!!
   Woah!!! Use this thing for all the flags!!! So excite!!
** name
   it will be a family of related libraries that are similar in structure

   so, rsyscall is I guess an okay name
** binding sockets
   so I can do:
   chdir; bind(./name); chdir

   or I can do

   open(O_DIRECTORY); bind(/dev/fd/n/name)

   the former gives me a full 16 or so extra characters.

   but obviously I have to do the latter
** fifos
   interesting, fifos should always be opened for reading with O_NONBLOCK

   that way they won't block forever when opening for read
   which means deadlocks aren't possible

   maybe all opens should be done with O_NONBLOCK

   nah it has no effect for files
** path object, O_PATH fds
   There are a few common operations between Paths and O_PATH fds.

   namely: fchdir, fstat, fstatfs

   and we can probably use it with AT_EMPTY_PATH

   do we want to support having a Path which is backed only by an O_PATH?
   maybe, let's think about it future
** sockets
   So to know the address type for a socket,
   I need to know what kind of socket it is.

   There are a bunch of calls using the address type:
   bind
   connect
   sendto
   recvfrom
   getsockname
   getpeername
   accept

   Also, sockets are only readable once they're connected.
   so...

   Also, after a shutdown, they're no longer readable.
   Hm.

   Let's not try and track the readability status.

   But let's indeed track the domain in the type.
   Since it can't change

   And what about the type?
   er, that is, the sockettype.

   Maybe I should mix that also into the class type?

   Okay, so all three parameters can change the address type, actually.

   Maybe I should focus specifically on the address type for these SocketFiles.

   Maybe it can be, like,
   SocketFile[InternetAddress]
   SocketFile[UnixAddress]

   And so on.

   Hmm, we also need to think about how to handle fd passing

   I guess when we recvmsg,
   we get back a t.Tuple[t.Optional[T_addr], bytes, t.List[ControlMessage], MsgFlags]

   Then we can parse the ControlMessages to see what kinda do-hicky we got.

   But, this can all be represented low-level.
   t.Tuple[t.Optional[bytes], bytes, t.List[t.Tuple[int, int, bytes]], int]

   god the socket API is so crap
   i think, maybe

   okay, so the question is whether we have the address type above or below the FileDescriptor.

   I think below, it's nicer.

   But processing of recvmsg results is above the FileDescriptor.
   What class does that take place in?

   Well, I suppose each ControlMessage will be parsed.

   And will already contain whatever fds we pull off the socket
   But we need to specify a File type.

   I guess we can change the File it refers to, after the fact.
   Just have it refer to a plain base File at first.

   OK! Remember the central goal:
   Expose Linux features as they are!
   Do not become opinionated!
** unix sockets
   interesting, connect on a unix socket immediately returns without blocking,
   as long as it's to a listening socket

   it doesn't block until the server accepts
** child task scheme, final version for sure
   So I can clone and exec at any time without restrictions.

   And when I clone I'm relying on someone up the supervision tree,
   to have enabled filicide.

   And there's an easy helper to take a task and enable filicide on it.
   (either by signal handler, or execing, or starting up a thread waiting in sigwait then calling filicide)

   And when I clone I get a ChildTask back,
   which is usually wrapped in a larger [Something]Task,
   which holds all the resources given to that ChildTask,
   so that they can be freed when the child is killed.

   And for rsyscall tasks,
   that object is RsyscallTask,
   and it has a reference to the Task,
   and it has an exec wrapper method,
   which execs in the Task and frees the resources it used and gives me the ChildTask.

   But I don't have to go through the RsyscallTask to exec,
   I can just exec directly if I so choose,
   including from the root task on any system,
   even though that may break things.
** exec
*** DONE child tasks stay around after exec
   Hmm.
   Also, when I exec, how do I make my child threads go away?
   If I exec up in the tree, my children won't get signal'd, because I am not exiting.

   Actually I think that is fine.

   I have a neat visual of the Starcraft terran HQ, lifting off, setting up a big hierarchy of threads,
   then anchoring again and transforming into a high-efficiency C program/factory/assault fort.

   For it to be efficient, though, we'd need to free lingering memory and close fds referenced by no task.
   or something. but that's for later.
*** TODO detect when exec has completed
   Though, hmm. how do I detect when an exec has completed?
   And won't now return with an error?
   Traditional way would be to use files being closed.

   This is one place where sfork was nice.
   When I exec'd, it returned to the parent on success.

   How would I ideally do this?

   Well I could use the sfork trick I guess
   Hm! This is a puzzle.

   So after exec, the fd table is unshared.
   So maybe we could start off by unsharing the fd table?
   Then we close the rsyscall-side fds,
   and then we exec.
   Then we'll get either a hangup or a result.

   Maybe I can just close the fromfd,
   so that if it tries to respond,
   it gets a sigpipe,
   and exits?

   Yes, let's just convert an error into an exit.

   Wait, wait, wait, again I can't do that, if I want to exec into rsyscall
   Because, if I want to share the fd in the threaded rsyscall with the process rsyscall,
   I have to keep them open.

   Well if I'm doing that then I'm not freeing all the resources owned by the task.

   I'm pretty much only freeing the stack space.
   So then the question is: How can I know when the stack space is unused?

   Hmm, I guess this is an issue that is solved by running in a separate address space.
   Then the stack space is definitely freed up when the task exits.

   But if I want to use clone...
   And thereby have a thread in my address space...
   I have to manually free that.

   So what's the trick?
   How do I know when to free the thing?
   Well, after an exit or an exec.

   OK, and also keep in mind that the task might not be our child.
   It's gotta be task-only.

   Though... in that case we don't have anything to free.

   Oh dang, also we can't do it by exiting on error,
   because then we don't know when to free stuff on success.

   We need some kind of event after the exec but before exit(0)

   If we constructed a process from scratch,
   then kicked it off,
   then it did something to no longer need the resources we gave it,
   shouldn't it free those resources itself?

   So we're in a situation where it's just borrowing resources
   And we need to know that it no longer needs them.

   Well how the heck do we do that

   I guess there's an easy way to be sure: if the address space changes
   For fds we can just see when there's no more object references

   Ehhhh hmmmm
   I think separate fd tables and address spaces are good because they keep things nicely reference counted
   Everything in a space is needed by that space,
   when a space goes away all those things have their refcounts decremented,
   and maybe go away.

   Yes, yes, that is all true!
   But what about when it's not a separate address space!
   Oh. Maybe then we can't call exec?
   But, argh, then how do we even get new tasks in the first place?

   So okay, clearly a separate process (address space, fd table) is very nice and it keeps things nicely refcounted.

   But we need to also support the case where we aren't in a separate fd or vm namepsace.
   In that case we have to know when things leave ourselves.

   So how do we figure out when the reference to the stack is dropped?
   Indeed even other more rich functionality could theoretically switch off its stack.

   Well, those things would have rich signaling functions to say when they drop refs.
   (or just be explicitly controlled by the runtime)

   But if we're talking just system calls, how do we do it?

   Well how would we manage this prop'ly?

   The stack would be linear and passed to the thread
   When it goes to exit or exec,
   we'd free the stack purely in registers.

   Okay, so that is all fine and good.
   What stops us from doing that with exec?

   Well with exec,
   the thread stops at the point of exec,
   it doesn't get the chance to continue.

   What is this in terms of continuations?

   OK so it looks like I can use CLONE_CHILD_CLEARTID

   I want to get notified on mm_release,
   which notifies currently on cleartid and vfork.

   cleartid is gross though because it does a futex wakeup instead of an FD wakeup.

   a CLOEXEC pipe is nicer than CLEARTID maybe

   Oh, but we have to unshare before exec if we do that.

   urgh

   hmm so that seems like it might be okay
   we'll force the thread to exit if its exec fails?

   how does that interact with main tasks and such?

   well, if a main task execs,

   well, I can't figure out a task to monitor it from in that case!
   a parent task who can share a pipe with it, argh

   the same problem applies for any exec-monitoring thing

   the problem of responding to the notification and freeing the resources after the exec

   that requirese some kind of monitoring

   which I guess only the wrapper can do.

   some kind of on-the-fly vfork?

   like, instead of setting vfork,
   I call block_until_child_exec,
   urgh which is essentially the same as vfork
   
   You could emulate vfork with clear_child_tid, I think.
   Do a clone,
   in the parent do a futex wait on an address,
   in a child do clear_child_tid,
   then proceed to exec.

   so we'd want something that we can bake into our event loop

   I guess if there was a waitid event on exec,
   then...

   we could also emulate vfork that way?
   neat

   so OK,
   two methods suggest themselves to me.
   1. use CHILD_CLEARTID
   2. use ptrace

   ptrace would obviously be disgusting but, is it really that bad?
   it's certainly better than futexing trash...
   and I only have like,
   a tiny bit of ptrace.

   But, of course, that is a portability problem because ptrace can be turned off (through the YAMA LSM)
   But, eh, whatever.

   ok so futexes seem utterly crap, I don't see a way to wait on multiple things

   could I like, mmap an fd or something, and specify that

   ughhhghhh

   so i'll just do traceme,
   set PTRACE_O_TRACEEXEC,
   forward any non ptrace signals,
   and untrace once exec is done successfully

   look on the bright side, supporting ptrace is cool!

   oh hey and uhhh
   could I speed this up with that seccomp yield to userspace  things?
   nah, there's no way to exec in the original process with that
   it tells me before the syscall instead of the syscall

   ptrace is not an acceptable solution
   because ptrace isn't recursive

   CHILD_CLEARTID or vfork it is.
   vfork is probably just plain unsuitable.

   so we need to figure out futex integration.

   I guess that essentially means one thread per futex.
   Doing.. what?
   ugh, maybe just bridging a futex to an eventfd?
   that would be cool and generic I guess...

   OK let's do it.
   Not that specifically, but a thread dedicated to waiting on the futex.
** fd namespace   
   It should contain a list of tasks in that namespace.

   And when an FD's task goes away,
   it picks a new one from the FDNamespace list.
** file descriptor table inheritance issues
   fork is really problematic
** shared vs unshared file descriptor tables
   The benefit of unshared is that we can detect things via file descriptor hangup, which is nice.
   Also it's just a lot more nice isolation, I guess.

   What's the benefit of shared?
   Well...
   We can very easily pass file descriptors between tasks?
   But we could do that with unix sockets anyway, just slightly harder.
   We kind of don't have to deal with inheritance being tricky,
   since any fd in a space is valid in new tasks.

   But we kind of want to explicitly move fds between tasks anyway,
   so...

   On the other hand, do we really want to rely on unshared fd tables?
   Because, isn't that somewhat expensive?
   Maybe not that expensive...
   It's mainly shared memory that we want to preserve.

   So we'll have the shared memory threads,
   but each with their own fd table.

   That seems fairly good I guess.

   I doubt we'll have issues around moving between tasks being too expensive.
   And if we do, we can resurrect the trickiness of shared fd tables.

   The issue with shared memory is the same as with shared fds, though.
   When one dies, its resources aren't freed.

   So I guess we could just go all the way to separate processes.
   But spawning them requires spawning threads, anyway, so...

   well, not if we use sfork... blargh

   okay, separate fd tables it is
   that is clean, I think.
   and also, that's kind of natural for "process is a virtual machine" notion.
   actually no it's not, why would each CPU have its own fd table,
   ridiculous

   maybe because of NUMA???? NUMA fds???? NUFA????

   maybe we should exec so we trigger cloexec so only explicitly passed fds go to our internaltasks

   if we start out shared, then move to unshared,
   how do we handle this inheritance question?
   we have to deal with that either way.
   how do we handle inheritance?
   it would be nice to have a CLOUNSHARE maybe
*** the file descriptor inheritance question
    I still need to think some about how to properly handle the semantics of fd table inheritance.
    that's why I currently have all my child tasks using CLONE_FILES so they share fd tables
    because that way there's no inheritance going on

    fd table inheritance means that any libraries not controlled by me will have their fds duplicated and kept open,
    which might break those libraries

    so this is why I can't have things in their own fd table, but in the same VM space as me.
    the only way to trigger clearing of the fd table is through exec.

    if there was a clounshare...

    wait wait

    a thread that I unshare in,
    if it's running rsyscall,
    can't use any fds I don't know about.
    since I won't send those commands

    but if it's running arbitrary code

    oh wait no

    it's the other libraries, the ones left behind
    the threads that live and breathe with fds unused
    if I fork a thread off, and then unshare, and I have the fds, and the original guys close the fd,
    then that's a weird situation, one which cloexec is supposed to prevent.

    I'd really want to have CLOEXEC also affect unshare.
    Then I don't have any of these issues, right?

    well... I can implement this myself with scanning the fd table.
** argh
   okay so I have two options

   option 1: implement CLOUNSHARE so I can deal with unsharing the fd table,
   and then have each task in an address space have a different fd table
   (this doesn't provide a way to exec into a new process and keep using the same rsyscall fds, right?)

   option 2: use futexes and child wait and all that stuff.
   and pdeathsig.
   this is gross but I can have everything in the same fd table so it's fine
** argh argh option 1
   OK, so let's be realistic, right?
   Isn't this required for starting subprocesses?

   We unshare(CLONE_FILES), assign some fds to some other fds (fds which we explicitly know the number of),
   then exec, and it's all good.

   There's no way to do the stdin/stdout assignment without unsharing CLONE_FILES.
   At most we could first exec a separate rsyscall process,
   and then start running from there.

   Although.

   Doing either of those irreversible things, really seems like maybe it something I shouldn't be doing.

   Feels like maybe I should have unshare(CLONE_FILES) from the start.

   Then I start my child, not passing CLONE_FILES,
   and it inherits all my fds.

   (I can have some other mode for running a thread pool for blocking syscalls over shared fds)

   Man, doesn't this suggest that we probably shouldn't even have CLONE_VM either?
   What if I keep alive some memory that a library was usin'?

   No no, that's exactly opposite. We're all in the same address space, but that's fine.
   Each one manages his own stuff.

   And we never need to unshare(CLONE_VM) (and it's impossible anyway, lol)
   Because there's no refcounting hangup stuff that we want to trigger.
   And things are never placed at known offsets.
   And all that stuff.

   Yeah, there's no way to inherit a CLONE_VM.
   You're either in the same address space or a completely new one.
   Er...
   Well, that's not true, you can inherit CLONE_VM as you move to a new thread - fork for example.

   Well, we won't be inheriting CLONE_VM, anyway.
   We'll be doing it all in a single place, right?
   A single address space

   And we manage memory explicitly within there.
   Why do this?

   After all, if some thread in that address space dies,
   its memory resources will not be collected.
   Only its fd resources.

   Why don't we just forcibly always move to a new process, and exec the thing?

   Because it's far cheaper to write to memory in a single process.

   Well, we could provide a shared memory section.

   And isn't this also an argument in favor of always being in the same fd table?
   After all,
   it's far cheaper to pass an fd to another task when you're in the same fd table;
   you don't have to send it over a Unix socket.

   But the thing differentiating the two is the new-process-start.
   That is, what happens when we exec.

   When we exec, the address space is destroyed and recreated, not inherited - fine.
   But the fd table is inherited.

   This means we can avoid dealing with address space inheritance.
   We could just have everything in its own fresh address space,
   all fine and good.

   But we can't avoid dealing with fd table inheritance?
   Is that right?

   It would be nice if we could get a fresh address space and put things in it.
   Inheriting addresses would be weird.
   And a gross abuse of virtual memory.

   Since ideally everyone would be in the same address space...
   And exec would merely set up a new program area...

   Which, in fact, we could do in userspace...

   We could implement processes in userspace -
   but only if we had each with their own fd table and sighand and stuff?
   (and assuming that malloc et al was smart enough to not collide other things)

   I mean, if they syscall directly,
   they really do each need their own fd table and things.

   But that is fine.

   Running them in a single address space, somewhat neat, though somewhat useless.

   But! The issue is!
   CLOEXEC does not trigger!

   When we start a new thread running a program we've loaded,
   its fd table is unshared,
   but not CLOEXEC'd.

   Furthermore, if we first spin up a thread to do preparation of the fd table,
   it'll be crazy


   OK!
   classes of fd:

   explicitly passed down: not cloexec or clounshare
   private to a thread: CLOEXEC, CLOUNSHARE
   dynamically passed down: not cloexec or clounshare

   But, what about when we fork?
   Do we really want all the library-private things to go away due to clounshare?

   so when we fork off a *thread*, or when we exec a new *program*, we can't use any of the libraries we had before
   even though the thread could call those functions, because they're in its address space

   but if we fork our process, the libraries are in our address space,
   and we're in the middle of a call stack,
   and we probably are going to want to use those libraries.

   okay, triggering cloexec

   urgh I want to be able to iterate over the cloexec fds from userspace :(

   okay, okay, we'll hack it, a lot of fds to iterate over, but whatever

   so this kind of leaves it as,

   I can pass in some fds into a new thread,
   and some will be dynscoped in.

   If I want to have multiple people operate on the same fd,
   I can just pass it into all of them.

   But the fd *table* isn't shared-mutable,
   the mutations each make aren't visible to others.

   Yes, it seems good

   okay...

   I wonder if I can support both methods?

   Both "perfect shared memory eternal glory",
   and "sublime justice private fd table".

   So I will clearly want to do CLONE_DO_CLOEXEC, right?
   Always?

   Okay, so what I have is,
   two different ways to handle detection of the memory space no longer being used.

   OK, so.
   The private fd space way is cleaner, isn't it?

   That's incremental.

   Now, a true thread would support sharing the table, wouldn't it?

   Well, no.
   It's cleaner for each thread to have its own table.
   FDs are exclusively owned by one thread, after all!

   And that allows for neat process stuff.

   But, sharing the fd table...

   Hey wait a second
   The futex task also exits when it's left the fd table.

   But, it's not clear what the task has taken with it.
   So does it really allow us to close those things?

   We can clearly close the fds in our address space
   The question is whether we can close the communication fds.
   And, again, yes, we clearly can, unless we are reusing it.

   Really, each thread having its own private fd table seems better.
   But it is limiting.

   Language-level access control over who owns an fd is more flexible/better/more glorious.

   We'd need to, um...
   On thread exit, track down all the fds it owns, and close them?
   Along with memory resources too?

   Well uhhhhhhhhhhhhh

   A thread doesn't really own an fd nor does it own memory.
   We have all that managed centrally.

   I mean, why would I use my own thread system, rly
   Well, to integrate it with a nice beautiful epoll event loop.

   OK, so let's suppose I start a thread that shares my fd table.

   I need to be notified when it drops the stack.

   Then I go forward and unshare the fd table.

   I need to CLONE_DO_CLOEXEC (after unsharing) so that private fds, and the fds of other threads, are not duplicated.

   Though.
   Yes, that makes sense.
   It's not possible to unset CLOEXEC while multithreaded-in-fd-table,
   since it may be inherited.

   Everything (except for dynscoped things) is CLOEXEC.

   Then I unshare and inherit everything,
   then I perform some manipulations and unset CLOEXEC on some things,
   and then I call exec.

   So I don't need to CLONE_DO_CLOEXEC, unless I'm not going to exec.
   Unless... I regard this as racy.
   Having the duplicated fd... is there a race there?
   Might I need to do this immediately, atomically?

   Maybe what I need, then, is some form of atomic,
   unshare, unset cloexec on things, do_cloexec

   I mean, alternatively...

   I just don't inherit anything?
   Sounds gross, that's not acceptable.

   So then, maybe something like:
   int unshare_fd_table(int *inherited_fds, size_t count);

   Which unshares the fd table, closing everything in it that is marked cloexec and which is not in inherited_fds.

   Well, what if I implement that by a socketpair?
   Oh, ha ha, I can't do that, because I can't inherit the socketpair! Gross.

   Okay, so I don't think there's a race here.
   Just imagine that I had unshare_fd_table, it's instant, sure?

   But it would be equally instant for me to do the unshare and unset cloexec and call do_cloexec.

   Er.

   That is, one possible interleaving is,
   if there's some thread A that wants to close fds which another thread is waiting for,
   A doesn't run for a while.

   But wait, there's certain things that aren't possible.
   Like.

   Thread A could close fds 4,5,6,
   then signal using a futex that they closed them.
   Then other coordinating process would believe with absolute certainty that it should get hups on them.
   Immediately! Right?
   (if they're local pipes, sure)

   Well, no, some other thread could also have just forked, as things do.
   Since it could have always happened before by fork, I think we don't need to care.

   But suppose we did care!
   It would sure be tough to deal with!

   How would we ever go from a shared fd table,
   to a private one,
   inheriting some things from it,
   while closing the things we don't want,
   without racing against others?
   ...locks?

   a lock on... close?

   no, yuck, screw that

   so if a library somewhere deep below,
   performs a fork,
   what should they do to ensure they aren't keeping other fds erroneously open?

   well, how do we pass fds to the fork?

   I guess this is a good reason to have something other than "dynamic scope".
   or, I guess, globals across multiple tasks

   you really want something absolutely task-local

   well.

   is keeping something memory mapped an example of keeping something erroneously open?
   the address space may be a problem in itself if that's the case...
** okay
   so
   I guess I'll do the split thing.

   I'll completely get exec and exit notification through the fd hangup.

   I'll additionally poke the thread, after that's done, to free the stack,
   and release the task.

   oh god okay

   so I'll make the thread, unsharing the fd table by not passing CLONE_FILES

   then I'll unset cloexec things,

   and then I will call do_cloexec with a thread thingy

   yeah and this is the only way to do it, I can't do cloexec at thread creation time
** unix task thing
   Environ
** translation API
   So we can translate fds in two ways:

   Through inheritance and through fd passing.

   Essentially we should have a function which takes a file descriptor in one task,
   and returns a file descriptor in another task/fd table.

   That works easily for fd passing

   But the range of validity is more limited for inheritance.
   Because new fds can be created in the old task.

   Also, you can only inherit once. Because after you inherit, the new FD could be closed or whatever.

   It sure does seem like a one-shot approach is better, hmm.

   But it's not truly flexible

   We want to inherit fds while simultaneously making arbitrary calls in each side of the task boundary.

   I mean, what would be the C API?
   We have some library object,
   and we want to use it in the new fork/in the new thread with a different fd table,
   so we have to do some things.

   Well, wouldn't it be better to just call into it and say,
   hey migrate yourself to the new task ok

   And it can make arbitrary calls to do so?

   And what exactly are the capabilities it uses?
   The FD translation, of course, but that's one-shot...

   OK, so again, what would be the C API if we provided FD translation?
   Well...
   We do this post-clone, and...
   It just picks some fds to preserve?
   And otherwise does nothing?

   It does this direct style?

   Doing this direct style seems a whole heckuva lot better.

   It would be nice if we could do this direct style while not having to be staged about this.
   i.e. if we kept the ability to translate forever

   Should we just use Unix socket based translation for everything?
   That is a heckuva lot more expensive...

   Well, no, we can just focus on that style of translation for long-term things.

   i.e., we keep the context manager approach,
   and the only do it once approach,
   and...

   OK ok okay, so just saying, "here are the objects I want to translate..."

   It's not really good because I can just write my code as, "take this translation thing in and then run arbitrary function" object.

   So a contextmanager where I can really run arbitrary code is better.
   And in that scope I do the things
   And I leave that scope
   And the curtain falls - but the task remains usable.

   And I have a check that I don't translate things made after the fork

   Maybe I can do all this in the kernel?

   Well...

   I mean, when I fork and unshare memory,
   then I get a nice snapshot of fds.

   When I unshared the fd table and don't unshare memory,
   the memory can change due to other threads,
   and mention other fds.

   So how do I work with that?

   Maybe if I could, like, build an fd table ahead of time and then use it?
   I guess I would want to be able to,
   operate in one fd table and send fds to another.

   But that's exactly what unix sockets do, so...

   The issue I guess is that to do a Unix socket you need to have a reference to both sides
   But how do you handle that when you have already cloned and cloexec'd.

   What about just passing a list explicitly to initialize new fd tables?
   And have everything cloexec be closed?

   Unshare can take this list too

   I guess the idea is, when you don't pass CLONE_FILES to clone, you can pass CLONE_DO_CLOEXEC and pass a list.
   And when you do pass CLONE_FILES to unshare, you can pass CLONE_DO_CLOEXEC and pass a list

   when I exec, I wipe out all the library-private memory (by making a fresh address space) and the library-private file descriptors (through cloexec)
   (it occurs to me that I can't really do exec in userspace, because I can't find out the library-private (executable-private) memory space)

   But what if I want to pass things down through an exec?

   First I need a thread

   That thread needs resources to do the exec

   I want to be able to actually run code in that thread, not just pre-prepare everything.

   But it does seem like pre-preparing everything is better.
   No! I reject that!
   Pre-preparing is in many ways not possible, such as setuid and setns.

   So if we really run substantial code in the thread,
   then it needs resources.

   Those resources could come through shared memory...
   and shared fd tables...


   Well, okay, so, if I want to run arbitrary code in a thread.
   That's not really something I can do.
   It needs to be in a separate address space not just for safety, but also for cleanup.

   So that's fine.
   So maybe I do that arbitrary logic in a separate process?

   Then what I need is a way to establish that separate process.

   And, so, well, I need a way to pass fds down to it.

   In practice, today, that means setting them as not cloexec.

   Which, actually, isn't that fine if I have a single-threaded space?

   So couldn't I, then, just mangle the fds up and then call a quick clone + exec?

   How do I get the error code of the exec?

   Just put it on the stack, whatever!
   It's easy enough.
   And we can use the futex thing to know about its success.
   
   Could vfork factor into this?
   Naw...

   OK, so this essentially takes the approach of, let's never share our FD table or our address spaces,
   except immediately before spawning a thread to exec into something else.

   That thing which execs, can be - if we so choose - a direct binary
   Or it can be a rsyscall thingy which we can then direct to exec.

   OK, but how does this help at all?

   This is just an approach of, requiring the fds to be specified up front.

   Actually the fact that you can't dynamically translate when doing it in a separate process really suggests,
   specifying the fds up front is best.

   OK, so we'll do that.
   Specify the fds, or whatever, up front.
   Now, we need to be careful about it.

   Should we have the cloexec baked into the thread-starter, then?

   And we just unset cloexec appropriately in our thread,
   and they'll inherit those things in their thread?

   We have to be single threaded, of course,
   but I guess we've always needed to be single threaded (or locked) to pass additional fds down through exec.

   If we did it after the clone/unshare...
   Then we could be multi-threaded...

   If we unshared the fd table so we could get exclusive ownership on fds,
   then set some not cloexec,
   then either did an exec or manually did a do_cloexec (rather than clone yet another thread)

   Well then we could be spawning things without a problom.
   Maybe the trick is to have the thing which runs in the thread and does an exec or a do_cloexec,
   not be generic.

   Instead have it be a totally not-generic thing for just unsetting cloexec.
   If I want something generic then I have to exec through into, rsyscall.
   Then exec from rsyscall into whatever.

   OK, but why not be generic, though?

   Because then we have the issue of any fd we want bein' eligible for passin' along.

   What if I have some kinda
   mock thing

   I say let's translate these fds,
   then with that I can build the rsyscall task and other things,
   assuming that they don't actually access those fds...
   though I could dup them...

   or pass them down and close them up above....

   should this be a thread-level API???
   if I pass file descriptors down???

   OK so last quick question, where do we do do_cloexec?

   We do it after creating the rsyscall thread,
   but before returning it to the user?

   Sure

   and we call a function which disables do_translate
   finish_translation or something
   I guess that's the best we can do, we can't give them access to the new task and gate it behind finishing the translation at the same time.
   How do we enforce that they call finish_translation?
   Well, we couldn't enforce that for arbitrary code.
   But for our code, we want that to happen... reasonably soon.
   We can't gate the RsyscallTask behind it because that's taking away real powers, and doesn't help anything.
   What if we could gate the unshare behind it?

   Like, alright, if we want to always be safe,
   then we need there to never be a time when fds are duplicated

   And we also want to make sure users don't try to translate fds that are made after duplication.

   All this really suggests doing it up front,
   before unsharing.

   But if we do it up front,
   that has its own limitations.

   Bah! Screw it! User handles it! Fug!

   Argh ARGH

   So I'm just about prepared to embrace "every rsyscall task has a unix socketpair to its parent"

   But let's see.

   let's list the problems
** list the problems
   We need there to never be a time when fds are duplicated
   And also make sure users don't try to translate fds that are made after duplication
   And also avoid closing fds that have been opened in the new task and marked cloexec
   And deal with the fact that an fd could be closed after translation but before do cloexec

   OK ok so we should do it up front
   We'll take a list of FileDescriptors, and damn the typing issues.
** paths
   Paths probably should not have Tasks embedded into them, it is too confusing.

   "also, we need to figure out how to prevent non-leaf tasks from closing"
** threads
   OK so we can avoid threads hmmmmmmmmmm

   nah

   wait yeah

   otherwise we have to have all this crap just to make threads, BORING

   wait nah

   because otherwise we block others from progressing.

   hmmm

   with mkdtemp, at least, I need the..
   need the thing...

   since I don't wanna block others from running while...
   the rm runs...

   yeah.

   so let's get some kinda standard task,
   and let rip

   StandardTask or something which has all the resources inside it

   Or like, ProcessResources?
   HostResources?

   TaskResources? That would have the FD things and child processes
   ProcessResources would have the functions and libraries
   FilesystemResources would have the binaries and stuff

   I guess we can put this inside the Task?
   And unshare them appropriately when necessary?

   Hmm.
   What if we had a dictionary of resources inside
   FDNamespace, VMNamespace, MountNamespace?

   Since inside those namespaces you can access anything...
   No that would be awful

   StandardResources containing the aforementioned.
   And a Task.

   Maybe StandardTask then since it contains a Task.
   Or EnrichedTask.
   StandardTask, and I'll change the name later if necessary
** passing path parameters
   So I have two alternatives.

   Alternative 1: I use a contextmanager to set the fd to not cloexec, then set it back if there's an exception.
   Alternative 2: I build up a list of fds that I need to pass down as I serialize various objects.

   Alternative 1 is kind of like building an implicit list of 2

   Maybe I should just not even re-set cloexec

   Just do the mutation with no way back

   After all that's how most things work...

   setsid for example

   ON the other hand, serializing a path over a unix socket, for example, demands that I get a list of fds out.

   And one can't really serialize a setsid.

   Building a list seems better then.

   Hmm, serializing a path over a unix socket is tricky though

   Because we can't encode the same fd number in it.

   Hmm, and furthermore we can't rely on root dir or cwd.

   Really when serializing a path over a unix socket,

   we need to always open it as a dirfd and a path segment,
   and pass those along explicitly.

   Which is very different from how we can inherit paths!

   Having a different path for inheritance seems better than.

   So it's a bit weird though,
   what if we make a path into an argument and then chroot?

   Well don't do that then!
** what to do
   Once it reaches a certain level of maturity, I should use this inside TS to get more validation/testing.

   i.e. immediately upon finishing the basic stuff

   My goal: configuring/running nginx.
* criu
  I feel like I have a bunch in common with criu

  They are manipulating and preparing tasks as objects,
  and I'm doing the same thing
* unrelated thought
  dependency injection is autoresolving dependencies by type???
  that's... that's... that's implicit parameters :(((((
  looking up things by type :((((
  but I hate dependency injection!!!

  urgh, I've just had a terrible thought
  a typeclass can be modeled as an additional argument that is automatically looked in a table indexed by type, right?

  well it's just occurred to me that there's another practice that is like that,
  where an implementation of some functionality is looked up in a table indexed by type,
  common in a certain primitive language.

  and it's a practice that has reviled and disgusted me,
  so it's terrible to think that they could be the same

  how can you achieve coherence without global uniqueness of instances?
  just error if there's ambiguity or whatever
* multiplexing without depending on event-loop specifics
is single-prompt yield not powerful enough to do multiplexing without control over the prompt?

how about multi-prompt yield?

well, if I had a multiprompt yield.
then I could have a prompt for my specific multiplexer
and I'd want the user to be able to just call into me without themselves having to be within some handler

so they do it
but
oh even with a wrapping a function around the prompt to get the continuation, I only get the cont under the prompt, derp

a message passing perspective

yield to specific prompt (that's the cap they call on)
then it's good
so multi-prompt yield with first class prompts would be fine, assuming we could keep the handler on the stack.

maybe I could implement multi-prompt yield in python

it wouldn't be slow:
we'd have a separate stack, essentially, for each yield thing. which is fast.

and it would be in direct style.
when someone yields back up to us,
we'd see that as a response to a send into them.

and we'd be able to continue on and mess with them.

but, I feel like that must be too powerful for the simple use case of wanting to multiplex access to a single resource
(aka: taking requests in, submitting them to the resource, and sending the responses back out, possible in a different order)
I feel like there must be a simpler way to think about multiplexing access to a single resource than that..

well that sounds like message passing
which is equally powerful

so really that's just exactly what it is...
is it *exactly* what it is?

in a sense, yes.

is it REALLY what it is?

what other ways to multiplex are there

well exokernels have embedded code
i could poll at user level
i could have it wake up everything
maybe there's some relevance of passing down an fd...

if I have it wake up everything then I could handle it in user code.
but how would I wake up multiple things???
they'd all wait for one thing
and when it happens they return

ho hum hee

if wait_readable is my only primitive, can I do it?

I guess this is related to the whole question of,
what is the syscaller interface? how wide is it? etc

hmm, I just realized that I think this is essentially the same question as another question that I have previously deferred until later
(that other question being, "do I really need to expose wait_readable in my interface-to-native-system-calls? maybe there's something more naturally ~Linuxy~ I could expose")

hmm, actually it just occurred to me that this question is essentially the same as an earlier question I was pondering and deferred until later, so I guess I'll just defer this one too, and be trio-specific for now
(the earlier question was whether to expose wait_readable in the system call interface which abstracts over making system calls in the current thread vs. in another thread, or to expose some other primitive async mechanism, or do something else, or whatever)
** should we call PDEATHSIG in the trampoline, or with rsyscall?
   In favor of trampoline is that it will apply for all threads

   Against trampoline is that it will not reaaally work for everything

   If the parent dies, but there are still some threads alive in the address space,
   when we die, our stack won't be cleaned up.

   Well wait, that's an issue with rsyscall as well.

   No, no, we solve that by just making sure PDEATHSIG is set for all of our immediate children, right?
   Is that the plan?
   Yes, I think so.
   We don't want anything in our address space to survive our death, right...?

   Well, we could. But that would require careful resource management.
   So I think PDEATHSIG does work.

   Then we're back to the question of trampoline or rsyscall.

   What if we wanted to trampoline into something that should survive our death and be in our address space?
   Again, this would require careful resource management.

   Do we even want anything to be able to survive our death? I argue no!
   Or, more specifically, by default that shouldn't happen.

   Also, in the future we'll have an inheritable immutable PDEATHSIG.

   Although, we still want to daemonize things, don't we?
   How will we deal with that?

   I guess we'll have a PersistentChildTask,
   which is really a supervise (possibly watching multiple children)

   We'll have to unset PDEATHSIG for the supervise,
   as well as for the children.

   But, it will be in a separate address space anyway.

   Hmm hmm hmm.

   So yeah, how will we design PersistentChildTask?
   That is something tricky.
** PersistentChildTask
   Oh boy, this is a tricky one.

   Yep.
   Tricky.

   Okay time to start thinking about it.

   Could we just have a persistent rsyscall task which is not our child?

   That's perfect, yes, let's do that.

   That matches the form of remote tasks too, we can just reconnect to them.

   It's SLIGHTLY awkward that if we kill it, it murders everything on that host.

   But I guess we could have multiple of them on a host.

   It could be some kind of rsyscall server, where when you connect to this unix socket,
   it spins up a thread for you?
   But it persists forever while the unix socket lasts?

   hmm

   but we do want a specific thread to actually persist,
   don't we?

   We kind of need that, to preserve the child-parent relationship.

   We could become aware of SUBREAPER...

   Though that doesn't help, does it, we'd just reparent to some parent person,
   and want to preserve them,
   because if they go away, then we're ruined.

   We could use supervise to make the child-parent relationship irrelevant.

   Hmm.
   If an rsyscall task gets wedged by me sending a bad syscall,
   aren't we screwed?

   We can't recover.

   Wait wait wait, of course we can recover.
   EINTR is a thing... wait, will we not get EINTRs?

   Can we send it a signal that we've specially designated for interrupting bad syscalls?
   Yes, and we can get EINTRs.

   That seems distasteful.

   But it works.

   Okay, but, so, I'll either do persistent rsyscall tasks, or use supervise and a Unix-socket-serving thread-server.

   But in either case, don't I need to be able to start things that don't die?

   Also if I use supervise...
   Can it be a thread inside the thread-server, which I communicate with using an fd?

   And I'd probably do the same with the persistent rsyscall tasks, right?
   They'd just be threads in the server and I'd access them through a thread-server.

   Hmm.

   A thread-server, which implements the thread creation logic I've done in Python, in C.
   That seems fairly interesting as a concept.
   PROCSESES AND THREADS R FILES, DEAL W/ IT HATERS

   Okay but wait, it would be specialized to only work with rsyscall server threads.
   That's undesirable.

   Could we..
   could we just...
   send it a function address and some arguments...
   and get back a whole lotta nothing...

   Hmm yeah, like, how do we send it to the fds or memory or all that stuff that we might want to send it.
   I guess those are questions for our runtime to solve, not our kernel-interaction-thingy.

   Only dealing with rsyscall servers seems neat.

   Though! How do I get the data?

   Also, again, I guess I'm just considering something like,
   I have some threads, I get the fds to talk to them with...
   I send those fds *into the server*,
   so it persists.
   Is that how I'd make a persistent thread?
   But how would I do that?
   How would I send the fds in?
   Since I only have one side, and they have only the other side,
   how would I get the fds from one side to another.

   Also, again, the data fd.

   Oh wait, I guess the threads I create wouldn't be persistent.
   In fact I could even connect to the thread server from inside the server.
   Then it'd be okay.

   So then, data fd. How do I data fd.
   Hmm...
   So I can allocate memory without memory, using mmap.

   Then I can write into that memory one byte at a time with syscalls, though that's mega-gross.

   Or I can do the multiplexing trick, but that requires care.

   So I guess, uh

   Oh wait, screw connect, let's just use a protocol.
   We'll say, "hey gimme some fds BAE",
   and it'll send them over.

   Although that's tricky when done remotely.

   We could have an initial process that we get,
   which we then use to connect to the persistent process server.

   And we get that thread,
   and we send the fds back to our initial process over unix sockets.
   Seems good.

   Wait, wait, no, no.

   Then we have to go through two address spaces. Not three, which is nice, but still two.

   So we'd really like a direct connection to the persistent process server.

   Wait, can't we just do that?
   We pick ourselves up, send our fds over, and throw ourselves over, by our bootstraps.

   So we get our initial process. It comes with a free data fd, compliments of the house.
   It has access to the unix socket.
   We open that to get a thread.
   We send the fds for our initial process over the Unix socket.
   Including the datafd.
   We dup those fds over the one for our thread inside the thread-server.
   Now we have direct access to the thread-server.
   And we only have one data fd.

   So that seems fine and good, and at least that's possible.
   We'll probably think of something better later.
** so how do we actually run one of these
   We probably have an initial initial process, right?
   Which just runs vanilla rsyscall.
   And we bootstrap our way into a persistent thread-server from there.

   And how do we do that?
   Well...
   I guess we can exec our way in, assuming our initial process isn't forcibly terminated.

   But is that sustainable?
   What if we want to create another one?

   We won't have another task that is unparented/unfilicided.

   So, what do we do?

   Oh wait, I think it's fine, isn't it?
   Don't we just double-fork?
   Standard bidness?
   That's gross though, I don't want to double-fork.

   Hm, so.
   This is sounding like I'm in favor of doing it in rsyscall, as a first-class thing.

   And then I can choose to not do it, and instead exec something that will persist.

   Of course, the real issue is nesting persistence...
   What if one of the applications I start, wants to persist?

   But I don't want to persist?

   I guess I have to give them the ability to persist.
   I have to pass it down.

   How do I pass down the ability to persist?

   Well. I can allow a task to live forever by default,
   and remove that ability with a persistent pdeathsig thing.

   Meh, it's some kind of silent persistence, but I guess it's fine.

   Maybe I should have the inheritable PDEATHSIG thing partnered with a clonefd?
   And the inheritable PDEATHSIG doesn't kill things made with clonefd.
   That way you can make pseudo-persistent processes which live past a tree's death.

   How would that work?

   I guess maybe I can like,
   get an fd for some process,
   which unsets pdeathsig thingy for it??

   Designing this API is hard :(

   OK, so yeah, doing it in rsyscall in code is better.

   Alternatively, we could just do it by default.
   *Should* it be by default?

   How do we control this?
   How do we control persisting tasks?

   Well, the ideal way is by linear resources.
   When we go away, the resources we own (our children) go away as well.
   
   But, we can opt in to survive the death of our parent.
   
   But, if we want to guarantee our children die because they might maliciously do that.

   Hm, hm.

   So what if it was literally a capability that's passed down?

   Oh, what if it was some kind of process group fd?
   And when they're closed, everything inside is killed...
   But you can attach your process to them, and survive!

   Like normal process groups, they aren't recursive.
   You can only be in one.

   And so, if someone passes down a "survival" process group,
   you can enter it.

   Actually maybe it should be recursive,
   then you can make more fine grained divisions.
   You can spawn a pgroupfd off of another one,
   and it will be the child of that one.
   And, if the parent is closed, it will be forcibly closed?

   Well, maybe that's silly

   Okay, why would we want to be recursive?
   Why can't we just add ourselves to the survival group?

   We could, we could, we certainly could.

   Can we also solve the "library wants to run subprocess" problem this way?
   Well, we can have the library set no signal at clone time.
   Then what?

   Could we also also solve the "detect reparenting" problem?
   Say, because you stay in the process group forever?
   And maybe you can get notifications when anything in it dies, no matter the depth?
   No, that's a mistake

   But maybe there's a "top level" of the process group,
   and when your parent at the top level dies,
   you go to the top level?

   And you can get notifications for things at the top level dying
   Nah that's complex

   Oh wait this doesn't work
   Self-reference, as usual, foils it.

   So that would imply we do need recursion.

   That's interesting.
   If things have owners, self-reference is a problem.
   But if owernship is a stack, it's no problem.
   That's like linear regions etc

   Anyway.

   I think the default would be death in any of these schemes.
   So I think it makes sense to default PDEATHSIG on.
   Turning it off is a rare operation anyway.

   Yeah and let's do it in rsyscall main, eh?
   umm

   oh wait, heh, futex waker isn't gonna wake up and die when rsyscall dies...
   no wait yes it will...
   let's put it in rsyscall main for now.

   naw, in that case, let's do it at startup.
   no wait, what if we die before that?
   we have to set pdeathsig as soon as possible to avoid this race of death...

   no wait, it's fine, there's no race, we can conditionally do it

   argh gotta fix up rsyscall trampoline or something

   blah, let's just put it in rsyscall and whatever
* cool feature: context manager container
  I can start a container with a contextmanager and spin it off by returning.

  hey, and supervise is a nice pid1 to run inside the container

  neat neat neat neaaat
* standardtask with my own additions
  I could have my own additions-thingy inherit from Task...

  And have it be stashed inside Paths and things...

  That's not particularly type-safe eh
* things to improve
** TODO fix the race with PDEATHSIG so that a thread dies even if we die before it sets that
   I guess we can... know the pid of our parent (have it passed down),
   set pdeathsig,
   getppid,
   if ppid isn't what we expect, suicide.
** TODO use AsyncExitStack heavily to manage resources!
   When we pass in a resource to something,
   we need to immediately put it on our AsyncExitStack.

   Then when we finish successfully, we can cancel some of the things off of our AsyncExitStack.

* portability things
** assuming there are 64 signals so sigset_t is an 8-byte integer
   I think you can probably compile the kernel with more signals?
   But maybe not because it looks like it's hardcoded in a header somewhere.
** rsyscall assembly code
** building a stack manually to pass to rsyscall trampoline
** architecture-specific syscalls
   clone at least is arch-specific in its argument order
** using /proc for bind and connect
   Because there's no bindat and connectat.

   This is an awkward blemish
* desirable kernel features
** notification of exec (more specifically: mm_release) through wait
   This way we can detect when a child task/thread has called exec,
   and free any resources they had.
   Otherwise there's no way to know for sure that they're done execing.

   This is like CLONE_CHILD_CLEARTID,
   but that only has a futex interface,
   which is not suitable for an event loop.

   Currently I'm just starting one thread to monitor each futex,
   which duplicates the number of threads I run.

   ptrace doesn't work because ptrace sucks and break gdb/strace.

   Passing down one end of a cloexec pipe doesn't work because if we're a CLONE_FILES thread calling exec,
   the end will still be open in other threads.

   Using futexes from a normal event loop sucks.

   vfork is kind of similar but sucks.

   I think this would allow an asynchronous posix_spawn.

   Currently posix_spawn *must* take over the thread to use vfork.
   But if you want to spawn another thread to do it...

   ugh then I guess you can just do that, spawn another thread and then posix_spawn.

   Oh but that messes up your child relationship! So yeah, you can't do that!
** a CLONE_DO_CLOEXEC flag to pass to clone, to close CLOEXEC fds
   This would be good and useful.

   Though we can do it in userspace, it's ugly and requires creating an extra thread for full correctness.

   And it would be simple.
   Just need to call do_close_on_exec.

   oh god, clone really is out of flags
** sigkill-resilient filicide
   Maybe, like, the combination of supervise and PDEATHSIG?

   Except, we'd want like,
   CHILDDEATHSIG...
   a signal to send to your children when you die.

   If we want to combine this with a better proc interface,
   we could maybe have some kind of childgroupfd,
   which when closed, kills all the children in that group.

   We can't realistically enforce things the same way capsicum does,
   which is to ban usage of non-forkfd process creation.
   Also I don't think that's robust to loops anyway.

   Oh, the same is true for the childgroupfd then.
   Well, no, they can create childgroupfd within them, but,
   they're still contained in the childgroupfd.

   Oh! Let's just have an inheritable PDEATHSIG which can't be unset!
   That's easier conceptually.
   Though, reparenting will still happen before the actual signalling, which is awkward.

   Maybe I should have the inheritable PDEATHSIG thing partnered with a clonefd?
   And the inheritable PDEATHSIG doesn't kill things made with clonefd.
   That way you can make pseudo-persistent processes which live past a tree's death.

   There are several things we want:
   - it should work to kill legacy processes that don't use this API
   - there should be a means for processes to still daemonize if we want to allow that

   A childgroupfd does actually meet these.
   If we have a childgroupfd,
   all the children in it can be killed when we close it,
   but not if they are inside their own childgroupfd.

   Like process groups, but as a file descriptor?
   Hmm, that seems like a nice idea.

   So you can escape the process group easily, is the thing?
   Maybe we have a flag, "escapable" or not.

   Of course, escaping will just require that something has a reference to its own process group fd.
   oh, capsicum doesn't support passing process fds
** remove old confusing comment from dequeue_signal
   in signal.c

   It just wasn't removed in b8fceee17a310f189188599a8fa5e9beaff57eb0 when it should have been
** bindat, connectat
   Would be nice to be able to pass a dirfd for these. And also lift the length limit.
** some way to deal with fd inheritance?
   CLOUNSHARE?
** the ability to take existing memory and wrap it up in a memfd
   This would be useful to allow a uniform interface for operations.

   Instead of providing both an interface for operating on memory,
   and an interface for operating on file descriptors,
   we could just have a single interface that operates on file descriptors.

   Anyone who wants to operate on memory has to wrap it up in a memfd first.
** clone_child_settid doesn't seem to work without clone_vm
   i.e. it doesn't work when the child is in a different address space
** exec needs to take a list of fds
   fds that it should pass down through exec.

   Even if they are cloexec.

   Then cloexec can be inherited.

   This allows us to use execveat on scripts and things.

   We just put the fd in the pass-down list.

   Of equal relevance, this also allows shared-fd-space tasks to exec and pass down fds.
   (otherwise they can't remove cloexec because it's racy)

   Hey, and exec has a remaining argument left!
   We could add it!
** make connect(Address), accept(FileDescriptor) symmetric
   i.e. so it's connect(FileDescriptor) and accept(FileDescriptor)
** unify connect/accept with passing file descriptors
   This would be a nice design...

   Even better would be to unify it also with openat.

   Then there'd be only one ways to get a file descriptor:
   Use openat on another file descriptor.

   (well, and also by passing it in)
** being able to pass a NULL name to memfd_create
   Requiring memory to allocate memory makes it impossible to bootstrap memory allocate using memfd_create.
** add a MAP_DONT_ROUND flag for mmap to not round the length and offset to page size
   Currently we will round them to the page size,
   and the page size depends on the arguments and what kind of file descriptor we're mapping.

   But because mmap rounds the length up,
   munmap on the resulting mapping won't work with the mapping we pass in.

   We'd rather fail to mmap,
   than get a mapping that unexpectedly can't be munmap'd.

   If we get a file descriptor passed to us from someone else,
   that file descriptor might have a larger page size than we expect,
   so if we mmap that file descriptor it will unexpectedly silently round up,
   and then munmap won't work.

   Plus we probably also want an fcntl to get the page size.

   Ah, and also this flag should probably require that we pass MAP_HUGETLB and the right size bits
   if we're mapping a hugetlb file descriptor.
   That way we won't accidentally pass the correct length?
   I guess if we accidentally pass the correct length then it's fine.

   Still probably should require the right size bits.
* possible kernel bugs
** MMAP_GROWSDOWN doesn't seem to work
   It doesn't grow.
** CLONE_CHILD_CLEARTID doesn't seem to work if you don't pass CLONE_VM
   Even if the address is in shared memory (mapped with MAP_SHARED, same inode and everything in both processes),
   it doesn't seem to do a futex wakeup.

   Didn't yet check whether the address is cleared or not - probably not.
** (possibly creates userspace bugs) munmap works differently on hugetlb files
   This means that if I control the file that some userspace program mmaps,
   I can ensure that their later munmap will fail.

   That seems at least good enough for a denial of service,
   and possibly could even cause security problems.
* design decisions
** TODO should we grab the Task out of other objects, or pass it explicitly?
   Consider RsyscallTask.make as an example.
   It takes a ChildTaskMonitor and an Epoller as arguments,
   both of which contain a Task.
   Should it also explicitly take a Task,
   or just use one of theirs?

   Also we can grab the Epoller out of the ChildTaskMonitor, for that matter.
* knowledge
** tkill vs kill
The difference between kill and tkill (besides the special negative parameters to kill, which tkill
doesn't support) is that kill always delivers the signal into the shared SIGHAND table, whereas tkill
delivers the signal into the task-private (thread-private) table.  Note that pthread_kill, raise, and
other such things work like tkill (though they actually use tgkill).
* game
  non player processes

  file descriptors are inventory

  namespaces are rooms

  avoid metaphor: namespaces aren't rooms, they're just what exists in this world
  essentially we'd build a rich shell for exploring Linux things
  (the python interpreter)
  but also have a text advanture like simple translation layer
  which translates simple commands into obvious 

  sockets are telephones?
  you can pick one up and it makes a copy I guess

  I can use this to demonstrate the essential weakness of the concept of,
  just putting an internet socket out there where anyone can dial it if they have the right number,
  by having a call verb that allows you to call people

  and have some kind of task where you want to provide a service,
  and want to avoid some prank callers from calling you.
* what is this? explanation to normies
it has the intent of providing an interface to Linux tasks as first-class internal entities,
that can be manipulated by running syscalls inside of them
** amoeba
it's kind of going back to python's roots and being a distributed scripting language :)
** make low level functionality generally available
   there are things that are easy at a low level, but become hard with layers upon layers of abstraction.

   let's make an abstraction that is much lower level and much thinner than usual,
   to undo those hardnesses that are created by abstractions.

   maybe abstraction is a bad word for this.
** name
   rsc is russ cox's username :(

   rsys is better
** it's a distributed thread library
** it's a library for building distributed systems
* explicitly modeling memory
how would I do this?

why do I want to do this?

so I need a syscall interface which is indifferent to memory,
I guess,
for most functionality.

or, no.
i'll only implement some few things as indifferent to memory?

I can implement

like, child monitoring, I guess

so ultimately what I want to do is read a file

and the most efficient way to do that is to run cat on the opened file,
connected to a TCP connection to the hub

but, when the opened file is on localhost

then, that is not necessary.

so we only need to bring it to localhost if we are actually gonna look at the bytes

read is not the way to get the bytes in a file.

we need some higher-level thing.

we want to bring it to the local task.

which makes local memory rather special

or rather, memory which is under language control

okay well I could deal completely with opaque memory

and then have a special case inside the library to copy/convert local opaque memory to language control

then I can have Path library that,
supports read_text,
and,
is aware of the distinction between localhost and remote host?

yeah...
one specific task has the attribute that,
its address space is shared with my address space.

(or multiple tasks like that really)

or really actually,
a task is in an address space,
so it has the AddressSpace object in a field
and there's a global local_address_space,
which we can compare against the task.address_space with "is",
to see if we can directly access memory associated with that task...

so that's good for the in-local-task or in-address-space-task.

but what about same-kernel, different address space?

socketpair

need to handle it anyway with data socket
ehhh

what do we put in standardtask?

standardtask can, i guess, be an abstraction?

no, it will be real

but then how will we achieve memory-using syscalls?

ah, we'll have standardtask hold an abstraction,
MemoryManipulator

which might be shared between others in the same address space, maybe.
or might point to a dedicated thread for the purpose of manipulating memory.

but in any case it will provide the memory-abstracted syscall interface we want, right?

maybe, or maybe not

we need a really true syscall interface

that presents all the details of memory

then on top of it we can build SyscallInterface
which is memory-abstract.

I guess StandardTask will also hold a MemoryAllocator,
which doesn't need to be abstract.

that's what users and classes can use,
to get memory in the remote task,
so they can make system calls that need memory and stuff.
it's a nice simple dynamic allocator.

then I guess the Path thing itself,
for reading a file to bytes...

cases on whether the Task is local or not, I guess

if it isn't local,
it uses the cat-to-datafd trick.

if it is local,
it just reads it directly.

this is an operation on a file descriptor, I guess
and it works in batches
and, is probably async?

now wait a second, how does the Path get the datafd
how does it manage to get a connection

well,
hm

a StandardTask, I guess, will provide a means to create a new pipe between,
the task inside the StandardTask,
and,
a closer task???

probably through a new TCP connection, at least inside TS where we have a trusted network.

so how do we read a fd to bytes?

so we have to have a reference to the local task, of course.
and a reference to the remote task.
and then, the ability to create a pipe between local and remote???

which possibly requires a route of tasks between local and remote

so then to read an fd to bytes

we create a pipe between the local task
and the remote task
and cat the fd to that pipe and read it in the local task

so in fact, in general,
this is a way to transfer data from,
one task to another.

so to read an fd to memory in another task,
we can do this thing.

we provide the memory

or we provide the fd to write out to
(if we provide an fd, we directly cat into it, giving the remote pipe in as cat's stdin and the fd out as stdout)
(like splice between two hosts)

and so the wrapper around this is just then,
to read something to bytes we first allocate memory,
then do a read from a remote fd to bytes in a different place)

and hm
if we're not going between tasks at all,
we can just read directly.

so this doesn't require special casing local memory nearly at all,
just in the very first step when we allocate memory,

this is nicely abstract

could we reduce it even more?
just have memory? or just have an fd, even better.

use a memfd if you want to read it in memory...

it would be nice if we could take some existing memory and wrap it up in a memfd.

that would be cool
we should not bake that in, though

so okay, this is the foundation, right?
we can take memory or file descriptors from any task,
and splice/copy it on to memory from any other task.

this is how our memory manipulator will work

we actually might not even have read/write operations on memory

instead all reading/writing has to happen directly locally.
cuz, duh, we can't actually read/write from remote stuff.

different thing: we can always run the read/write system calls, on task-local memory and fds.

so when we're reading a file, what will we do?

I guess we'll go through the generic interface

we'll say,
here's some memory in the local task so we can get some bytes
here's an fd in some whatever task
then we pass those two to rsplice
and it does the efficient thing to copy from fd to memory.

what about, also, when we can just directly use local memory?
that is to say:
when we're making syscalls in the local space,
we can build the args with bytes,
then directly pass those bytes to the syscalls.

so really that's another thing I guess?
appearify: mem -> task -> mem
which takes some memory, and a task
and returns some memory in the task

naturally,
this is a method on the memory manipulator or memory allocator thing
since it might need to allocate?
but also might not?

and we have our bytes,
and we turn it to a memory object,
and pass it in,
and possibly it is used.

it's nice that bytes are immutable :)

hmm that's not good though, hackers, that's not good

what if i need rw memory out
like what if it's an in-out parameter

and what about ownership?
do i lose ownership of the bytes once I pass them in?
that can't be right

oh i think i should just build any arguments in memory allocated
by the allocator

hmm but then i have read/write methods on remote memory, not good

so two alternatives:
- build directly in memory allocated by MemoryAllocator.
  This requires read/write methods on the remote memory.
- build in local bytes, allocate memory and copy to remote task if necessary
  This is inefficient in the local case where we might be able to build directly in memory.
  Hm.

so I think we should have

some kind of allocate and fill thing

where the memory allocater takes a bytes
and returns memory containing that bytes

and ownership of the bytes is lost,
since you see, bytes are immutable

and it has a with-context manager

and it takes a writable=True parameter,
so that if the memory is read-only

then the bytes can be used

and if the memory is writable,
then a copy is fine.

also a bytebuffer would be maybe okay

but maybe not,
who knows whatever

essentially split out all the logic into individual classes

have only the primitive interface centrally
seems good!

hmm, direct access to the channel would be better though

but sharing a TCP connection is hard - how can we ensure that previous users have cleared the buffer?
and it's too expensive to create a new connection on each usage, probably
a TCP handshake on each syscall is probably too expensive

although
maybe we can just
allocate a TCP connection for each object?

we are probably making a TCP connection for each file we read...
and a cat process...

so we could easily afford one for each object that wants to do syscalls
but I don't think so, no, that's a bad idea
we can, instead, just share the connection! abstracted slightly
yeah, it's fine to just abstract it.

in any case, explicitly modelling memory is definitely a good idea:
at the moment, if we wanted to proxy between two file descriptors in the same task,
and we didn't want to use splice for whatever reason,
all the data would have to hop through the hub.
gross!
we should be able to read into remote memory and write that remote memory back out,
without pulling the data into the hub.
hub is a bad word, I should say "runtime" maybe, or "language"

ok so we have a simple SyscallInterface that just has,
the method,
"syscall"
* rsplice
  I guess we can grant the ability to,
  for any "bytes",
  get a Pointer and Task and stuff for that bytes,
  in the local address space.

  and we'll have a gateway class thing

  which is the gateway between here and there

  and supports splicing things

  and it's an abstract interface

  and the awkwardness of this interface will discourage working with data

  what to call it?

  gateway?
  rsplicer?

  HomePortal

  rdma...?

  Gateway.

  And it'll be in standardtask?

  Or in task?

  but it certainly requires async processing stuff...

  how can we put it in Task?

  we can't.

  oh but we must?

  no, we only need to,
  if we want to have nice syscall wrappers.

  memory gateway, hm.

  memory is just an actor that we send messages to, y'know

  baking it into task is really necessary tho right

  well no, it's technically possible to get along with just syscall

  we could bootstrap our way up.

  hmmmmmmm MMMMm

  if I don't put it into Task,
  I'll need StandardTask for pretty much everything

  So yeah, stick it in task.

  maybe merge it with syscallinterface
  maybe not

  ok so now I need to think about how to handle this split thing
  where two things want to write to memory,
  but the write isn't needed until the latter is done?
* pipelining/mobile code
** muse 1
mobile code hmm

i mean, for any specific function,
we can always stick a function into the library to handle it.
** muse 2
  i'm tempted to bring this all to its ultimate conclusion
  by just sending over snippets of assembly to execute

  they can have the syscall args embedded into them
  and do "syscall"...

  then we'll return back, what?
  the register state?
  eax?
  nothing? force it to do its own return?

  meh we'll stick to just syscalls for now,
  it's nicely constrained to force heavy use of Linux stuff
** muse 3
this explicit modeling of all these things
like memory and such
makes mobile code pretty viable y'know

unusually viable

that's a later step if we choose to do it

we can easily reimplement syscallinterface and gateway on top
** tail calling
   we could even have the mobile-code-receiver be entirely tail-called

   as in, it has no main loop,
   we're just require to send over the right mobile code,
   which has a tail section which calls wait_for_more_code

   we can even be stackless
** explicitly managing resources
explicitly managing resources on the remote side is so much better

instead of sending a message which causes some mysterious amount of resources to be used,
instead just have some code which you control which runs on the remote side,
and which you communicate with using some internal protocol,
and if peple want to talk with you they go through that code, I guesso
* logging
maybe we should have a per-task logger?

or better yet: a per syscall-interface logger?

then we call getlogger on it to get the right logger.

well what is it I really want?
I want to be able to know which task a syscall is coming from.

I don't actually care about customizing the level based on each one

I guess a logger per syscallinterface seems fine.
* small unix socket paths
  What would I do eternally?

  So I want to be able to connect to an arbitrary Unix socket path.

  hm.

  Why don't I just open the directory in the connector,
  and then connect to the path?

  That's not fully generic, but it comes close.

  Yeah okay so, the path is limited in size...
  but we can do it by directory...

  but then the final component still is a maximum of 108 bytes...

  which we can work around by... making a symlink, I guess?

  oh, hm, can we do this with an O_PATH socket?

  that would be neat... :)

  WOAH IT WORKS!

  ZOMG

  this is transformative

  so you can connect to a socket

  through an O_PATH descriptor

  this is great

  oh and it makes sense, since the /proc/self/fd entry is a symlink to the real socket.

  okay so I guess what we'll do is,
  we'll pass down the O_PATH descriptor?
  hmm, maybe not
  I think we should probably just pass down the path and have the user open it.

  on the other hand, hmmm..

  what happens if I O_PATH open something under a dirfd

  it works! omg!

  so this handles the connecting side.

  but now what about the listening side? does this let us listen on sockets with components more than 108 characters?
  
  in other words what happens when I listen on a symlink?

  i guess we can find out but

  i am not sure this whole thing actually works in different filesystem namespaces!!

  in fact I think it's likely that it doesn't...

  oho of course we can't bind through a symlink

  we can't O_PATH open something that doesn't exist yet!

  we can use mknod to create a socket, I guess.
  then the socket exists, but can we bind to it?

  wait wait, can we use a symlink to do the binding?

  and how does this even look when we call getsockname?
  will we get a huge (>108 bytes) address back and have to find size by strlen???

  what??? I guess we get the dirfd-relative path when we call sockname or peername?
  even on the remote side??

  well no on the OH hm

  what???
  we do???

  this is confusing...

  okay so let's try binding on a symlink

  ohhh it fails because the symlink already exists!

  hmmm

  what if I bind the socket then rename it?

  yes that seems wise.

  okay!
  so for connecting:
  open O_PATH the socket then connect to /proc/self/fd/pathfdnum

  for binding/listening:
  open (O_PATH or O_DIRECTORY) the directory you want to bind in,
  then bind on /proc/self/fd/pathfdnum/whatever,
  then rename whatever to the real name.

  Well, here's one issue:
  What if whatever is taken as a name?

  I guess we are just generating a temp name.
  Hmm, tricky.

  Also we only do this if the name is long I guess?

  Otherwise we bind directly.

  so yeah I guess getpeername is just whatever the original name was.
  lol

  so, at least I've discovered this!
* stuff

  write a shell with rsyscall

  some other thing what was it
  some explanation of what rsyscall is

  meh
  it was nothing original

  it was I guess the notion of a language based system?
  
* another advantage
  Don't hardcode position of fds in fd space!

  That's bad!

  It prevents you from using other things such as a debugger!

* unshared memory
  ok so I guess the first step is to do this with a thread

  rather than execing into it

  hmm or should we actually exec into a separate process?

  after all what's the point in forking really...

  it's dumb!

  copying your address space? pointless!

  primarily because all the other threads, and the runtime, don't get cleaned up...

  so yeah let's go with execing

  alternatively, that's a bit of a headache, so let's not

  okay yeah so we have to pass down the stuff

  ah! I need to figure out how to avoid blocking the rest of the program

  I guess having a single linear Task,
  which I pass in and pass out whenever I'm gonna block,
  is the way to do it ideally.

  And what abstraction would we build around this?

  I guess we'd have some thing that we can ask to borrow the task,
  so we do the blocking internally?

  It's better for that blocking to happen on the actual task.
  i.e. we should be blocking on a response coming back;
  the requests should all be pipelined

  hmm.

  if we have two epollers both with the activity_fd subscribed, we'll get into a spinning situation.

  the first one will wait, then the second will wait, which will wake up the first one,
  and the first one will send a new wait request through,
  which will wake up the second,
  and the first will send a new wait request through,
  and we'll just be spinning! ugly.

  okay so how do we handle avoiding the spinning with epollers?
  wait, hm.
  what if it's edge-triggered?
  suppose it's edge-triggered.

  oh wait, but.

  we can't have a loop in epoll.
  we can't have an epollfd which is our "do we have stuff to do",
  which points to another epoll which is their "do we have stuff to do",
  which points back at us.

  could we do that nicely if, hum.
  if we, um.
  if we were edge-triggered?

  hmm, loops are possible, but loops are always unnatural I think.

  so we want something that references "the rest of the world".

  and we want that something to also reference us? nonsense!

  we are the only thing that is!

  so when we're us, we are, um, hum herm.

  I should read more composable concurrency stuff.

  ughhh that's a drag

  ok so if i don't read more concurrency stuff.

  how would I just do this in Unix?

  I have a good ole fashioned C library.

  It performs some function and exposes an fd.

  I include that fd in my event loop.

  This way I pump its work loop whenever it needs something.
  So it can maintain its service.

  What if there's some function that completes asynchronously?

  well I guess I can just,
  pass down a,

  callback thing.

  yes and mhm

  I pass down a callback

  and eventually it gets called

  and okay I guess it um

  it just y'know

  oh I pass down a callback,

  as well as a rest of the program thing to do immediately,
  and return up and continue running,

  and eventually I pump its work loop with an epoll

  and, yeah.

  but the question is.

  what if I'm a-wanting to do some things in the function?

  and...

  okay so...


  I want to do an operation that is atomic, aka a syscall.

  It's a blocking function.

  How do I do that?

  I just don't, I guess.

  I have some epollfd and I just add to that and assume someone will call it.

  So yeah okay so that's the thing.

  Someone can't pass in to me, and have me block for them.

  Okay I was thinking about this before yeah

  Either we take in an activity fd,
  and we block on our own events plus that,
  or we take in a blocking fd to add our events to,
  and we just return.

  Um, that's weird.

  So either we take an object on which we can register events and supply callbacks,
  or we take in the event representing the rest of the program and do our own blocking.

  Oh so no I guess a CML way would be,
  we bundle together our own events into a single event and return it.
  Yeah, yeah.

  So either we return a single event representing us,
  or we take in an event representing the rest of the program.

  bah let's just have it work by accident.
** proglangdesign question
  Hey #proglangdesign

  So, consider a program that does only minor computation,
  and spends all its time blocked, waiting for any of a bunch of things to happen.

  There are lots of languages and idioms where (put maximally vaguely) if there's such a program A, and a component B of that program,
  and A calls into B, and B wants to perform some blocking operation,
  then B (at some level of abstraction) will send to the rest of the program A some event thing that A can run/monitor.

  For example, B might return a future, or might register a pair of (event description, callback) and return nothing,
  or might be a coroutine that yields up some event description to some central event loop handler thingy,
  or more esoteric things.

  Essentially, B reifies the blocking operation and the subsequent operations it wants to do,
  and gives that to A to run at A's leisure.

  But what about the inverse operation?
  If B wants to perform some blocking operation and doesn't want to block the rest of the program,
  why doesn't *A* reify all the blocking operations that A wants to do,
  and send it in to B?

  Essentially, why can't we reify the blocking operation and subsequent operations that A wants to do,
  and give that to B to run at B's leisure?
** monads
   kinda like monads I guess

   or I guess like objects sending messages? no...

   I mean, ultimately I want to avoid having to do the conventional thing,
   (but which I've recently convinced myself is weird)
   where I'm in some deep library function and I make a call up to do blocking.
   I think that deeply requires weird stateful stuff.

   Instead I'd rather be able to block directly, and wherever I am, always have the rest of the program be calling down into me to do blocking.
* memory again
  OK so I used the memorygateway agressively.

  Now let's evaluate whether it's necessary :)

  So if I just had a "pipe" which allowed me to move memory from one place to another.
  That would be fine and good wouldn't it?

  talking about memory as memory allows me to, um
  well I can transparently use RDMA stuff y'know

  if I specify,
  hey here's the two sides.
  here's the memcpy.

  well yeah the pipe is really essentially this gateway thing.

  should it be unidirectional?

  yes. wait no.

  HMM I think we want read and write and all those things to not be memory-abstracted.

  That way we can easily do PeerMemoryGateway.

  I guess read can just be a free helper function.

  Yes and it can take a memory allocator thing and allocate some memory
  and write it into there.

  I guess. But then what about easily, y'know, reading from a file descriptor and getting those bytes into the runtime?

  Inspecting those bytes is tricky yeah.

  But I need the ability to read and write arbitrary bytes to even do the basics of AsyncFD stuff.

  Y'know I feel like I should probably extract some of those fundamental components into helper libraries/threads/things.

  Though maybe not, I should prove the concept of doing it all through syscalls.

  Keep both implementations and test equivalence?

  Anyway, so I allocate memory and copy into it with a helper.
  Seems fine and good.

  Then we need some help to get it into the runtime.

  I guess that's really more of a, hm.

  We have some kind of connection from the host, to the runtime.
  And, yeah.

  Well, I guess we maybe could have a big Runtime object,
  which has connections to all the hosts?

  Hmm no I guess we need the connection to the individual host.

  And we'll do a read() method on an individual file descriptor.

  Yes that makes sense.

  That is to say, that kind of reading is in the io level,
  not in the base level?

  How will Path work then?

  Path will rely on this connection, I guess, since we maintain the data in the runtime.

  Or will it?

  I guess we could go all the way, and have Path be only dirfds.

  But that's just as slow as maintaining the path pointer remotely.

  So if we maintain this data in the runtime.

  And send it over on demand.

  Then, that's all good.


  HMMM wait a second so, hm.
  Ah yeah so our Path class is correct, right, it correctly operates on Paths.
  And we could indeed replace it with a dirfd-only thing.

  I guess operations would still require memory access to actually be performed.

  So we are not really improving anything if we want to allow easily accessing arbitrary filenames.

  So Path, because it needs to work on arbitrary filenames which are stored in the runtime, needs to absolutely rely on the memory gateway.

  Well.

  We could have the dirfd thing,
  and have it operate on pointers.

  We could, I guess, have dirfds, as well as cwd and root things.
  And have them indeed operate on pointers.

  I suppose I could slide this API in under the current API.

  Should this be the same as base or separate from base?

  I think separate. We can combine later if necessary.

  Basically, a memory-access-free task and abstraction layer.

  ya okay I guess
* object capability design question
  It is a central precept of capabilities to not separate naming from authority.

  But, what about a situation where I can precisely name an object,
  but there are multiple possible (equivalent in power) authorities that I could use to access it?

  Hmm okay so they're suggesting just only exposing io.Pointer and supporting an equality between io.Pointers to see that they are the same.

  We could, I guess, only carry around the SyscallInterface along with the io.Pointer.
  But wait, we do need to be able to do the dynamic check on the task to check its authority.

  Well, hmm, I guess we can couple the io.Pointer with a MemoryGateway actually, not a task.
  Well, a MemoryGateway wraps a Task.

  An FD is the thing that we'll couple with a Task.
  And the Task I guess has all the namespaces on it?
* epoll notes
    # first, let's have them both be externally driven, I suppose
    # no wait, what are we going to do testwise?
    # so we have them each registered as epoll on the other one.
    # we'll call wait on the pipe_rfd on both of them.
    # then we'll activate some... stuff...
    # how do we call wait on the pipe_rfd on both of them, if wait blocks??
    # we can't, I guess.
    # before blocking, I guess we need to ensure that no other tasks are ready to run.
    # that's tricky, so let's go with external blocking for now.
    # okay so this is hard, very hard, we will probably not be able to run things until everything is blocked, so...
    # so...
    # how exactly do we ensure that everything that can run, has run?
    # well it's a matter of doing all pending work that we control
    # and only then blocking
    # blocking surrenders control back elsewhere...
    # but if we run a function for someone else...
    # and they call back into us...
    # well actually that only will happen if we have a level-triggered approach.
    # also... don't we need to do something level triggered then?
    # I guess we'll, um...
    # when we do the call into some other guy, they'll do just the, um...
    # just the nonblocking approach.
    # they won't block.
    # we'll block.
    # but no we won't block either.
    # well then when will we block?
    # okay so but yeah.
    # when we do an external blocking for some other guy,
    # that is, internal blocking from our perspective,
    # external blocking for them,
    # then,
    # they need to not block.
    # so yeah...
    # but then alternatively, we can say, let's externalize our blocking to this other guy,
    # and they'll call us when we're good.
    # I mean...
    # proposal is to do blocking internally, when exactly???
    # like, I guess we really do have two modes?
    # one external-blocking always, one internal-blocking always?
    # an internal-blocking guy can monitor for an external-blocking guy...
    # but here's the issue, how can we handle doing internal-blocking in any of our stuff??
    # after all, when we do an internal-block,
    # well, we can't be sure that other events aren't going to appear
    # well, except when we can in fact be sure?????
    # like when we are the only thing in the world,
    # we call our thing,
    # and any new action has to activate us?
    # blaaaaaaaah!

* segmentation
  lol this is a lot memory segmentation stuff
  
  basically I have far pointers in terms of memory segmentation

  well, there's an additional thing beyond memory segmentation:
  I have to perform the access through a specific resource.

  also the segments are not just numbers, you have to use an object to set your segment to something.

  model segment registers with dynamic scope
  I think that would be a really cool mapping

  so then how do you ensure your accesses are correct?

  not sure

  but you could also manage the current, um
  the current filesystem namespace with it, yeah

  yeah, you'd just have, um, setting that dynamic scope as,
  an expensive operation, I guess,
  but nevertheless an operation,
  and you basically can assign the current namespace to a thing.

  so, yeah.

  but now I just need to understand how dynamic scope can be associated with an object.

  I took notes about this before.

  Well, wait, reverting back to the previous state is expensive too.
  It requires you to save a thing.

  So... maybe we'd just want to be mutable.
  Urgh. So okay, let's just be mutable.

  We have the Task, and it has some mutable attributes.
  And that's it!

  OK so FarFD is fine.

  The fact that threads are first class objects is weird.
  But cool.

  Let's do the Task thing now.

  meh there's still the weirdness of...

  given a far pointer, I still need an additional thing to access it. (a thread)

  I wonder if there's a "physical" (hardware) metaphor for that too?

  afaik, far pointer systems never had to allocate and identify the,
  like, bus lane that they were accessing memory through :)
  
  that would be cool though.
  would that correspond to what I'm talking about?

  maybe:
  if the segment physically exists,
  and we set up a bus lane so that it's actually pointing at that segment and pointing at me,

  then that's kinda the same?

  we still want the far pointers on our side.

  hmmm.

  oh, the fs and gs!

  effectively you just,
  you sent up all the segments right,
  and then you use them!

  hmm what if we don't have far pointers, we just have pointers used with a specific task?

  that would be less realistic, true

  but wait, we could support join on them to get equality.

  hmmmmmmmmmmm

  that's strictly less functional though...
  because it doesn't invalidate pointers used before.

  paths used before are not invalidated,
  nor are other things.

  but maybe that's good?
  no that is not good, the paths are not in general valid after a change.

  this is kind of the issue with moving a thread between address spaces,
  because the instruction pointer might need to be invalidated.

  and, yeah... the fact that the we have some specific channels we can do instructions through,
  and they are each mutable with respect to which segment they use...

  it's a lot like us...

  which is useful, since I could, say, set up FS and GS at the start of some block to point to specific segments,
  and then use far pointers within that block by just passing the correct segment override prefix.
  but because this is stateful, how do I make sure that I don't mix this up,
  and use the wrong segment override prefix with the wrong far pointer?
  or what if I have some user-provided far pointer and I want to use it,
  and the user *should* be providing a far pointer whose segment matchesr

  it seems like the best I can do is a dynamic check at the time that I use so-and-so far pointer,

  how do I make sure that I use the segment override prefix with the right addresses?

  so were there ever any idioms for managing those?
  
  also! what about idioms for managing the current state of the extra segment registers?
  of course SS and CS are constant most of the time, but {E,F,G}S could, if I understand right,
  be arbitrarily pointed at new things

  which is useful, since if, say, the user provides a bunch of far pointers in the same to me,
  and I want to do a bunch of operations on them,
  I can just set up FS correctly at the start and just use the segment override prefix.
  but how do I make sure that all those far pointers are in the same segment?
  seems like I need to do a dynamic check.

  but, hm.
  if they provided me a bunch of near pointers,
  and a segment.

  that would be type-safe I guess...

  well, what would that correspond to?

  i guess, they provide me with a bunch of raw addresses,
  and a syscallinterface.

  that would be type-safe.

  so that's actually more like,
  they provide me a bunch of near pointers and a segment override prefix.

  so yeah, what if the user provides a far pointer to me,
  and helpfully they claim they've already set up one of the segment registers to match that segment,
  which is good because they're slow to change.
  so how do I validate that?

  well they should provide near pointers to me I guess.

  what if we bundle a far pointer and a segment register override prefix?

  so imagine that we have more segments than we have registers

  and, we want to pass around far pointers and the segment register that's been set up with the matching segment id.

  so that we don't have to allocate segments.
  

  so we clearly need to have an interface that is,

  f(segment register, near pointer)

  then probably we have something that's:

  f(Bundle(segment register, far pointer))

  which does the type check...

  so if it's much more expensive to change a segment register than it is to do a dynamic check...
  maybe this works?
  maybe this makes sense?

  we do a join across all the segment registers to make sure they're the same...
  well, across all bundles to make sure they've got the same segment register, I guess...
  well, no, we extract the segment register and check the thing,
  check that they are all the same.
  and we also reduce the bundle to a near pointer.

  no so, that's not maximally exploiting the efficiency.
  the segment register doesn't need to be the same across all of them;
  it just needs to... um...
  it just needs to point to the same segment?

  
  no, this is silliness...

  hmm..

  okay so we have this pointer type which is a packed pointer,
  which is the segment register plus a far pointer.

  we pass around the packed pointer to places.

  what are the methods on the packed pointer?

  I guesse they are,
  "gimme some far pointers and I'll do these things"?

  or packed pointers?

  or near pointers?

  so f(segment register, far pointer) doesn't seem that bad *if* it does the type check, right

  f(segment register, far pointer, far pointer, far pointer)

  and have it do the check that segments are correct.

  so then we have a bundle like,
  Bundle(segment register, far pointer),
  which has methods which take... what?

  Bundles or far pointers?

  Well, if it's far pointers, it's cheap, innit.
  We just delegate on down to the function.

  If it's bundles... isn't it also cheap?
  We also just delegate down?

  I guess we just don't even bother checking the segment registers of the others.

  segment registers are required to actually access things.
  near pointer combined with segment register is what's necessary

  okay so i've got objects which are near pointers and objects which are segment registers,
  and then i've got a function (near pointer, segment register) -> stuff,
  which is how we ultimately perform an access to a near pointer, we run an instruction
  with a segment register override prefix to specify which segment register to use,
  and pass a near pointer in a general purpose register as an argument to the instruction.

  and then i've got an object containing a near pointer and a segment id, aka a far pointer object.
  and then i've got a function (far pointer, segment register) -> stuff,
  which is where I specify a segment register to use for accessing this far pointer,
  and we do a dynamic check to check that the segment register is set to the correct segment id.
  this is nice and fast, if changing the segment register is slower than checking that it's correct.

  so what would I call an object containing a far pointer and a segment register?

  the existence of this object asserts that we can correctly access this pointer,
  and throws an exception if we try to use the far pointer and segment register together and it breaks.

  near pointer

  far pointer

  a segment is a place

  a pointer is a location inside the place

  a segment id is a place identifier

  a segment register then is a route to the place?
  or like, a channel you can follow to get to the place?

  how does that relate to a notion of source routes as far pointers...

  if a source route is a far pointer

  then what's the segment register?
  the network interface????

  naw...
  maybe a segment register is just an even more far pointer

  so hmm

  each segment register could theoretically have different segment ids?

  no i guess segment ids are global

  from our perspective it's truly uniquely defined

  i guess it's hierarchical

  but a representation as a graph is better.

  i guess putting a segment register on it is just like one step further on the graph.

  but we need to check that that element in the graph is correct?
  and, hey, wait, we don't walk through the namespace really!

  although, do we actually?
  we walk from the task to the far pointer,
  but that fails if they differ.

  I guess it really is just a more far far pointer.

  distant pointer?

  what's the word for more far,
  but also more accessible?

  far implies you have to go through some additional absolute thingy to get to it.
  or, I guess, you have to go from some larger context.

  a wider view on the world

  astral pointer?

  so we really do need this thing

  but then I guess it's wrapped in a thing which also carries a memory gateway?

  and translates down to the active fd?

  how do we do that as a free function?

  I guess we just have the memory gateway
  and allocator
  and far pointer

  I guess we could have them be methods on active FD after all

  No that's a bad idea

  We'll have the allocators, um...
  return near pointers??

  the only thing we really need is to have the io.Task inherit from or compose from the far.Task.
  which is easy! we can even import the near and far stuff in the base stuff

* getting it to work
** blocking on local thread
   reading/writing is blocking us indefinitely when it's on the local thread.
   hm.
   I guess putting it on a different thread would be fine.
   but, argh, running the syscall connection on a different thread is weird
   can't we be async about this?
   let's lay it out

   we want local reads to go through an Epoller,
   which requires reading pointers to get the result.

   remote reads can't go through an epoller because we're implementing the memory-reading functionality here.

   I guess one side can be async and the other not async?

   that would make sense.

   yeah and running on a separate thread isn't really helpful anyway,
   it'll still block others.

   OK! one async, one direct.
   That makes sense, right?
   There's a "local" part of the gateway,
   whose memory we can already access;
   then there's a "remote" part,
   whose memory we cannot access.

   we need to support a pointer for async fd things.

   I guess we should take a lock when getting the near pointer.
** blocking forever on wait, I guess
   okay so why is this happening?

   aha the futex is hanging forever

   i bet because the address is not properly shared?

   maybe

   hmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm maybe we could/should invoke it after creating the thing?

   that's a little weird

   oh my globbing glob glob

   ugh okay i'll look at the memory maps

   bah! it seems to be the same file mapped!!
   the inode is the same!!

   okay what if i just set set_tid_address after launching?

   what the heck aaaaaaa

   okay so when we're unsharing memory,
   we only need to do this if we start another thread in the same address space,
   because then the stack will stay around

   what does that even mean, how do we even represent that

   bah argh blairhlagh

   okay so, yeah, so launching a futex thread for this is useless when it's in a different address space, right?

   like, we wouldn't do that for a standalone exec

   would we?

   no, it would be useless.

   the address space would close and...

   so yeah let's just make a task that doesn't have this futex thing.

   also it might make sense to have a near pointer and far pointer for memory mappings.

   that way we can refer to ones in other address spaces

   hmmmm we need to support threads setting up the stack right

   wait actually it's quite different because we immediately free the stack
** start up rsyscall executable
   so it's working in thread

   now for execing a separate process

   so I need to write a helper

   I think the helper for remote systems will be different from the local helper.

   probably maybe statically linked so I can get into it immediately?

   or should I use Nix for it?

   statically linked is best I think. it's a small binary anyway.

   and the interface with two fds specified is fine

   so hm hm hm
** helper C function for exec
   should I make a helper function that just does exec?
   that's all the system call does?

   it would need to take arguments at a static location if it wasn't to use a stack

   or just use a stack I guess.

   yeah that's best

   derp

   we can specify the stack and the address and all that stuff

   we can use a pre-existing stack memory so we don't have to allocate

   then it would be good
** okay so we do need the futex
   and it does seem to work at home so let's move on

   (possibly a kernel bug? luckily my home kernel is more recent I think so it should be fixed)

   so we just need to figure out a nice api for detecing the finishing of an exec or exit

   it needs to incoporate both waiting for a message on the fd,
   and also waiting for a child event.

   And when I get either I cancel the other

   And when I cancel a system call, I close the interface;
   that's the cleanest way to work,
   since otherwise I have weird races.

   OK yeah seems really clean

   and, to exec, we need to have one of these monitor things

   or I could just even close the the interface rather than cancel.

   oh nooooo aaaa okay I see

   so if I want to just pass the sysif down... without closing the fds...

   then I need to treat this somewhat specially.

   ok so, really, closing the interface is not going to help.

   more specifically, I still need to cancel on thingy.

   Then the close interface shouldn't actually close stuff?

   Hmm tricky

   So we definitely, probably, maybe,
   need to make a new syscall interface after exec.

   even if it's the same...

   bah maybe I should do all the stuff twice

   no hmm hmm I think I can do it externally I guess

   if I can just harvest the fds from the syscall interface.

   hmm

   so what do I need

   the infd and outfd need to stay around

   I probably need to make a new gateway???
   but if it's already a different address space it'll stay with that gateway?

   but what if that address space is already shared with others?
   well.

   we could be on a shared gateway

   yes hmm

   meh, let's just use additional file descriptors for now.

   so wait, um

   so we're talking about that because,
   achieving a clean execve requires cancelling the syscall
   and cancelling the syscall requires or suggests closing the interface so it can't be reused
   we don't *have* to do that though

   what will we pass down?
   nothing perhaps, we can do the cancel outside.

   it doesn't make *that* much sense to have the Thread be the thing
   after all, we can make an MMReleaseMonitor after the fact,
   in a separate process.

   hmm so getting notified of when the exec is successful is fairly important for,
   a scenario where,
   you spawn a process using a thread?
   and are passing resources through your address space and file descriptors and stuff?
   I guess maybe the need for that notification is why people don't do that.
** blocking memcpy
   OK so it looks like trio is not even monitoring our epoll fd.

   Somehow it's blocked.

   Oops it was my fault, I cancelled the thing
** TODO make sure cancellation is all good
** ok
   so things are working.

   what's this division between things with an exec monitoring thing,
   and things without?

   things either have an "active tid address" object or an "inactive tid address" object,
   and they move from one to the other.

   otherwise we'd not be able to track erroneous creation of one invalidating the other.

   how do we compare to make sure it's valid?

   I guess that's a thing inside the Task?

   a tid address thingy?

   how would I handle this with my ideal API, something coming through wait?

   well I'd need to turn it on again after each fork I guess?
   or maybe it'd be inherited?

   in any case I'd store the current state of it.
   no i'd just force it on..
   no wait, it would break my grandchildren so I'd need to turn it on explicitly.

   so I have to activate it for each new thread
   I activate it after the fact, after spawning.

   Then I can only exec in threads with it on?

   Would it be inherited over exec?
   No I guess not.

   so I have two things:
   a monitoring for exec complete thing,
   and a flag-of-whether-that's-enabled thing
** TODO yikes i really hacked this together
** just do ssh too
   okay so I probably need to do this by socket forwarding?

   that is distasteful because it places requirements on the remote side

   hmm using an ssh library directly would be good

   no let's do it with openssh command line
** openssh unix socket forwarding bootstrap
   1. Run bootstrap command on remote host using ssh.
   2. Bootstrap command prints out environment,
      and binds a Unix socket at some path and prints the path and file descriptor number.
   3. Run a second ssh command forwarding a local unix socket to the remote unix socket path.
   4. Connect twice to the local Unix socket.
   5. Bootstrap calls accept twice on its bound Unix socket,
      and prints out the fd numbers,
      and execs rsyscall binary with those fd numbers in arguments.
   6. rsyscall binary prints out relevant C library function addresses,
      then calls rsyscall main loop function

   We don't specify ControlMaster; if the user wants connection sharing, they should configure it!

   Need to start with the rsyscall binary thing
   and starting threads in there.

   so the binary needs a stdout passed down
   to which it will write various addresses
** rsyscall binary thing
   OK so I did that

   But to test, I want to run a thread inside it:
   which requires multi-hop memory copying, I think.

   Or some kind of scheme or awareness or something.
** multi-hop memory handling
   Let's figure this out.

   This is also in general the problem of data at more that one remove;
   a datafd that isn't direct. hm.

   I think we should try to support direct stuff,
   and just not support indirect stuff for the moment.

   I guess that's 0 or 1 hops are allowed, in a sense, hm.
** efficient socketpipe
   We want to be opening direct connections for all our things.

   Actually doesn't this include, like, pipes to subprocesses too?

   That should be good, very clean and nice.

   We'll explicitly deal with the socketpair and socket passing.

   We'll have some kind of class which contains the connection-maker (abstract),
   and an optional Unix socketpair (concrete).

   It will have a helper method to make the connection and pass it over the socketpair if necessary.

   OK so I guess what is really going on is,
   I have a local fd space?
   And the helper gives me something where one end is in that space?

   Weird.
   No that's not right

   I have a task,
   and the helper gives me something where one end is in that task?

   One end is in the local address space, that's clear.

   But an fd is not in any address space!

   It's a matter of task:
   The fd is in an fd space such that there's at least one task in the local address space.

   I guess that's a bit weird so I guess we'll return an active fd.

   We return an fd, plus a task to use it, where the task is in the local address space.

   This means we can use local pointers.

   And the other fd we return, is just a far FD, in some fd space, who cares where.

   And we pass that around to get it locally???

   No, I think we return a far FD in some concrete known fd space.
   And then we can pass it around to get it to the fd space we want it to be in.

   so... LocalSocketpair?
   EfficientSocketpair?

   Bridge?
   BridgeMaker?

   Wormhole?

   Tuber returning a Tube?

   BridgeMaker
   Bridge
   DirectBridgeMaker

   Connection

   ConnectionMaker

   Connection

   We need to do this because,
   we don't know up front how many children we want to make:
   we want to do that direct style.

   Which means we need to make new connections to children,
   which, because we want those connections to be efficient,
   means we need to make those new connections directly.
   And in the worst case that means passing fds around.

   But I don't think it's really that crazy.
* socket passing
  So CMSG structures are aligned to sizeof(size_t).

  Interestingly I guess this is true even for struct cmsghdr and len independently?

  I guess yeah so things are aligned like that.

  So I guess I'll support CMSG_LEN stuff

  Oh I can just use CMSG_LEN from the standard library, that's convenient.

  So I support I can use this array thingy.

  I guess I just build the struct???

  I'll use it with cffi I guess

  Yeah I'll define the structure in cffi.
  that'll check compatibility nicely

  same with msghdr
* garbage collection
  I think I will just neglect freeing things from now on.

  I could achieve a GC'd by writing to a queue in __del__ and monitoring that queue from some closing thread.

  Oh but I guess I need to flush that queue when closing the task or changing the task's namespace.

  Oh and I guess I'll have to use active fds and pointers to do ownership.
  They're the ones that truly own things.

  wait but if I change the task, then I can no longer close it.
  very tricky

  I should support both explicit context managers and __del__ based freeing.

  And, I suppose, explicit closing. but maybe also this subsumes "release"?
  hmm if i support context managers, I need release.
  well, only if I support them in a way that is actually usable :) :) :) :)
* great post
http://www.evanjones.ca/software/threading-linus-msg.html

This shows some of the core idea.
* user namespaces
very important caveat: they don't nest!
* secure usage of rsyscall when you setuid to untrusted users
  prctl(PR_SET_DUMPABLE, SUID_DUMP_DISABLE);
* DONE look into using msg_dontwait for pipes
  Maybe we don't need to use O_NONBLOCK at all.

  Nope, doesn't work.
* plan
need to make a nice API for passing an fd, I guess.
that's the first step?
well, I guess the first step is tasks started by level zero.
then a nice api for fd passing, then tasks started by level one.
then tasks started by level two and on forward.
then, tasks in a different subprocess
meh let's do the fd passing API first

so, let's just stick to using rsyscall_spawn_exec - i.e., operating in a separate process - for everything.

shifting back to a single process should be an easy optimization after that!
** passing
   Three tasks are involved in every child start:

   The access task, through which we access the syscall and data fds.
   The parent task, which will actually clone off the child.
   And the connecting task,
   which has the ability to both create file descriptors to communicate with the access task,
   and transfer file descriptors to the parent task.

   If the connecting task is the same as the access task,
   then we just use socketpair.

   If the connecting task is the same as the parent task,
   then we don't have to transfer file descriptors.

   The capability to create the gateway and to transfer file descriptors,
   are both optional arguments,
   which must be supplied if needed.

   The transferring file descriptor capability is simply a socketpair,
   one end of which is in the connecting task,
   and one end which is in the parent task.

   The bridge-creation capability is... hmm.

   Well, let's keep it reified for now.
   It's a (Unix socket address, listening Unix socket) pair,
   one end of which is in the connecting task,
   and one end which is in the access task.
   
   (I guess it can be abstracted to just a generic address)

   Let's begin with all three the same.
** ok neat
   so we got subprocesses working.

   now we need to get a different access task working.

   the way we do that is, hm.

   do we even need this connecting task thing?

   oh, yes we do, because otherwise we're forced into using a filesystem unx socket thingy, yuck

   So there will be an optional access_connection.

   Which is, umm...

   A listening socket on the connecting task side,
   and an address on the access task side.

   And we make the task and we're good.

   We abstract over address family,
   but hardcode sock_stream.
** ok now ssh
   we got the unix socket now ssh

   so what if we have a separate bootstrap for the unix socket,
   such that we can actually print the path?

   erm hrm part of the issue here is wanting to support multiple terminal applications.
   multiplexing is a bit tricky

   we really want each process under the control of rsyscall,
   not spawned by ssh,
   that's silly.

   so we really want to just pass the terminal fds in to rsyscall,
   and then spawn something using them.
** new plan
   OK so what we'll do instead is:

   1. Run socket_binder on remote host using ssh.
   2. socket_binder prints out the path of two Unix sockets to stdout.
   3. Run a second ssh command forwarding a local Unix socket to the first unix socket path,
      and also running the rsyscall bootstrap.
   4. The rsyscall bootstrap connects to the second unix socket path,
      and receives the listening socket for the first socket via fd-passing.
   5. The first ssh command exits.
   6. Connect four times to the local (forwarded) Unix socket.
   7. Bootstrap calls accept four times on its bound Unix socket,
      using the four fds as (bootstrap_describe, describe, syscall, data) pipes.
   8. Bootstrap prints out the fd numbers,
      and the environment,
      and anything else relevant to the bootstrap_describe pipe,
      then closes it.
   9. Bootstrap execs rsyscall binary with (describe, syscall, data) as arguments.
   10. rsyscall binary prints out relevant C library function addresses to the describe pipe,
       then closes it.
   11. Rsyscall binary calls the rsyscall main loop function.
** socket binder
   Ok we'll start with socket_binder I guess
** exec
   OK so to properly exec something,
   we always need to have the futex around.

   To have the futex around,
   we need, hm

   Well after calling exec, I think it disengages.
   So we need to reset it.

   Yes it disengages.

   So after exec, we definitely need to re-engage it.

   The weird thing is that I don't see any reason it should, umm,
   not fire on the clear_child_tid in a,
   non-CLONE_VM-created process.

   Weird.

   Seems like a possible bug.

   So anyway, we still need to re-engage the futex after an exec.
   So let's do that.

   We need to, hmm, share some memory.
   Possibly we pass down a memfd?

   That seems sensible.

   Man I sure have to do a lot of working around limits in the Linux API! Oh well.

   So I pass down a memfd,
   I map it into memory,
   I set_futex_address,
   I spawn a futex monitoring task elsewhere - likely in the connecting_task maybe.

   Hmm it could be a separate memfd for each task.

   But! We do need to remember the memfd!
   Because if we exec, we need to save it!
   Um, well, we are unlikely to exec into the syscall server again...

   So maybe instead we just, hmm...
   Make a memfd and inherit it right then and there,
   and mmap it and set_futex_address,
   and spawn a futex waiting task in addition to the main task.

   That's a bit inefficient to make a memfd each time, but it's fine.
** memory mapping object
   ok, should we have a memory mapping object?

   I guess we can take a memory mapping object and split it.
   By only mapping part of it.

   I guess I should indeed have a memory mapping object.

   HmmmmmmmmmmmMMMM

   Is it kinda like segments?

   Paging?

   I guess I represent paging explicitly?

   Yes that clearly should be done then.

   But how do I differentiate from pointers then?

   A pointer is something inside a specific range

   MemoryMapping contains a ptr???
*** huge pages issue
    so the length is different and rounded up, hmm.

    which means, we should only be sending in correct lengths.

    because we don't want to invoke that rounding, it's weird.

    so that means we need to know the page size.

    the page size is not *inherently* 4096.

    and offset and length must both be multiples of the page size.

    soooo maybe I should, hm.

    have an argument saying what we think the page size is?

    there are multiple sizes of pages...

    so we essentially say,

    "hey, given whatever the page size of this object is,
    map some number of pages for it."

    yeah, awkwardly we can get hugepages via MAP_HUGETLB,
    or we can get hugepages via mapping hugetlbfs

    either of them can determine the page size, and we have no idea!

    ideally I guess we'd, hmm.

    so an fd corresponds to some pages of memory

    so maybe munmap should take that fd?

    how else do we know the page size?

    alternatively the system call could take the page size,
    and assert that it's correct.

    argh or even better, the system call could not do any rounding.
    that would be the ideal.

    yeah so if I have an optional argument specifying the page size,
    and it just does a validation that the length and offset match the page size,
    then I can return a mapping and things are fine.

    ideally the underlying system call would do that validation but it doesn't.
    oh well!

    and we'll default the page size to 4096 because that's what everything uses.

    Oh and we need the page size in the mapping object.
* futex goofiness
  what the heck, I can't get futexes to work across processes! argh!

  or more precisely, I can't get clone_clear_childtid to clear the tid

  I guess

  ffs I can't get futex to work properly in separate address spaces

  FUCK my life

  atomic_read(&mm->mm_users) > 1)

  GOD DAMN IT

  OK so we're fucked

  How are we supposed to figure out whether the exec succeeds??

  this piece of shit...
  fuck...

  fuck!!!

  okay, what about vfork?

  I could use a robust futex????

  OK so, hm.

  So I could use pthreads?

  I lock a mutex at the start

  OH!
  The robust list actually is per-process!

  Hmm!

  If I give up completely on running arbitrary code...
  Then I can do it.

  aaaaa okay okay fine
** robust list
  so the only way to do this is by setting the robust list after clone or exec or whatever.

  Let's, I guess, do this in the futex page?

  ugh it's not working
** kernel hacking
   so I need to get a kernel hacking setup running again, ffs.

   I guess what I want is a Nix expression that builds a disk image and script,
   which I can pass a kernel image to.

   Ooh, I can do super Emacs integration awesomeness.

   OK so I can start qemu with -qmp pointing to a pipe of my control.

   Then I use that to learn whether it's nographic or not, figuring out the terminal.

   that may be excessive

   ok so basically,

   meh let's just simplify it down to just starting something that gives us the monitor,

   and we do the appropriate attaching on top of that.
   very simple helper function.


   or the more expansive vision would
   prompt you for a configuration.nix which you could edit on the fly,
   then build and start it in a VM

   I guess I'd want an emacs command that rebuilds, and hard reboots the VM.

   Then another command that runs a test in the VM?

   bah BAH

   okay so we need to mount it across, hmm

   and we'll start off by kernel debuggin' when we enter this process or something.

   that gives us a test we can run inside the kernel.

   or we can just printk in mm_release

   so yeah we need to run the thing inside the VM now.
** yay it works!
   everything is good once more!
* need to remember
** DONE recv/send
  gotta try using recv/send instead

  now for recv/send stuff!

  oh it doesn't work oh well

  at least that's resolved!
** using main epoll
  also the using the epollfd in different threads

  what we need then on an asyncfd is,

  the fd,
  an epoll in the same fd space as that fd so we can add/remove/modify the fd,
  an epoll in any space so we can wait on the fd.
** linking
  also another thought is,
  I'm kinda doing a distributed dynamic linking thing here.

  you know, if I went full-in on Nix,
  I could nicely not have to specify the path to the bootstrap.
  Instead I specify from the source host,
  exactly what I want on the remote host,
  and Nix satisfies it.

  of course this requires Nix to be on the PATH on the remote host,
  but that's more expected than the rsyscall bootstrap being there.

  I might go ahead and do that
* alright okay
  moving on, we need to get remoteness working

  for automated tests we'll probably have to set up our own SSH environment, y'know?

  which might be useful for at TS

  anyway remoteness

  we theoretically just need to run a bootstrapper task,
  which will be self-contained,
  and the parent of that task will be the access task,
  and the resulting task will be the connecting task.

  that task can't exec, right

  oh, right, my first step was to run the socket_binder

  On the local host

  And connect to it from Python and get the bound socket.

  Hmm no we will run it over ssh, and do the ssh forwarding, and all that.

  And then launch a process using that as our connection.

  Ah, we'll fake the bootstrapping.

  We'll just start a new child task which is local instead of remote.

  I want the minimal task required to start a child.

  Well.

  That's nothing.

  What I want really is the minimal task required to start a child *which can exec*.

  That needs a whole buncha things.
  I need child monitoring,
  I need thread trampoline...

  Well, if I don't monitor the child, there's no trouble.

  I still need the trampoline though.

  I also need a memory gateway and allocator, of course.

  Why do I ask for this?

  Well, exec_full needs to spawn a thread, so that it can exec into the rsyscall process.

  Hmm I need not just the trampoline but also the syscall function.

  Oh I don't actually need the memory gateway do I?
  Oh, I do because of the trampoline. Argh.

  Hmmm.
  Can I avoid that somehow?

  By making something specialized for rsyscall_server?
  So, I need to communicate the fds that it will use.

  If it's in the same fd namespace, those can't be hardcoded.

  OK so I need the memory gateway just to make a thread.

  But then, shouldn't I have everything I need?

  Yes, I do.

  But the resulting task does not have a memory gateway.

  When I start a task that's going to go across ssh,
  what's the relationship between access task, connecting task, parent task, new task?

  Usually the parent task is the access task.
  But if that's not the case,
  then we have to make a memory-gateway which connects two others.

  Let's just pass down StandardTask and call spawn, maybe?

  this task is implemented in userspace?

  the complexity of rsyscall_spawn is that it needs to work with remote processes?
  but bootstrapping is in userspace?
** ok it seems like it worked kinda
   I was able to spawn a child and exec and everything. Which is cool...

   So now the next step is adding actual ssh?
   And a bootstrap binary?

   Ah, hmm, no, we can do the real bootstrap binary first.

   Then we'll add ssh after that - ssh is gonna require building an ssh test env.
   Which will be the first real useful application - nginx wasn't actually motivated by anything.
** bootstrap binary
   So this is just a C program, I can write this.
   
   I wrote it.
* locations of things in the filesystem
  Do we really want to look up standard utilities in PATH?
  That seems lame.

  Should we bind ourselves to Nix?

  We can have FilesystemResources be an interface which Nix or other things can implement.

  For now, let's just find the locations of executables we depend on,
  at build time.
  That will work with Nix and also some other systems - namely, when it's installed system-wide.

  So we'll do this in ffibuilder?
** building standardtask
   ok, so, surprisingly, it worked without a nightmare of debugging.

   so, now we wrap in ssh, I guess?
  
   yes, we've done the bootstrap binary, so now we need to build an ssh test env.
* ssh environment
   this is the kinda thing that Nix should be able to do

   y'know it would be cool if we got a whole, local environment thing, set up

   your own self-contained userspace

   OK so we can use sshd -i, that's a good improvement.

   Is it? Well, it at least prevents using the network... well, whatever...

   Let's just run regular sshd, ummmmm wait no let's not. I hate hardcoded ports!

   So we'll spawn sshd -i from us, that's fine.

   This guy's legit engineering concerns about efficiency, are,
   well,
   we can get a global view from our python script no problem.
   Saving on startup work is trickier, but if ssh was designed differently,
   we could achieve it.
   by, like, pre-setting-up everything in our own thread,
   and forking it off each time,
   or something.

   So sshd -i, sure.

   Something like:
/nix/store/2kbxp7w3wfmisn71pkf5cx721hsp5gk6-openssh-7.7p1/bin/sshd -d -e -h host_rsa_key -f /dev/null -i

But actually:
openssl genpkey thing | sshd -d -e -h /dev/stdin -f /dev/null -i
   
also viable for key generation is:
echo y | ssh-keygen -t rsa -N '' -f /dev/stdout
but that's not a very nice interface

I guess we generate keys for both halves and let 'er rip?

Let's go ahead and do this with a test_ssh file.

That way we can use -i?

Ah so we need an absolute path to the host key, mhm


OH Neat cool!!!

I could just have ssh -o ProxyCommand 'sshd -i'!!

That would be super mega rad!!!
